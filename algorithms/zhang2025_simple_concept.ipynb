{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d6d53b",
   "metadata": {},
   "source": [
    "# Zhang 2025: Key Insight (Simple Pedagogical Example)\n",
    "\n",
    "**Paper**: \"Loss-Minimizing Model Compression via Joint Factorization Optimization\" (Zhang et al., 2025)\n",
    "\n",
    "This notebook demonstrates the fundamental insight in the clearest possible way using a simple 3×3 matrix example.\n",
    "\n",
    "## Core Question\n",
    "\n",
    "When we factorize a matrix $W \\approx L \\cdot R^T$, we introduce noise $\\delta = W - L \\cdot R^T$.\n",
    "\n",
    "**Traditional approach (SVD, etc.)**: Minimize $\\|W - L \\cdot R^T\\|^2$ without considering any downstream objective.\n",
    "\n",
    "**Zhang 2025 insight**: The factorization noise affects your objective function. If the noise points **opposite** to the gradient, your loss **decreases**!\n",
    "\n",
    "$$\\Delta \\text{Loss} = \\frac{\\partial \\text{Loss}}{\\partial W} \\cdot \\delta$$\n",
    "\n",
    "If $\\frac{\\partial \\text{Loss}}{\\partial W} \\cdot \\delta < 0$, then **loss decreases** ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7d2ec",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd062713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc2914",
   "metadata": {},
   "source": [
    "## Step 1: Create Original Matrix and Define Objective\n",
    "\n",
    "We'll use a simple 3×3 matrix and a toy objective function: we want the row sums to match target values.\n",
    "\n",
    "**Objective**: Make row sums equal to $[10, 20, 30]$\n",
    "\n",
    "**Loss function**: $\\text{Loss} = \\sum_i (\\text{row\\_sum}_i - \\text{target}_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 3x3 matrix\n",
    "W_original = np.array([[1.0, 2.0, 3.0],\n",
    "                        [4.0, 5.0, 6.0],\n",
    "                        [7.0, 8.0, 9.0]])\n",
    "\n",
    "print(\"Original Matrix W:\")\n",
    "print(W_original)\n",
    "print()\n",
    "\n",
    "# Suppose we want to achieve some objective (e.g., row sums = targets)\n",
    "targets = np.array([10.0, 20.0, 30.0])  # Desired row sums\n",
    "actual = W_original.sum(axis=1)\n",
    "\n",
    "print(f\"Target row sums:  {targets}\")\n",
    "print(f\"Current row sums: {actual}\")\n",
    "print(f\"Errors:           {actual - targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bababeb",
   "metadata": {},
   "source": [
    "## Step 2: Compute Gradient\n",
    "\n",
    "The **gradient** $\\frac{\\partial \\text{Loss}}{\\partial W}$ tells us: \"How does each element of $W$ affect the loss?\"\n",
    "\n",
    "For our row-sum objective:\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial W_{ij}} = 2 \\cdot \\text{error}_i$$\n",
    "\n",
    "All elements in the same row contribute equally to that row's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient: how does each element affect the objective?\n",
    "errors = actual - targets\n",
    "gradient = errors[:, np.newaxis] @ np.ones((1, 3))  # Broadcast to full matrix\n",
    "\n",
    "print(\"Gradient (∂Loss/∂W):\")\n",
    "print(gradient)\n",
    "print()\n",
    "print(\"→ Gradient shows: each element contributes equally to row sum error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbad0b",
   "metadata": {},
   "source": [
    "## Step 3: Traditional SVD Factorization\n",
    "\n",
    "**Traditional approach**: Use SVD to get low-rank approximation that minimizes $\\|W - L \\cdot R^T\\|^2$\n",
    "\n",
    "SVD doesn't know about our objective function—it just minimizes reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d300182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional SVD: minimize ||W - L·R^T||² without considering gradient\n",
    "U, s, Vt = svd(W_original)\n",
    "\n",
    "# Rank-2 approximation\n",
    "L_svd = U[:, :2] @ np.diag(np.sqrt(s[:2]))\n",
    "R_svd = Vt[:2, :].T @ np.diag(np.sqrt(s[:2]))\n",
    "W_svd = L_svd @ R_svd.T\n",
    "\n",
    "noise_svd = W_svd - W_original\n",
    "\n",
    "print(\"Traditional SVD (rank=2):\")\n",
    "print(f\"  Reconstruction error: {np.linalg.norm(W_original - W_svd, 'fro'):.2e}\")\n",
    "print()\n",
    "print(\"Noise introduced by SVD:\")\n",
    "print(noise_svd)\n",
    "print()\n",
    "\n",
    "# Check objective\n",
    "actual_svd = W_svd.sum(axis=1)\n",
    "loss_original = np.sum((actual - targets) ** 2)\n",
    "loss_svd = np.sum((actual_svd - targets) ** 2)\n",
    "\n",
    "print(f\"Original loss: {loss_original:.4f}\")\n",
    "print(f\"SVD loss:      {loss_svd:.4f}\")\n",
    "print(f\"Change:        {loss_svd - loss_original:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890f3b42",
   "metadata": {},
   "source": [
    "### Zhang 2025 Analysis: Check Gradient · Noise\n",
    "\n",
    "The key equation from Zhang 2025:\n",
    "$$\\Delta \\text{Loss} \\approx 2 \\cdot (\\text{gradient} \\cdot \\text{noise})$$\n",
    "\n",
    "If $\\text{gradient} \\cdot \\text{noise} < 0$, loss **decreases** ✓  \n",
    "If $\\text{gradient} \\cdot \\text{noise} > 0$, loss **increases** ✗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang 2025 insight: gradient · noise tells us about loss change\n",
    "inner_product_svd = np.sum(gradient * noise_svd)\n",
    "predicted_change_svd = 2 * inner_product_svd  # Factor of 2 from quadratic loss\n",
    "\n",
    "print(\"Zhang 2025 Analysis of SVD:\")\n",
    "print(f\"  gradient · noise = {inner_product_svd:.4f}\")\n",
    "print(f\"  Predicted loss change: {predicted_change_svd:.4f}\")\n",
    "print(f\"  Actual loss change:    {loss_svd - loss_original:+.4f}\")\n",
    "print()\n",
    "\n",
    "if inner_product_svd > 0:\n",
    "    print(\"  → Noise in SAME direction as gradient → Loss INCREASES\")\n",
    "elif inner_product_svd < 0:\n",
    "    print(\"  → Noise in OPPOSITE direction to gradient → Loss DECREASES ✓\")\n",
    "else:\n",
    "    print(\"  → Noise orthogonal to gradient → Loss unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd524f7e",
   "metadata": {},
   "source": [
    "## Step 4: Zhang 2025 Gradient-Guided Factorization\n",
    "\n",
    "**Zhang 2025 approach**: Instead of blindly minimizing reconstruction error, choose the factorization noise to point **opposite to the gradient**.\n",
    "\n",
    "This way, the factorization error helps reduce the objective loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise in opposite direction to gradient\n",
    "noise_zhang = -0.5 * gradient  # Opposite to gradient, small magnitude\n",
    "W_zhang = W_original + noise_zhang\n",
    "\n",
    "print(\"Gradient-guided noise (opposite to gradient):\")\n",
    "print(noise_zhang)\n",
    "print()\n",
    "\n",
    "print(\"New matrix W:\")\n",
    "print(W_zhang)\n",
    "print()\n",
    "\n",
    "# Check objective\n",
    "actual_zhang = W_zhang.sum(axis=1)\n",
    "loss_zhang = np.sum((actual_zhang - targets) ** 2)\n",
    "\n",
    "print(f\"Original loss: {loss_original:.4f}\")\n",
    "print(f\"Zhang loss:    {loss_zhang:.4f}\")\n",
    "print(f\"Change:        {loss_zhang - loss_original:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4954b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_product_zhang = np.sum(gradient * noise_zhang)\n",
    "predicted_change_zhang = 2 * inner_product_zhang\n",
    "\n",
    "print(\"Zhang 2025 Analysis:\")\n",
    "print(f\"  gradient · noise = {inner_product_zhang:.4f}\")\n",
    "print(f\"  Predicted loss change: {predicted_change_zhang:.4f}\")\n",
    "print(f\"  Actual loss change:    {loss_zhang - loss_original:+.4f}\")\n",
    "print()\n",
    "print(\"  ✓ Loss DECREASES because noise opposes gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b2e08",
   "metadata": {},
   "source": [
    "## Step 5: Comparison\n",
    "\n",
    "Compare the two approaches side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: SVD vs. Zhang 2025\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"SVD:         Loss = {loss_svd:.4f}  (change: {loss_svd - loss_original:+.4f})\")\n",
    "print(f\"Zhang 2025:  Loss = {loss_zhang:.4f}  (change: {loss_zhang - loss_original:+.4f})\")\n",
    "print()\n",
    "\n",
    "improvement = ((loss_svd - loss_zhang) / loss_original) * 100\n",
    "print(f\"Zhang is {abs(improvement):.1f}% better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310aef4",
   "metadata": {},
   "source": [
    "## Step 6: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Gradient\n",
    "ax = axes[0]\n",
    "im = ax.imshow(gradient, cmap='RdBu_r', vmin=-10, vmax=10)\n",
    "ax.set_title('Gradient\\n(∂Loss/∂W)')\n",
    "ax.set_xlabel('Column')\n",
    "ax.set_ylabel('Row')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, f'{gradient[i,j]:.1f}', \n",
    "                ha='center', va='center', color='white' if abs(gradient[i,j]) > 5 else 'black')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 2: SVD Noise\n",
    "ax = axes[1]\n",
    "im = ax.imshow(noise_svd, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_title(f'SVD Noise\\n(gradient·noise = {inner_product_svd:.2f})')\n",
    "ax.set_xlabel('Column')\n",
    "ax.set_ylabel('Row')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, f'{noise_svd[i,j]:.2f}', \n",
    "                ha='center', va='center', color='white' if abs(noise_svd[i,j]) > 0.5 else 'black')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 3: Zhang Noise\n",
    "ax = axes[2]\n",
    "im = ax.imshow(noise_zhang, cmap='RdBu_r', vmin=-5, vmax=5)\n",
    "ax.set_title(f'Zhang 2025 Noise\\n(gradient·noise = {inner_product_zhang:.2f})')\n",
    "ax.set_xlabel('Column')\n",
    "ax.set_ylabel('Row')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, f'{noise_zhang[i,j]:.2f}', \n",
    "                ha='center', va='center', color='white' if abs(noise_zhang[i,j]) > 2 else 'black')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a91fc",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "**Traditional factorization (SVD, etc.)** minimizes $\\|W - L \\cdot R^T\\|^2$ without considering the objective function.\n",
    "\n",
    "**Zhang 2025**: Choose $L \\cdot R^T$ such that the noise $(W - L \\cdot R^T)$ points in the **OPPOSITE direction to the gradient** → Loss DECREASES!\n",
    "\n",
    "This is **joint optimization**: factorization error + objective are optimized simultaneously, not sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "### Application to SAXS Deconvolution\n",
    "\n",
    "In SAXS, we have: $D = C \\cdot S^T + \\text{noise}$\n",
    "\n",
    "**Current methods (EFA, MCR-ALS, REGALS)**:\n",
    "1. Stage 1: Minimize $\\|D - C \\cdot S^T\\|^2$ (factorization)\n",
    "2. Stage 2: Apply constraints (non-negativity, smoothness, etc.)\n",
    "\n",
    "**Zhang 2025 suggests**: Optimize both stages **simultaneously** by choosing factorization that inherently satisfies physical constraints while fitting data.\n",
    "\n",
    "This could:\n",
    "- Avoid rotation ambiguity\n",
    "- Prevent rank inflation\n",
    "- Find optimal number of components\n",
    "- Improve physical plausibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
