{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696d322a",
   "metadata": {},
   "source": [
    "# Zhang 2025: Joint Optimization for Matrix Factorization (Full Demo)\n",
    "\n",
    "**Paper**: \"Loss-Minimizing Model Compression via Joint Factorization Optimization\" (Zhang et al., 2025)\n",
    "\n",
    "This notebook demonstrates the complete algorithm with realistic data and iterative optimization.\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "Given:\n",
    "- Matrix $W \\in \\mathbb{R}^{n \\times m}$ (e.g., data or model weights)\n",
    "- Downstream task with objective function $\\mathcal{L}(W, \\text{targets})$\n",
    "\n",
    "Goal: Find low-rank approximation $W \\approx L \\cdot R^T$ with rank $k \\ll \\min(n,m)$\n",
    "\n",
    "## Two Approaches\n",
    "\n",
    "### Approach 1: Traditional (Sequential)\n",
    "1. Stage 1: Minimize $\\|W - L \\cdot R^T\\|^2$ using SVD\n",
    "2. Stage 2: Use factorized $W' = L \\cdot R^T$ for downstream task\n",
    "3. Hope that good reconstruction → good task performance\n",
    "\n",
    "### Approach 2: Zhang 2025 (Joint)\n",
    "1. Simultaneously optimize:\n",
    "   - Factorization accuracy: $\\|W - L \\cdot R^T\\|^2 \\leq \\epsilon$\n",
    "   - Task performance: minimize $\\mathcal{L}(L \\cdot R^T, \\text{targets})$\n",
    "2. Key insight: Choose factorization that reduces objective loss\n",
    "\n",
    "**Result**: Joint optimization achieves better task performance than sequential approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda1413",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4da3a1",
   "metadata": {},
   "source": [
    "## Step 1: Generate Toy Data\n",
    "\n",
    "Create a matrix with low-rank structure + noise to simulate realistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2361f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toy_data(n_samples=100, n_features=50, rank=5, noise=0.1):\n",
    "    \"\"\"Generate toy data with low-rank structure + noise\"\"\"\n",
    "    # True low-rank factors\n",
    "    U_true = np.random.randn(n_samples, rank)\n",
    "    V_true = np.random.randn(n_features, rank)\n",
    "    \n",
    "    # True low-rank matrix\n",
    "    W_true = U_true @ V_true.T\n",
    "    \n",
    "    # Add noise\n",
    "    W_noisy = W_true + noise * np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    return W_noisy, W_true, U_true, V_true\n",
    "\n",
    "# Generate data\n",
    "n_samples, n_features = 100, 50\n",
    "true_rank = 5\n",
    "W_noisy, W_true, _, _ = generate_toy_data(n_samples, n_features, true_rank, noise=0.5)\n",
    "\n",
    "# Generate target values for objective function\n",
    "# (In SAXS: this could be \"physical plausibility scores\")\n",
    "targets = W_true.sum(axis=1) + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "print(f\"Data shape: {W_noisy.shape}\")\n",
    "print(f\"True rank: {true_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456f9bd",
   "metadata": {},
   "source": [
    "## Step 2: Define Objective Function and Gradient\n",
    "\n",
    "For demonstration, we use a simple objective: predict binary targets from matrix row sums.\n",
    "\n",
    "In SAXS context, this could represent \"physical plausibility\" measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(W, targets):\n",
    "    \"\"\"\n",
    "    Simple objective: predict binary targets from matrix W\n",
    "    Loss = mean squared error between row sums and targets\n",
    "    \"\"\"\n",
    "    predictions = W.sum(axis=1)\n",
    "    loss = np.mean((predictions - targets) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_gradient(W, targets):\n",
    "    \"\"\"\n",
    "    Gradient of objective function with respect to W\n",
    "    Tells us: \"How does each element of W affect the loss?\"\n",
    "    \"\"\"\n",
    "    predictions = W.sum(axis=1)\n",
    "    errors = predictions - targets\n",
    "    \n",
    "    # Gradient: ∂Loss/∂W[i,j] = 2 * error[i] / n_features\n",
    "    gradient = 2 * errors[:, np.newaxis] @ np.ones((1, W.shape[1])) / W.shape[1]\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Test the functions\n",
    "print(\"Original data objective loss:\", objective_function(W_noisy, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305eedc",
   "metadata": {},
   "source": [
    "## Step 3: Demonstrate Gradient Direction Insight\n",
    "\n",
    "Zhang 2025's key insight: $\\Delta\\text{Loss} = \\frac{\\partial \\text{Loss}}{\\partial W} \\cdot \\delta$\n",
    "\n",
    "Where $\\delta$ is the factorization noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D example\n",
    "W = np.array([[1.0, 2.0, 3.0],\n",
    "               [4.0, 5.0, 6.0]])\n",
    "targets_demo = np.array([10.0, 25.0])\n",
    "\n",
    "gradient = compute_gradient(W, targets_demo)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY INSIGHT: Gradient Direction Effect\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Original matrix W:\")\n",
    "print(W)\n",
    "print()\n",
    "print(f\"Gradient ∂Loss/∂W:\")\n",
    "print(gradient)\n",
    "print()\n",
    "\n",
    "# Case 1: Random factorization noise (no gradient info)\n",
    "delta_random = 0.5 * np.random.randn(*W.shape)\n",
    "inner_product_random = np.sum(gradient * delta_random)\n",
    "\n",
    "W_random = W + delta_random\n",
    "loss_original = objective_function(W, targets_demo)\n",
    "loss_random = objective_function(W_random, targets_demo)\n",
    "\n",
    "print(f\"Case 1: Random noise\")\n",
    "print(f\"  Inner product (gradient · δ):  {inner_product_random:.4f}\")\n",
    "print(f\"  Loss change:                    {loss_random - loss_original:.4f}\")\n",
    "print()\n",
    "\n",
    "# Case 2: Gradient-guided noise (Zhang 2025 approach)\n",
    "delta_guided = -0.5 * gradient  # Opposite direction to gradient\n",
    "inner_product_guided = np.sum(gradient * delta_guided)\n",
    "\n",
    "W_guided = W + delta_guided\n",
    "loss_guided = objective_function(W_guided, targets_demo)\n",
    "\n",
    "print(f\"Case 2: Gradient-guided noise (opposite to gradient)\")\n",
    "print(f\"  Inner product (gradient · δ):  {inner_product_guided:.4f}\")\n",
    "print(f\"  Loss change:                    {loss_guided - loss_original:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"✓ Gradient-guided factorization REDUCES loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e7601",
   "metadata": {},
   "source": [
    "## Step 4: Traditional SVD Factorization\n",
    "\n",
    "Stage 1: Minimize $\\|W - L \\cdot R^T\\|^2$ using SVD  \n",
    "Stage 2: Hope factorized version works well for objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_svd_factorization(W, rank):\n",
    "    \"\"\"\n",
    "    Traditional approach: Minimize ||W - L·R^T||² using SVD\n",
    "    \"\"\"\n",
    "    U, s, Vt = svd(W, full_matrices=False)\n",
    "    \n",
    "    # Keep only top 'rank' components\n",
    "    L = U[:, :rank] @ np.diag(np.sqrt(s[:rank]))\n",
    "    R = Vt[:rank, :].T @ np.diag(np.sqrt(s[:rank]))\n",
    "    \n",
    "    W_approx = L @ R.T\n",
    "    \n",
    "    reconstruction_error = np.linalg.norm(W - W_approx, 'fro')\n",
    "    \n",
    "    return L, R, W_approx, reconstruction_error\n",
    "\n",
    "# Apply traditional SVD\n",
    "print(\"APPROACH 1: Traditional SVD Factorization\")\n",
    "print(\"-\" * 70)\n",
    "L_svd, R_svd, W_svd, error_svd = traditional_svd_factorization(W_noisy, true_rank)\n",
    "loss_svd = objective_function(W_svd, targets)\n",
    "\n",
    "print(f\"Factorization error: {error_svd:.4f}\")\n",
    "print(f\"Objective loss:      {loss_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bebc6",
   "metadata": {},
   "source": [
    "## Step 5: Zhang 2025 Joint Optimization\n",
    "\n",
    "Simultaneously optimize:\n",
    "1. Factorization accuracy: $\\|W - L \\cdot R^T\\|^2 \\leq \\epsilon$\n",
    "2. Objective function: minimize $\\mathcal{L}(L \\cdot R^T, \\text{targets})$\n",
    "\n",
    "Key: Update $L$ and $R$ to reduce BOTH factorization error and objective loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zhang_joint_optimization(W, targets, rank, n_iterations=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Zhang 2025 approach: Joint optimization\n",
    "    \"\"\"\n",
    "    n_samples, n_features = W.shape\n",
    "    \n",
    "    # Initialize with SVD (good starting point)\n",
    "    L, R, _, _ = traditional_svd_factorization(W, rank)\n",
    "    \n",
    "    # Compute gradient of objective\n",
    "    gradient = compute_gradient(W, targets)\n",
    "    \n",
    "    losses = []\n",
    "    factorization_errors = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        W_current = L @ R.T\n",
    "        \n",
    "        # Current loss and factorization error\n",
    "        current_loss = objective_function(W_current, targets)\n",
    "        current_error = np.linalg.norm(W - W_current, 'fro')\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "        factorization_errors.append(current_error)\n",
    "        \n",
    "        # Gradient descent on loss\n",
    "        delta_W = -lr * gradient\n",
    "        \n",
    "        # Project delta_W onto low-rank factors\n",
    "        R_norm = R.T @ R + 1e-8 * np.eye(rank)\n",
    "        L_norm = L.T @ L + 1e-8 * np.eye(rank)\n",
    "        \n",
    "        L = L + lr * (delta_W @ R) @ np.linalg.inv(R_norm)\n",
    "        R = R + lr * (delta_W.T @ L) @ np.linalg.inv(L_norm)\n",
    "        \n",
    "        # Reproject to maintain factorization accuracy\n",
    "        if iteration % 10 == 0:\n",
    "            W_current = L @ R.T\n",
    "            error = W - W_current\n",
    "            L = L + 0.1 * error @ R @ np.linalg.pinv(R.T @ R)\n",
    "    \n",
    "    W_final = L @ R.T\n",
    "    final_error = np.linalg.norm(W - W_final, 'fro')\n",
    "    \n",
    "    return L, R, W_final, final_error, losses, factorization_errors\n",
    "\n",
    "# Apply Zhang 2025 joint optimization\n",
    "print(\"APPROACH 2: Zhang 2025 Joint Optimization\")\n",
    "print(\"-\" * 70)\n",
    "L_joint, R_joint, W_joint, error_joint, losses, errors = zhang_joint_optimization(\n",
    "    W_noisy, targets, true_rank, n_iterations=100, lr=0.01\n",
    ")\n",
    "loss_joint = objective_function(W_joint, targets)\n",
    "\n",
    "print(f\"Factorization error: {error_joint:.4f}\")\n",
    "print(f\"Objective loss:      {loss_joint:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba87165",
   "metadata": {},
   "source": [
    "## Step 6: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a31c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original matrix loss:        {objective_function(W_noisy, targets):.4f}\")\n",
    "print(f\"SVD factorization loss:      {loss_svd:.4f}\")\n",
    "print(f\"Joint optimization loss:     {loss_joint:.4f}\")\n",
    "print()\n",
    "print(f\"Loss improvement (SVD):      {objective_function(W_noisy, targets) - loss_svd:.4f}\")\n",
    "print(f\"Loss improvement (Joint):    {objective_function(W_noisy, targets) - loss_joint:.4f}\")\n",
    "print()\n",
    "\n",
    "improvement_ratio = (loss_svd - loss_joint) / loss_svd * 100\n",
    "print(f\"Joint is {improvement_ratio:.1f}% better than SVD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb2ee8",
   "metadata": {},
   "source": [
    "## Step 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98484c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Original data\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(W_noisy, aspect='auto', cmap='viridis')\n",
    "ax.set_title(f'Original Noisy Matrix\\nObjective Loss: {objective_function(W_noisy, targets):.4f}')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Samples')\n",
    "\n",
    "# Plot 2: SVD reconstruction\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(W_svd, aspect='auto', cmap='viridis')\n",
    "ax.set_title(f'SVD Reconstruction (rank={true_rank})\\nObjective Loss: {loss_svd:.4f}')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Samples')\n",
    "\n",
    "# Plot 3: Joint optimization result\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(W_joint, aspect='auto', cmap='viridis')\n",
    "ax.set_title(f'Joint Optimization (rank={true_rank})\\nObjective Loss: {loss_joint:.4f}')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Samples')\n",
    "\n",
    "# Plot 4: Optimization trajectory\n",
    "ax = axes[1, 1]\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "line1 = ax.plot(losses, 'b-', label='Objective Loss', linewidth=2)\n",
    "line2 = ax2.plot(errors, 'r--', label='Factorization Error', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Objective Loss', color='b')\n",
    "ax2.set_ylabel('Factorization Error', color='r')\n",
    "ax.tick_params(axis='y', labelcolor='b')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "ax.set_title('Joint Optimization Trajectory')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax.legend(lines, labels, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eb493",
   "metadata": {},
   "source": [
    "## Application to SAXS Deconvolution\n",
    "\n",
    "### Current SAXS Methods (Two-Stage Approach)\n",
    "\n",
    "In SAXS, we decompose: $D = C \\cdot S^T + \\text{noise}$\n",
    "\n",
    "**Stage 1 (Factorization)**: \n",
    "- EFA, MCR-ALS, REGALS minimize $\\|D - C \\cdot S^T\\|^2$\n",
    "\n",
    "**Stage 2 (Constraints)**: \n",
    "- Apply physical constraints: non-negativity, smoothness, compact support\n",
    "\n",
    "**Problem**: Two-stage is suboptimal (Zhang 2025 confirms this!)\n",
    "\n",
    "### Zhang 2025 Paradigm for SAXS\n",
    "\n",
    "**Joint optimization**:\n",
    "$$\\min_{C,S} \\|D - C \\cdot S^T\\|^2 + \\lambda_1 \\cdot \\text{Smoothness}(C) + \\lambda_2 \\cdot \\text{Smoothness}(S)$$\n",
    "\n",
    "Subject to: $C \\geq 0, S \\geq 0$\n",
    "\n",
    "**Benefits**:\n",
    "1. **Optimal K**: Lemma 3 provides framework for determining optimal number of components\n",
    "2. **Avoid rotation ambiguity**: Physical objectives guide factorization directly\n",
    "3. **Prevent rank inflation**: Joint optimization naturally selects minimal rank\n",
    "4. **Better physical plausibility**: Constraints built into factorization, not added after\n",
    "\n",
    "### Potential Applications\n",
    "\n",
    "1. **Gradient-guided SVD** for initial decomposition\n",
    "2. **Joint optimization** of reconstruction + physical objectives\n",
    "3. **Optimal rank determination** using loss-compression trade-off (Lemma 3)\n",
    "4. **Direct incorporation** of P(r) smoothness, Kratky analysis, etc.\n",
    "\n",
    "This could significantly improve SAXS deconvolution beyond current methods!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
