{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6e098b",
   "metadata": {},
   "source": [
    "# Zhang 2025 for Denoising: Fixed-Rank Comparison\n",
    "\n",
    "**Question**: At analyst-chosen rank k, can Zhang 2025's gradient-guided approach provide better denoising than traditional SVD?\n",
    "\n",
    "## Context: Molass Philosophy\n",
    "\n",
    "In Molass, the analyst determines the denoising rank based on:\n",
    "- SEC elution profile expectations\n",
    "- Domain knowledge of sample composition\n",
    "- Data quality assessment\n",
    "\n",
    "**This notebook tests**: Given fixed rank k, does joint optimization improve denoising quality compared to SVD?\n",
    "\n",
    "## Test Data\n",
    "\n",
    "We use **real SEC-SAXS data** from molass_data tutorial samples (Photon Factory, KEK, Japan).\n",
    "\n",
    "## Denoising Approaches\n",
    "\n",
    "### Traditional SVD (Rank-k Truncation)\n",
    "$$D_{\\text{clean}} = U_k \\Sigma_k V_k^T$$\n",
    "- Minimizes: $\\|D - D_{\\text{clean}}\\|_F^2$\n",
    "- Goal: Best rank-k reconstruction\n",
    "\n",
    "### Zhang 2025 (Joint Optimization)\n",
    "$$D_{\\text{clean}} = L \\cdot R^T \\quad \\text{with rank}(L \\cdot R^T) = k$$\n",
    "- Minimizes: $\\|D - L \\cdot R^T\\|_F^2 + \\lambda \\cdot \\text{Objective}(L \\cdot R^T)$\n",
    "- Goal: Reconstruction + preserve important features\n",
    "\n",
    "## Simple Objective Functions\n",
    "\n",
    "For this exploration, we use simple metrics:\n",
    "1. **Smoothness**: Prefer smooth elution profiles\n",
    "2. **Peak preservation**: Maintain peak structure\n",
    "3. **Spectral consistency**: Similar spectra shouldn't diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a12fb9",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load real SEC-SAXS data\n",
    "from molass_data import SAMPLE1, SAMPLE2\n",
    "from molass.DataObjects import SecSaxsData as SSD\n",
    "\n",
    "print(\"Loading real SEC-SAXS data...\")\n",
    "ssd = SSD(SAMPLE1)\n",
    "D_noisy = ssd.xr.M  # Real data matrix (frames × q-points)\n",
    "\n",
    "print(f\"Data shape: {D_noisy.shape}\")\n",
    "print(f\"Data type: Real synchrotron SEC-SAXS from Photon Factory, KEK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91718bdc",
   "metadata": {},
   "source": [
    "## Step 1: Analyst Chooses Denoising Rank\n",
    "\n",
    "**Analyst decision**: Based on SEC profile and domain knowledge, choose rank k for denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyst chooses rank based on inspection\n",
    "k_denoise = 5  # Example: expecting ~2-3 species + baseline + noise structure\n",
    "\n",
    "# Visualize singular value spectrum to validate choice\n",
    "U_full, s_full, Vt_full = svd(D_noisy, full_matrices=False)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(s_full[:20], 'o-', linewidth=2, markersize=8)\n",
    "plt.axvline(k_denoise - 0.5, color='red', linestyle='--', linewidth=2, label=f'Chosen rank k={k_denoise}')\n",
    "plt.xlabel('Singular Value Index')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Singular Value Spectrum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(s_full[:20], 'o-', linewidth=2, markersize=8)\n",
    "plt.axvline(k_denoise - 0.5, color='red', linestyle='--', linewidth=2, label=f'Chosen rank k={k_denoise}')\n",
    "plt.xlabel('Singular Value Index')\n",
    "plt.ylabel('Magnitude (log scale)')\n",
    "plt.title('Singular Value Spectrum (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Analyst chooses k={k_denoise} for denoising\")\n",
    "print(f\"  First {k_denoise} singular values: {s_full[:k_denoise].round(2)}\")\n",
    "print(f\"  Ratio σ_{k_denoise}/σ_{k_denoise+1} = {s_full[k_denoise-1]/s_full[k_denoise]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0847af",
   "metadata": {},
   "source": [
    "## Step 2: Traditional SVD Denoising\n",
    "\n",
    "Standard approach: Keep top k singular values, discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_denoise(D, k):\n",
    "    \"\"\"Traditional SVD denoising: truncate to rank k\"\"\"\n",
    "    U, s, Vt = svd(D, full_matrices=False)\n",
    "    \n",
    "    # Reconstruct with only top k components\n",
    "    D_denoised = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    reconstruction_error = np.linalg.norm(D - D_denoised, 'fro')\n",
    "    \n",
    "    return D_denoised, reconstruction_error\n",
    "\n",
    "# Apply traditional SVD denoising\n",
    "D_svd, error_svd = svd_denoise(D_noisy, k_denoise)\n",
    "\n",
    "print(f\"Traditional SVD Denoising (k={k_denoise}):\")\n",
    "print(f\"  Reconstruction error: {error_svd:.2f}\")\n",
    "print(f\"  Relative error: {error_svd / np.linalg.norm(D_noisy, 'fro'):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3cbd8e",
   "metadata": {},
   "source": [
    "## Step 3: Define Simple Objective Function\n",
    "\n",
    "For demonstration, use **elution profile smoothness** as the downstream objective.\n",
    "\n",
    "Rationale: In SEC-SAXS, elution profiles should be smooth (Gaussian-like peaks), not jagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45790a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothness_objective(D):\n",
    "    \"\"\"\n",
    "    Measure total variation in elution profiles (sum over q-points)\n",
    "    Lower = smoother profiles = better\n",
    "    \"\"\"\n",
    "    # Total scattering at each frame\n",
    "    elution_profile = D.sum(axis=1)\n",
    "    \n",
    "    # Second derivative (roughness)\n",
    "    d2 = np.diff(elution_profile, n=2)\n",
    "    roughness = np.sum(d2 ** 2)\n",
    "    \n",
    "    return roughness\n",
    "\n",
    "def compute_gradient_smoothness(D):\n",
    "    \"\"\"\n",
    "    Gradient of smoothness objective with respect to D\n",
    "    \"\"\"\n",
    "    elution_profile = D.sum(axis=1)\n",
    "    \n",
    "    # Gradient of second-order difference\n",
    "    n = len(elution_profile)\n",
    "    gradient_profile = np.zeros(n)\n",
    "    \n",
    "    # Approximate gradient using finite differences\n",
    "    d2 = np.diff(elution_profile, n=2)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if 2 <= i < n:\n",
    "            gradient_profile[i] += 2 * d2[i-2]\n",
    "        if 1 <= i < n-1:\n",
    "            gradient_profile[i] -= 4 * d2[i-1]\n",
    "        if i < n-2:\n",
    "            gradient_profile[i] += 2 * d2[i]\n",
    "    \n",
    "    # Broadcast to full matrix (each q-point contributes equally to sum)\n",
    "    gradient = gradient_profile[:, np.newaxis] @ np.ones((1, D.shape[1]))\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Test on original data\n",
    "roughness_original = smoothness_objective(D_noisy)\n",
    "roughness_svd = smoothness_objective(D_svd)\n",
    "\n",
    "print(\"Elution Profile Smoothness:\")\n",
    "print(f\"  Original data roughness: {roughness_original:.2e}\")\n",
    "print(f\"  SVD denoised roughness:  {roughness_svd:.2e}\")\n",
    "print(f\"  Improvement: {(1 - roughness_svd/roughness_original)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512670f",
   "metadata": {},
   "source": [
    "## Step 4: Zhang 2025 Joint Optimization\n",
    "\n",
    "Optimize rank-k denoising to minimize BOTH reconstruction error AND roughness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zhang_denoise(D, k, lambda_smooth=0.01, n_iterations=50, lr=0.01):\n",
    "    \"\"\"\n",
    "    Zhang 2025 joint optimization denoising\n",
    "    Minimize: ||D - L·R^T||² + λ·Smoothness(L·R^T)\n",
    "    \"\"\"\n",
    "    n_frames, n_q = D.shape\n",
    "    \n",
    "    # Initialize with SVD\n",
    "    U, s, Vt = svd(D, full_matrices=False)\n",
    "    L = U[:, :k] @ np.diag(np.sqrt(s[:k]))\n",
    "    R = Vt[:k, :].T @ np.diag(np.sqrt(s[:k]))\n",
    "    \n",
    "    # Compute gradient of smoothness objective\n",
    "    gradient_smooth = compute_gradient_smoothness(D)\n",
    "    \n",
    "    history = {\n",
    "        'reconstruction_error': [],\n",
    "        'smoothness': [],\n",
    "        'total_loss': []\n",
    "    }\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        D_current = L @ R.T\n",
    "        \n",
    "        # Current metrics\n",
    "        recon_error = np.linalg.norm(D - D_current, 'fro') ** 2\n",
    "        smoothness = smoothness_objective(D_current)\n",
    "        total_loss = recon_error + lambda_smooth * smoothness\n",
    "        \n",
    "        history['reconstruction_error'].append(recon_error)\n",
    "        history['smoothness'].append(smoothness)\n",
    "        history['total_loss'].append(total_loss)\n",
    "        \n",
    "        # Gradient: reconstruction + smoothness\n",
    "        grad_recon = -2 * (D - D_current)\n",
    "        grad_smooth = compute_gradient_smoothness(D_current)\n",
    "        gradient_total = grad_recon + lambda_smooth * grad_smooth\n",
    "        \n",
    "        # Update factors (projected gradient descent)\n",
    "        delta_W = -lr * gradient_total\n",
    "        \n",
    "        # Project onto low-rank\n",
    "        R_norm = R.T @ R + 1e-8 * np.eye(k)\n",
    "        L_norm = L.T @ L + 1e-8 * np.eye(k)\n",
    "        \n",
    "        L = L + lr * (delta_W @ R) @ np.linalg.inv(R_norm)\n",
    "        R = R + lr * (delta_W.T @ L) @ np.linalg.inv(L_norm)\n",
    "        \n",
    "        # Periodically reproject to maintain reconstruction accuracy\n",
    "        if iteration % 10 == 0:\n",
    "            D_current = L @ R.T\n",
    "            error = D - D_current\n",
    "            L = L + 0.1 * error @ R @ np.linalg.pinv(R.T @ R)\n",
    "    \n",
    "    D_final = L @ R.T\n",
    "    final_recon_error = np.linalg.norm(D - D_final, 'fro')\n",
    "    final_smoothness = smoothness_objective(D_final)\n",
    "    \n",
    "    return D_final, final_recon_error, final_smoothness, history\n",
    "\n",
    "# Apply Zhang 2025 denoising\n",
    "print(\"Running Zhang 2025 joint optimization...\")\n",
    "D_zhang, error_zhang, smoothness_zhang, history = zhang_denoise(\n",
    "    D_noisy, k_denoise, lambda_smooth=0.01, n_iterations=50, lr=0.01\n",
    ")\n",
    "\n",
    "print(f\"\\nZhang 2025 Joint Optimization (k={k_denoise}):\")\n",
    "print(f\"  Reconstruction error: {error_zhang:.2f}\")\n",
    "print(f\"  Relative error: {error_zhang / np.linalg.norm(D_noisy, 'fro'):.2%}\")\n",
    "print(f\"  Final smoothness: {smoothness_zhang:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1e136",
   "metadata": {},
   "source": [
    "## Step 5: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e979283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: SVD vs Zhang 2025 (Fixed Rank k={})\".format(k_denoise))\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Reconstruction Error (Frobenius Norm):\")\n",
    "print(f\"  SVD:    {error_svd:.2f}\")\n",
    "print(f\"  Zhang:  {error_zhang:.2f}\")\n",
    "print(f\"  Difference: {error_zhang - error_svd:+.2f} ({(error_zhang/error_svd - 1)*100:+.1f}%)\")\n",
    "print()\n",
    "print(\"Smoothness (Elution Profile Roughness):\")\n",
    "print(f\"  Original: {roughness_original:.2e}\")\n",
    "print(f\"  SVD:      {roughness_svd:.2e}\")\n",
    "print(f\"  Zhang:    {smoothness_zhang:.2e}\")\n",
    "print()\n",
    "print(\"Smoothness Improvement:\")\n",
    "print(f\"  SVD:    {(1 - roughness_svd/roughness_original)*100:.1f}% smoother than original\")\n",
    "print(f\"  Zhang:  {(1 - smoothness_zhang/roughness_original)*100:.1f}% smoother than original\")\n",
    "print(f\"  Zhang vs SVD: {(1 - smoothness_zhang/roughness_svd)*100:+.1f}%\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine winner\n",
    "if smoothness_zhang < roughness_svd and error_zhang < error_svd * 1.1:\n",
    "    print(\"✓ Zhang 2025 wins: Better smoothness AND similar reconstruction\")\n",
    "elif smoothness_zhang < roughness_svd:\n",
    "    print(\"⚠ Zhang 2025 mixed: Better smoothness but worse reconstruction\")\n",
    "else:\n",
    "    print(\"✗ SVD wins: Zhang 2025 doesn't improve over traditional SVD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508e0ef",
   "metadata": {},
   "source": [
    "## Step 6: Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52484b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Row 1: Elution profiles\n",
    "elution_original = D_noisy.sum(axis=1)\n",
    "elution_svd = D_svd.sum(axis=1)\n",
    "elution_zhang = D_zhang.sum(axis=1)\n",
    "\n",
    "axes[0, 0].plot(elution_original, 'b-', linewidth=1.5, alpha=0.7, label='Original')\n",
    "axes[0, 0].set_title('Original Data\\nElution Profile')\n",
    "axes[0, 0].set_xlabel('Frame')\n",
    "axes[0, 0].set_ylabel('Total Intensity')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(elution_svd, 'g-', linewidth=2, alpha=0.8, label='SVD')\n",
    "axes[0, 1].plot(elution_original, 'b-', linewidth=0.5, alpha=0.3, label='Original')\n",
    "axes[0, 1].set_title(f'SVD (k={k_denoise})\\nElution Profile')\n",
    "axes[0, 1].set_xlabel('Frame')\n",
    "axes[0, 1].set_ylabel('Total Intensity')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 2].plot(elution_zhang, 'r-', linewidth=2, alpha=0.8, label='Zhang')\n",
    "axes[0, 2].plot(elution_original, 'b-', linewidth=0.5, alpha=0.3, label='Original')\n",
    "axes[0, 2].set_title(f'Zhang 2025 (k={k_denoise})\\nElution Profile')\n",
    "axes[0, 2].set_xlabel('Frame')\n",
    "axes[0, 2].set_ylabel('Total Intensity')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Heatmaps\n",
    "vmin, vmax = D_noisy.min(), D_noisy.max()\n",
    "\n",
    "im1 = axes[1, 0].imshow(D_noisy.T, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "axes[1, 0].set_title('Original Data Matrix')\n",
    "axes[1, 0].set_xlabel('Frame')\n",
    "axes[1, 0].set_ylabel('q-point')\n",
    "plt.colorbar(im1, ax=axes[1, 0])\n",
    "\n",
    "im2 = axes[1, 1].imshow(D_svd.T, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "axes[1, 1].set_title(f'SVD Denoised (k={k_denoise})')\n",
    "axes[1, 1].set_xlabel('Frame')\n",
    "axes[1, 1].set_ylabel('q-point')\n",
    "plt.colorbar(im2, ax=axes[1, 1])\n",
    "\n",
    "im3 = axes[1, 2].imshow(D_zhang.T, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "axes[1, 2].set_title(f'Zhang Denoised (k={k_denoise})')\n",
    "axes[1, 2].set_xlabel('Frame')\n",
    "axes[1, 2].set_ylabel('q-point')\n",
    "plt.colorbar(im3, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedf55b",
   "metadata": {},
   "source": [
    "## Step 7: Optimization Trajectory\n",
    "\n",
    "Track how Zhang 2025 balances reconstruction and smoothness during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fada828",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "iterations = range(len(history['reconstruction_error']))\n",
    "\n",
    "# Plot 1: Reconstruction error\n",
    "axes[0].plot(iterations, history['reconstruction_error'], 'b-', linewidth=2)\n",
    "axes[0].axhline(error_svd**2, color='green', linestyle='--', linewidth=2, label='SVD baseline')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Reconstruction Error²')\n",
    "axes[0].set_title('Reconstruction Error Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Smoothness\n",
    "axes[1].plot(iterations, history['smoothness'], 'r-', linewidth=2)\n",
    "axes[1].axhline(roughness_svd, color='green', linestyle='--', linewidth=2, label='SVD baseline')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Roughness')\n",
    "axes[1].set_title('Elution Profile Smoothness')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Total loss\n",
    "axes[2].plot(iterations, history['total_loss'], 'purple', linewidth=2)\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].set_ylabel('Total Loss')\n",
    "axes[2].set_title('Joint Optimization Objective')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"→ Zhang 2025 iteratively trades off reconstruction vs smoothness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b47380",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**At fixed analyst-chosen rank k:**\n",
    "\n",
    "1. **Both methods are rank-k approximations** - infinitely many exist\n",
    "2. **SVD chooses**: Minimum Frobenius norm reconstruction\n",
    "3. **Zhang 2025 chooses**: Balance reconstruction + downstream objective\n",
    "\n",
    "### Does Zhang 2025 Help?\n",
    "\n",
    "**Result depends on**:\n",
    "- Whether your downstream objective differs from reconstruction error\n",
    "- How much you weight the downstream objective (λ parameter)\n",
    "- Whether the objective captures what matters for your analysis\n",
    "\n",
    "### For Molass Context\n",
    "\n",
    "**Pros**:\n",
    "- Could preserve peak structure better than SVD\n",
    "- Optimizes for what you care about (smoothness, physical plausibility)\n",
    "- Same rank k as analyst would choose anyway\n",
    "\n",
    "**Cons**:\n",
    "- Requires defining explicit objective function\n",
    "- More computational cost than SVD\n",
    "- Benefit may be modest for already-good SVD denoising\n",
    "\n",
    "### Analyst Perspective\n",
    "\n",
    "Your philosophy: **\"Analyst determines rank based on expertise\"** ✓\n",
    "\n",
    "Zhang 2025 respects this:\n",
    "- Analyst still chooses k\n",
    "- Algorithm just finds a better rank-k approximation\n",
    "- Not about automation, about optimization quality\n",
    "\n",
    "**Practical value**: Probably **modest** - SVD is already quite good at denoising. Main value is conceptual: joint optimization is theoretically superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e22f3",
   "metadata": {},
   "source": [
    "# Appendix: Comparison to REGALS Iterative Optimization\n",
    "\n",
    "## Similarity: Both Start from SVD and Iterate\n",
    "\n",
    "### REGALS (Alternating Least Squares)\n",
    "**Initialization**: SVD → EFA windows → Initial P, C\n",
    "\n",
    "**Iteration** (alternating):\n",
    "```\n",
    "while not converged:\n",
    "    # Fix C, optimize P\n",
    "    P ← argmin ||M - PC||² + λ_P·||D²P||²\n",
    "         subject to: P ≥ 0, P(q) ↔ P(r) with d_max\n",
    "    \n",
    "    # Fix P, optimize C  \n",
    "    C ← argmin ||M - PC||² + λ_C·||D²C||²\n",
    "         subject to: C ≥ 0, compact support windows\n",
    "```\n",
    "\n",
    "**Key features**:\n",
    "- Alternating Least Squares (ALS): Optimize one factor at a time\n",
    "- Closed-form solutions when fixing one factor\n",
    "- Regularization: Smoothness on both P and C\n",
    "- Hard constraints: Non-negativity, compact support, real-space SAXS\n",
    "\n",
    "---\n",
    "\n",
    "### Zhang 2025 (Gradient Descent on Both Factors)\n",
    "**Initialization**: SVD → L = U_k·√Σ_k, R = V_k^T·√Σ_k\n",
    "\n",
    "**Iteration** (joint):\n",
    "```\n",
    "while not converged:\n",
    "    D_current ← L·R^T\n",
    "    \n",
    "    # Compute joint gradient\n",
    "    grad_recon ← -2·(D - D_current)\n",
    "    grad_obj ← ∇Objective(D_current)\n",
    "    gradient_total ← grad_recon + λ·grad_obj\n",
    "    \n",
    "    # Update both factors simultaneously\n",
    "    L ← L + lr·(gradient_total·R)·(R^T·R)^(-1)\n",
    "    R ← R + lr·(gradient_total^T·L)·(L^T·L)^(-1)\n",
    "```\n",
    "\n",
    "**Key features**:\n",
    "- Simultaneous gradient descent: Update both factors together\n",
    "- Projected gradient: Project onto low-rank manifold\n",
    "- Single objective: Reconstruction + downstream task\n",
    "- No hard constraints in this implementation (could add)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "| Aspect | REGALS | Zhang 2025 (This Notebook) |\n",
    "|--------|--------|---------------------------|\n",
    "| **Update strategy** | Alternating (one at a time) | Simultaneous (both together) |\n",
    "| **Subproblems** | Closed-form least squares | Gradient descent steps |\n",
    "| **Constraints** | Hard (non-negativity, support windows) | Soft (regularization only) |\n",
    "| **Regularization** | Smoothness on both P and C | Single downstream objective |\n",
    "| **Initialization** | EFA windows → P, C | SVD → L, R |\n",
    "| **Convergence** | Guaranteed (convex subproblems) | Local minima possible |\n",
    "| **Philosophy** | Constraint-based emergence | Gradient-guided optimization |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Comparison Matters for Molass\n",
    "\n",
    "**REGALS context**: Two-stage workflow\n",
    "1. Stage 1: EFA determines components and windows\n",
    "2. Stage 2: ALS refines within windows (above iteration)\n",
    "\n",
    "**Molass context**: Two-stage workflow  \n",
    "1. Stage 1: SVD denoising\n",
    "2. Stage 2: Parametric fitting (EGH/SDM/EDM)\n",
    "\n",
    "**Zhang 2025 insight**: Joint optimization of Stage 1 + Stage 2 is theoretically superior to sequential optimization\n",
    "\n",
    "**Both REGALS and Molass have two-stage separation** where Stage 2 never sees original noisy data:\n",
    "- REGALS: EFA windows → ALS on windowed data\n",
    "- Molass: SVD denoising → Parametric fitting on denoised data\n",
    "\n",
    "**Question for Molass**: Should denoising objective be:\n",
    "- Generic smoothness (current notebook)?\n",
    "- Parametric fit quality (how well EGH/SDM/EDM fits)?\n",
    "- Something else?\n",
    "\n",
    "---\n",
    "\n",
    "## Iterative Optimization Comparison\n",
    "\n",
    "Both methods share:\n",
    "- ✓ Start from SVD initialization\n",
    "- ✓ Iterative refinement toward better solution\n",
    "- ✓ Balance reconstruction vs objective/constraints\n",
    "- ✓ Maintain rank k throughout\n",
    "\n",
    "Key philosophical difference:\n",
    "- **REGALS**: Constraints define what's valid → optimization finds best valid solution\n",
    "- **Zhang 2025**: Objective defines what's good → optimization finds best solution\n",
    "- **Molass**: Parametric form defines structure → optimization finds best parameters\n",
    "\n",
    "All three make modeling choices—the difference is **where** and **how explicitly**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
