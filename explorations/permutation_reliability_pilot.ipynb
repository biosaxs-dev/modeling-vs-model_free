{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656e7446",
   "metadata": {},
   "source": [
    "# Permutation Selection Reliability - Pilot Study\n",
    "\n",
    "**Purpose**: Test feasibility of multi-start optimization to detect permutation ambiguity\n",
    "\n",
    "**Date**: January 26, 2026\n",
    "\n",
    "**Context**: Following discrete_ambiguity_demonstration.ipynb, we now ask: \"How reliably do model-free regularization constraints select the physically correct permutation?\"\n",
    "\n",
    "**This pilot**: Simplest possible test case\n",
    "- 2 components (one permutation: swap vs no-swap)\n",
    "- Moderate overlap (50% separation)\n",
    "- Gaussian concentration profiles\n",
    "- Clean data (SNR = 100)\n",
    "\n",
    "**Goals**:\n",
    "1. Generate synthetic data with known ground truth\n",
    "2. Test REGALS multi-start workflow\n",
    "3. Develop permutation detection methods\n",
    "4. Validate that we can identify selection reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f02fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d8f14",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data with Known Ground Truth\n",
    "\n",
    "### Design\n",
    "\n",
    "**Component 1** (elutes first):\n",
    "- Peak position: frame 35\n",
    "- Width: σ = 5 frames\n",
    "- SAXS profile: Simple Gaussian in q-space\n",
    "\n",
    "**Component 2** (elutes second):\n",
    "- Peak position: frame 55\n",
    "- Width: σ = 5 frames  \n",
    "- Separation: 20 frames = 4σ = moderate overlap\n",
    "- SAXS profile: Different Gaussian in q-space\n",
    "\n",
    "**This is the KNOWN GROUND TRUTH** we'll try to recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fac354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time axis (elution frames)\n",
    "n_frames = 100\n",
    "frames = np.arange(n_frames)\n",
    "\n",
    "# Concentration profiles (ground truth)\n",
    "c1_true = norm.pdf(frames, loc=35, scale=5)  # Component 1: early elution\n",
    "c2_true = norm.pdf(frames, loc=55, scale=5)  # Component 2: late elution\n",
    "\n",
    "# Normalize to sum = 1 (for visualization)\n",
    "c1_true = c1_true / c1_true.sum()\n",
    "c2_true = c2_true / c2_true.sum()\n",
    "\n",
    "C_true = np.vstack([c1_true, c2_true])  # 2 × 100 matrix\n",
    "\n",
    "print(f\"Concentration matrix shape: {C_true.shape}\")\n",
    "print(f\"Component 1 peak at frame: {np.argmax(c1_true)}\")\n",
    "print(f\"Component 2 peak at frame: {np.argmax(c2_true)}\")\n",
    "print(f\"Separation: {np.argmax(c2_true) - np.argmax(c1_true)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-axis (scattering vector)\n",
    "n_q = 50\n",
    "q = np.linspace(0.01, 0.3, n_q)  # Typical SAXS q-range (Å⁻¹)\n",
    "\n",
    "# SAXS profiles (ground truth)\n",
    "# Component 1: smaller particle (broader peak in q)\n",
    "p1_true = np.exp(-0.5 * ((q - 0.1) / 0.05)**2)\n",
    "\n",
    "# Component 2: larger particle (narrower peak in q, shifted)\n",
    "p2_true = 1.5 * np.exp(-0.5 * ((q - 0.15) / 0.03)**2)\n",
    "\n",
    "P_true = np.vstack([p1_true, p2_true])  # 2 × 50 matrix\n",
    "\n",
    "print(f\"SAXS profile matrix shape: {P_true.shape}\")\n",
    "print(f\"Profile 1 max at q = {q[np.argmax(p1_true)]:.3f} Å⁻¹\")\n",
    "print(f\"Profile 2 max at q = {q[np.argmax(p2_true)]:.3f} Å⁻¹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct data matrix M = P^T · C\n",
    "M_clean = P_true.T @ C_true  # 50 × 100 matrix (q × frames)\n",
    "\n",
    "# Add noise (SNR = 100)\n",
    "noise_level = M_clean.mean() / 100\n",
    "noise = np.random.normal(0, noise_level, M_clean.shape)\n",
    "M_noisy = M_clean + noise\n",
    "\n",
    "# Ensure non-negativity (physical constraint)\n",
    "M_noisy = np.maximum(M_noisy, 0)\n",
    "\n",
    "print(f\"Data matrix shape: {M_noisy.shape}\")\n",
    "print(f\"Signal mean: {M_clean.mean():.4f}\")\n",
    "print(f\"Noise std: {noise_level:.4f}\")\n",
    "print(f\"SNR: {M_clean.mean() / noise_level:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb6b7a",
   "metadata": {},
   "source": [
    "### Visualize Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Top left: Concentration profiles\n",
    "axes[0, 0].plot(frames, c1_true, 'b-', linewidth=2, label='Component 1 (early)')\n",
    "axes[0, 0].plot(frames, c2_true, 'r-', linewidth=2, label='Component 2 (late)')\n",
    "axes[0, 0].fill_between(frames, 0, c1_true, alpha=0.3, color='blue')\n",
    "axes[0, 0].fill_between(frames, 0, c2_true, alpha=0.3, color='red')\n",
    "axes[0, 0].set_xlabel('Frame')\n",
    "axes[0, 0].set_ylabel('Concentration (normalized)')\n",
    "axes[0, 0].set_title('Ground Truth: Concentration Profiles')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top right: SAXS profiles\n",
    "axes[0, 1].plot(q, p1_true, 'b-', linewidth=2, marker='o', markersize=4, label='Component 1')\n",
    "axes[0, 1].plot(q, p2_true, 'r-', linewidth=2, marker='s', markersize=4, label='Component 2')\n",
    "axes[0, 1].set_xlabel('q (Å⁻¹)')\n",
    "axes[0, 1].set_ylabel('Intensity')\n",
    "axes[0, 1].set_title('Ground Truth: SAXS Profiles')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom left: Data matrix (clean)\n",
    "im1 = axes[1, 0].imshow(M_clean, aspect='auto', cmap='viridis', origin='lower',\n",
    "                        extent=[frames[0], frames[-1], q[0], q[-1]])\n",
    "axes[1, 0].set_xlabel('Frame')\n",
    "axes[1, 0].set_ylabel('q (Å⁻¹)')\n",
    "axes[1, 0].set_title('Clean Data Matrix M = P^T · C')\n",
    "plt.colorbar(im1, ax=axes[1, 0], label='Intensity')\n",
    "\n",
    "# Bottom right: Data matrix (noisy)\n",
    "im2 = axes[1, 1].imshow(M_noisy, aspect='auto', cmap='viridis', origin='lower',\n",
    "                        extent=[frames[0], frames[-1], q[0], q[-1]])\n",
    "axes[1, 1].set_xlabel('Frame')\n",
    "axes[1, 1].set_ylabel('q (Å⁻¹)')\n",
    "axes[1, 1].set_title('Noisy Data Matrix (SNR=100)')\n",
    "plt.colorbar(im2, ax=axes[1, 1], label='Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_pilot_ground_truth.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Ground truth data generated and visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb290b94",
   "metadata": {},
   "source": [
    "## Part 2: SVD Analysis (Baseline)\n",
    "\n",
    "Before testing REGALS, check that 2 components are clearly identifiable from singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c18348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD\n",
    "U, s, Vt = svd(M_noisy, full_matrices=False)\n",
    "\n",
    "# Compute explained variance\n",
    "explained_var = (s**2) / (s**2).sum()\n",
    "\n",
    "print(\"Singular values (first 10):\")\n",
    "for i in range(min(10, len(s))):\n",
    "    print(f\"  σ_{i+1}: {s[i]:.4f} ({explained_var[i]*100:.2f}% variance)\")\n",
    "\n",
    "print(f\"\\nCumulative variance (first 2): {explained_var[:2].sum()*100:.2f}%\")\n",
    "print(f\"Ratio σ₂/σ₃: {s[1]/s[2]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: Scree plot\n",
    "axes[0].plot(range(1, min(11, len(s)+1)), s[:10], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=2, color='r', linestyle='--', label='True rank = 2')\n",
    "axes[0].set_xlabel('Component')\n",
    "axes[0].set_ylabel('Singular value')\n",
    "axes[0].set_title('Scree Plot')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Cumulative variance\n",
    "axes[1].plot(range(1, min(11, len(s)+1)), np.cumsum(explained_var[:10])*100, \n",
    "             'go-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=99, color='r', linestyle='--', label='99% threshold')\n",
    "axes[1].axvline(x=2, color='r', linestyle='--', label='True rank = 2')\n",
    "axes[1].set_xlabel('Number of components')\n",
    "axes[1].set_ylabel('Cumulative variance (%)')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_pilot_svd.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ SVD analysis complete - rank 2 clearly identifiable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761332cd",
   "metadata": {},
   "source": [
    "## Part 3: Simple Alternating Least Squares (ALS) Implementation\n",
    "\n",
    "Before using REGALS, implement a simple ALS to understand the workflow.\n",
    "\n",
    "**Algorithm**:\n",
    "1. Initialize P, C (from SVD or random)\n",
    "2. Fix P, solve for C: C = (P^T P)^(-1) P^T M^T\n",
    "3. Fix C, solve for P: P = M C^T (C C^T)^(-1)\n",
    "4. Enforce non-negativity\n",
    "5. Repeat until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_als(M, k=2, max_iter=100, tol=1e-6, init='svd', random_state=None):\n",
    "    \"\"\"\n",
    "    Simple non-negative ALS for matrix factorization M ≈ P^T · C\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : array (n_q × n_frames)\n",
    "        Data matrix\n",
    "    k : int\n",
    "        Number of components\n",
    "    max_iter : int\n",
    "        Maximum iterations\n",
    "    tol : float\n",
    "        Convergence tolerance\n",
    "    init : str\n",
    "        Initialization method ('svd' or 'random')\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    P : array (k × n_q)\n",
    "        SAXS profiles\n",
    "    C : array (k × n_frames)\n",
    "        Concentration profiles\n",
    "    history : dict\n",
    "        Convergence history\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_q, n_frames = M.shape\n",
    "    \n",
    "    # Initialize\n",
    "    if init == 'svd':\n",
    "        U, s, Vt = svd(M, full_matrices=False)\n",
    "        P = (U[:, :k] * s[:k]).T  # k × n_q\n",
    "        C = Vt[:k, :]              # k × n_frames\n",
    "    elif init == 'random':\n",
    "        P = np.random.rand(k, n_q)\n",
    "        C = np.random.rand(k, n_frames)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init: {init}\")\n",
    "    \n",
    "    # Enforce non-negativity\n",
    "    P = np.maximum(P, 0)\n",
    "    C = np.maximum(C, 0)\n",
    "    \n",
    "    history = {'iteration': [], 'error': [], 'delta': []}\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        P_old = P.copy()\n",
    "        C_old = C.copy()\n",
    "        \n",
    "        # Update C (fix P)\n",
    "        # M^T ≈ C^T · P → C^T = M^T · P^T · (P · P^T)^(-1)\n",
    "        PtP = P @ P.T + 1e-10 * np.eye(k)  # regularization for stability\n",
    "        C = np.linalg.solve(PtP, P @ M).clip(min=0)\n",
    "        \n",
    "        # Update P (fix C)\n",
    "        # M ≈ P^T · C → P = (M · C^T · (C · C^T)^(-1))^T\n",
    "        CCt = C @ C.T + 1e-10 * np.eye(k)\n",
    "        P = np.linalg.solve(CCt, C @ M.T).clip(min=0)\n",
    "        \n",
    "        # Compute error\n",
    "        M_recon = P.T @ C\n",
    "        error = np.linalg.norm(M - M_recon, 'fro')\n",
    "        delta_P = np.linalg.norm(P - P_old, 'fro')\n",
    "        delta_C = np.linalg.norm(C - C_old, 'fro')\n",
    "        delta = max(delta_P, delta_C)\n",
    "        \n",
    "        history['iteration'].append(i)\n",
    "        history['error'].append(error)\n",
    "        history['delta'].append(delta)\n",
    "        \n",
    "        if delta < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "    \n",
    "    return P, C, history\n",
    "\n",
    "print(\"✓ Simple ALS implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8829c09",
   "metadata": {},
   "source": [
    "### Test ALS with SVD Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ALS from SVD initialization\n",
    "P_svd, C_svd, history_svd = simple_als(M_noisy, k=2, init='svd', random_state=42)\n",
    "\n",
    "print(f\"\\nFinal reconstruction error: {history_svd['error'][-1]:.6f}\")\n",
    "print(f\"Number of iterations: {len(history_svd['iteration'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_svd['iteration'], history_svd['error'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Frobenius norm error')\n",
    "axes[0].set_title('Reconstruction Error')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogy(history_svd['iteration'], history_svd['delta'], 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Parameter change')\n",
    "axes[1].set_title('Convergence (log scale)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_pilot_als_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ ALS converged successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541815f",
   "metadata": {},
   "source": [
    "### Compare with Ground Truth - Check for Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26816e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_permutation(C_result, C_truth):\n",
    "    \"\"\"\n",
    "    Identify which permutation was found by correlating with ground truth.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    permutation : list\n",
    "        Mapping from result to truth [result_0 → truth_?, result_1 → truth_?]\n",
    "    is_swapped : bool\n",
    "        True if components are swapped relative to ground truth\n",
    "    correlation : float\n",
    "        Best correlation value\n",
    "    \"\"\"\n",
    "    k = C_result.shape[0]\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            corr[i, j] = np.corrcoef(C_result[i], C_truth[j])[0, 1]\n",
    "    \n",
    "    # Find best permutation (Hungarian algorithm for general case, but for k=2 it's simple)\n",
    "    if k == 2:\n",
    "        # Option 1: No swap (0→0, 1→1)\n",
    "        corr_no_swap = corr[0, 0] + corr[1, 1]\n",
    "        # Option 2: Swap (0→1, 1→0)\n",
    "        corr_swap = corr[0, 1] + corr[1, 0]\n",
    "        \n",
    "        if corr_no_swap > corr_swap:\n",
    "            permutation = [0, 1]\n",
    "            is_swapped = False\n",
    "            best_corr = corr_no_swap / 2\n",
    "        else:\n",
    "            permutation = [1, 0]\n",
    "            is_swapped = True\n",
    "            best_corr = corr_swap / 2\n",
    "    \n",
    "    return permutation, is_swapped, best_corr\n",
    "\n",
    "perm, is_swapped, corr = identify_permutation(C_svd, C_true)\n",
    "\n",
    "print(f\"Permutation found: {perm}\")\n",
    "print(f\"Components swapped: {is_swapped}\")\n",
    "print(f\"Average correlation: {corr:.4f}\")\n",
    "\n",
    "if is_swapped:\n",
    "    print(\"\\n⚠ WARNING: Components are SWAPPED relative to ground truth!\")\n",
    "else:\n",
    "    print(\"\\n✓ Components match ground truth order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison (accounting for possible permutation)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Reorder C_svd according to permutation for visualization\n",
    "C_svd_aligned = C_svd[perm]\n",
    "P_svd_aligned = P_svd[perm]\n",
    "\n",
    "# Top row: Concentration profiles\n",
    "for i in range(2):\n",
    "    ax = axes[0, i]\n",
    "    ax.plot(frames, C_true[i], 'k-', linewidth=3, label='Ground truth', alpha=0.7)\n",
    "    ax.plot(frames, C_svd_aligned[i], 'r--', linewidth=2, label='ALS result')\n",
    "    ax.fill_between(frames, 0, C_true[i], alpha=0.2, color='black')\n",
    "    ax.set_xlabel('Frame')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.set_title(f'Component {i+1}: Concentration Profile')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    corr_val = np.corrcoef(C_true[i], C_svd_aligned[i])[0, 1]\n",
    "    ax.text(0.98, 0.95, f'Corr: {corr_val:.3f}', \n",
    "            transform=ax.transAxes, ha='right', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Bottom row: SAXS profiles\n",
    "for i in range(2):\n",
    "    ax = axes[1, i]\n",
    "    ax.plot(q, P_true[i], 'k-', linewidth=3, label='Ground truth', alpha=0.7, marker='o', markersize=5)\n",
    "    ax.plot(q, P_svd_aligned[i], 'r--', linewidth=2, label='ALS result', marker='s', markersize=4)\n",
    "    ax.set_xlabel('q (Å⁻¹)')\n",
    "    ax.set_ylabel('Intensity')\n",
    "    ax.set_title(f'Component {i+1}: SAXS Profile')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    corr_val = np.corrcoef(P_true[i], P_svd_aligned[i])[0, 1]\n",
    "    ax.text(0.98, 0.95, f'Corr: {corr_val:.3f}', \n",
    "            transform=ax.transAxes, ha='right', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_pilot_als_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64628527",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Start Experiment\n",
    "\n",
    "Now the key test: Run ALS from multiple random initializations.\n",
    "\n",
    "**Question**: Do different initializations converge to:\n",
    "1. The same permutation (reliable)?\n",
    "2. Different permutations with similar objectives (ambiguous)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple ALS optimizations from different random starts\n",
    "n_runs = 10\n",
    "results = []\n",
    "\n",
    "print(\"Running multi-start experiment...\\n\")\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Random initialization\n",
    "    P_run, C_run, history_run = simple_als(\n",
    "        M_noisy, k=2, init='random', random_state=run\n",
    "    )\n",
    "    \n",
    "    # Identify permutation\n",
    "    perm_run, is_swapped_run, corr_run = identify_permutation(C_run, C_true)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'run': run,\n",
    "        'P': P_run,\n",
    "        'C': C_run,\n",
    "        'permutation': perm_run,\n",
    "        'is_swapped': is_swapped_run,\n",
    "        'correlation': corr_run,\n",
    "        'final_error': history_run['error'][-1],\n",
    "        'n_iterations': len(history_run['iteration'])\n",
    "    })\n",
    "    \n",
    "    swap_str = \"SWAPPED\" if is_swapped_run else \"correct\"\n",
    "    print(f\"Run {run:2d}: {swap_str:7s} | Error: {history_run['error'][-1]:.6f} | Corr: {corr_run:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Multi-start experiment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574585f5",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78276e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count permutations\n",
    "n_swapped = sum(r['is_swapped'] for r in results)\n",
    "n_correct = n_runs - n_swapped\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-START ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total runs: {n_runs}\")\n",
    "print(f\"Correct order: {n_correct} ({n_correct/n_runs*100:.1f}%)\")\n",
    "print(f\"Swapped order: {n_swapped} ({n_swapped/n_runs*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Objective values\n",
    "errors = [r['final_error'] for r in results]\n",
    "errors_correct = [r['final_error'] for r in results if not r['is_swapped']]\n",
    "errors_swapped = [r['final_error'] for r in results if r['is_swapped']]\n",
    "\n",
    "print(f\"Reconstruction errors:\")\n",
    "print(f\"  Overall: {np.mean(errors):.6f} ± {np.std(errors):.6f}\")\n",
    "if errors_correct:\n",
    "    print(f\"  Correct order: {np.mean(errors_correct):.6f} ± {np.std(errors_correct):.6f}\")\n",
    "if errors_swapped:\n",
    "    print(f\"  Swapped order: {np.mean(errors_swapped):.6f} ± {np.std(errors_swapped):.6f}\")\n",
    "print()\n",
    "\n",
    "# Statistical test (if both permutations found)\n",
    "if errors_correct and errors_swapped:\n",
    "    from scipy.stats import ttest_ind\n",
    "    t_stat, p_value = ttest_ind(errors_correct, errors_swapped)\n",
    "    print(f\"t-test (correct vs swapped):\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"  ✓ Objectives are significantly different (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ No significant difference in objectives (p > 0.05)\")\n",
    "        print(f\"  → Regularization does NOT strongly prefer one permutation!\")\n",
    "else:\n",
    "    print(\"Only one permutation found - selection appears consistent\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize objective distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: Histogram of errors\n",
    "if errors_correct and errors_swapped:\n",
    "    axes[0].hist(errors_correct, bins=5, alpha=0.7, color='green', label='Correct order')\n",
    "    axes[0].hist(errors_swapped, bins=5, alpha=0.7, color='red', label='Swapped order')\n",
    "    axes[0].legend()\n",
    "else:\n",
    "    axes[0].hist(errors, bins=10, alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Reconstruction error')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Final Errors')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Scatter plot\n",
    "colors = ['green' if not r['is_swapped'] else 'red' for r in results]\n",
    "axes[1].scatter(range(n_runs), errors, c=colors, s=100, alpha=0.7)\n",
    "axes[1].axhline(y=np.mean(errors), color='blue', linestyle='--', label='Mean')\n",
    "axes[1].set_xlabel('Run number')\n",
    "axes[1].set_ylabel('Reconstruction error')\n",
    "axes[1].set_title('Error by Run')\n",
    "axes[1].legend(['Mean', 'Correct order', 'Swapped order'])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_pilot_multistart_errors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466963d",
   "metadata": {},
   "source": [
    "## Part 4b: Add Smoothness Regularization\n",
    "\n",
    "**Key question**: Does smoothness constraint break the permutation ambiguity?\n",
    "\n",
    "**Hypothesis**: \n",
    "- If smoothness prefers the correct permutation → regularization helps selection\n",
    "- If ambiguity persists → need additional constraints or global optimization\n",
    "\n",
    "We'll add the term: $\\lambda_C \\|D^2 C\\|^2$ where $D^2$ is the second derivative operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_d2_operator(n):\n",
    "    \"\"\"\n",
    "    Create second-order finite difference operator D² for n points.\n",
    "    \n",
    "    D²[i] ≈ c[i-1] - 2*c[i] + c[i+1]\n",
    "    \n",
    "    Returns: (n-2) × n matrix\n",
    "    \"\"\"\n",
    "    D2 = np.zeros((n-2, n))\n",
    "    for i in range(n-2):\n",
    "        D2[i, i] = 1\n",
    "        D2[i, i+1] = -2\n",
    "        D2[i, i+2] = 1\n",
    "    return D2\n",
    "\n",
    "\n",
    "def smooth_als(M, k=2, lambda_c=1.0, max_iter=100, tol=1e-6, init='svd', random_state=None):\n",
    "    \"\"\"\n",
    "    Non-negative ALS with smoothness regularization for M ≈ P^T · C\n",
    "    \n",
    "    Objective: ||M - P^T·C||² + λ_C ||D²C||²\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : array (n_q × n_frames)\n",
    "        Data matrix\n",
    "    k : int\n",
    "        Number of components\n",
    "    lambda_c : float\n",
    "        Smoothness regularization parameter\n",
    "    max_iter : int\n",
    "        Maximum iterations\n",
    "    tol : float\n",
    "        Convergence tolerance\n",
    "    init : str\n",
    "        Initialization method ('svd' or 'random')\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    P : array (k × n_q)\n",
    "        SAXS profiles\n",
    "    C : array (k × n_frames)\n",
    "        Concentration profiles\n",
    "    history : dict\n",
    "        Convergence history\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_q, n_frames = M.shape\n",
    "    \n",
    "    # Initialize\n",
    "    if init == 'svd':\n",
    "        U, s, Vt = svd(M, full_matrices=False)\n",
    "        P = (U[:, :k] * s[:k]).T  # k × n_q\n",
    "        C = Vt[:k, :]              # k × n_frames\n",
    "    elif init == 'random':\n",
    "        P = np.random.rand(k, n_q)\n",
    "        C = np.random.rand(k, n_frames)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown init: {init}\")\n",
    "    \n",
    "    # Enforce non-negativity\n",
    "    P = np.maximum(P, 0)\n",
    "    C = np.maximum(C, 0)\n",
    "    \n",
    "    # Create D² operator\n",
    "    D2 = create_d2_operator(n_frames)\n",
    "    D2tD2 = D2.T @ D2  # n_frames × n_frames (smoothness penalty matrix)\n",
    "    \n",
    "    history = {'iteration': [], 'data_fit': [], 'smoothness': [], 'total': [], 'delta': []}\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        P_old = P.copy()\n",
    "        C_old = C.copy()\n",
    "        \n",
    "        # Update C (fix P) - component-wise with smoothness\n",
    "        for j in range(k):\n",
    "            # Current residual without component j\n",
    "            C_temp = C.copy()\n",
    "            C_temp[j, :] = 0\n",
    "            R = M - P.T @ C_temp  # Residual to be explained by component j\n",
    "            \n",
    "            # Minimize: ||R - p_j^T·c_j||² + λ||D²·c_j||²\n",
    "            # Normal equation: (||p_j||²·I + λ·D²^T·D²)·c_j = R^T·p_j\n",
    "            p_j = P[j, :]\n",
    "            pj_norm_sq = np.dot(p_j, p_j)\n",
    "            A = pj_norm_sq * np.eye(n_frames) + lambda_c * D2tD2\n",
    "            b = R.T @ p_j\n",
    "            C[j, :] = np.linalg.solve(A, b).clip(min=0)\n",
    "        \n",
    "        # Update P (fix C)\n",
    "        CCt = C @ C.T + 1e-10 * np.eye(k)\n",
    "        P = np.linalg.solve(CCt, C @ M.T).clip(min=0)\n",
    "        \n",
    "        # Compute objectives\n",
    "        M_recon = P.T @ C\n",
    "        data_fit = np.linalg.norm(M - M_recon, 'fro')**2\n",
    "        smoothness = sum(np.linalg.norm(D2 @ C[j])**2 for j in range(k))\n",
    "        total_obj = data_fit + lambda_c * smoothness\n",
    "        \n",
    "        delta_P = np.linalg.norm(P - P_old, 'fro')\n",
    "        delta_C = np.linalg.norm(C - C_old, 'fro')\n",
    "        delta = max(delta_P, delta_C)\n",
    "        \n",
    "        history['iteration'].append(i)\n",
    "        history['data_fit'].append(data_fit)\n",
    "        history['smoothness'].append(smoothness)\n",
    "        history['total'].append(total_obj)\n",
    "        history['delta'].append(delta)\n",
    "        \n",
    "        if delta < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "    \n",
    "    return P, C, history\n",
    "\n",
    "print(\"✓ Smoothness-regularized ALS implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd177d",
   "metadata": {},
   "source": [
    "### Test with λ = 1.0 (moderate smoothness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-start with smoothness regularization\n",
    "n_runs = 10\n",
    "lambda_c = 1.0\n",
    "results_smooth = []\n",
    "\n",
    "print(f\"Running multi-start experiment with smoothness (λ = {lambda_c})...\\\\n\")\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Random initialization\n",
    "    P_run, C_run, history_run = smooth_als(\n",
    "        M_noisy, k=2, lambda_c=lambda_c, init='random', random_state=run\n",
    "    )\n",
    "    \n",
    "    # Identify permutation\n",
    "    perm_run, is_swapped_run, corr_run = identify_permutation(C_run, C_true)\n",
    "    \n",
    "    # Store results\n",
    "    results_smooth.append({\n",
    "        'run': run,\n",
    "        'P': P_run,\n",
    "        'C': C_run,\n",
    "        'permutation': perm_run,\n",
    "        'is_swapped': is_swapped_run,\n",
    "        'correlation': corr_run,\n",
    "        'data_fit': history_run['data_fit'][-1],\n",
    "        'smoothness': history_run['smoothness'][-1],\n",
    "        'total_obj': history_run['total'][-1],\n",
    "        'n_iterations': len(history_run['iteration'])\n",
    "    })\n",
    "    \n",
    "    swap_str = \"SWAPPED\" if is_swapped_run else \"correct\"\n",
    "    print(f\"Run {run:2d}: {swap_str:7s} | Total: {history_run['total'][-1]:.6f} | \" +\n",
    "          f\"Data: {history_run['data_fit'][-1]:.6f} | Smooth: {history_run['smoothness'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\\\n✓ Multi-start with smoothness complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373128f",
   "metadata": {},
   "source": [
    "### Analyze Smoothness Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count permutations\n",
    "n_swapped_smooth = sum(r['is_swapped'] for r in results_smooth)\n",
    "n_correct_smooth = n_runs - n_swapped_smooth\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SMOOTHNESS-REGULARIZED ALS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total runs: {n_runs}\")\n",
    "print(f\"Correct order: {n_correct_smooth} ({n_correct_smooth/n_runs*100:.1f}%)\")\n",
    "print(f\"Swapped order: {n_swapped_smooth} ({n_swapped_smooth/n_runs*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Objective values\n",
    "total_objs = [r['total_obj'] for r in results_smooth]\n",
    "total_correct = [r['total_obj'] for r in results_smooth if not r['is_swapped']]\n",
    "total_swapped = [r['total_obj'] for r in results_smooth if r['is_swapped']]\n",
    "\n",
    "data_fits = [r['data_fit'] for r in results_smooth]\n",
    "smoothness_vals = [r['smoothness'] for r in results_smooth]\n",
    "\n",
    "print(f\"Total objectives:\")\n",
    "print(f\"  Overall: {np.mean(total_objs):.6f} ± {np.std(total_objs):.6f}\")\n",
    "if total_correct:\n",
    "    print(f\"  Correct order: {np.mean(total_correct):.6f} ± {np.std(total_correct):.6f}\")\n",
    "if total_swapped:\n",
    "    print(f\"  Swapped order: {np.mean(total_swapped):.6f} ± {np.std(total_swapped):.6f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Data fit terms:\")\n",
    "print(f\"  Overall: {np.mean(data_fits):.6f} ± {np.std(data_fits):.6f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Smoothness terms:\")\n",
    "print(f\"  Overall: {np.mean(smoothness_vals):.4f} ± {np.std(smoothness_vals):.4f}\")\n",
    "print()\n",
    "\n",
    "# Statistical test (if both permutations found)\n",
    "if total_correct and total_swapped:\n",
    "    from scipy.stats import ttest_ind\n",
    "    t_stat, p_value = ttest_ind(total_correct, total_swapped)\n",
    "    print(f\"t-test (correct vs swapped):\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"  ✓ Objectives are significantly different (p < 0.05)\")\n",
    "        print(f\"  → Smoothness provides selection bias!\")\n",
    "        if np.mean(total_correct) < np.mean(total_swapped):\n",
    "            print(f\"  → Correctly favors the TRUE permutation!\")\n",
    "        else:\n",
    "            print(f\"  → WARNING: Favors the WRONG permutation!\")\n",
    "    else:\n",
    "        print(f\"  ⚠ No significant difference in objectives (p > 0.05)\")\n",
    "        print(f\"  → Smoothness does NOT break the ambiguity!\")\n",
    "else:\n",
    "    print(\"Only one permutation found - selection appears consistent\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3add55",
   "metadata": {},
   "source": [
    "### Compare: Non-regularized vs Smoothness-regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0db65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top left: Selection rate comparison\n",
    "methods = ['No regularization', 'Smoothness (λ=1.0)']\n",
    "correct_rates = [n_correct/n_runs*100, n_correct_smooth/n_runs*100]\n",
    "swapped_rates = [n_swapped/n_runs*100, n_swapped_smooth/n_runs*100]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, correct_rates, width, label='Correct order', color='green', alpha=0.7)\n",
    "axes[0, 0].bar(x + width/2, swapped_rates, width, label='Swapped order', color='red', alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Percentage (%)')\n",
    "axes[0, 0].set_title('Selection Reliability Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(methods)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='Random chance')\n",
    "\n",
    "# Top right: Objective distributions (for smoothness)\n",
    "if total_correct and total_swapped:\n",
    "    axes[0, 1].hist(total_correct, bins=5, alpha=0.7, color='green', label='Correct order')\n",
    "    axes[0, 1].hist(total_swapped, bins=5, alpha=0.7, color='red', label='Swapped order')\n",
    "    axes[0, 1].legend()\n",
    "else:\n",
    "    axes[0, 1].hist(total_objs, bins=10, alpha=0.7, color='blue')\n",
    "axes[0, 1].set_xlabel('Total objective (smoothness)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Objective Distribution (λ=1.0)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom left: Scatter plot of objectives\n",
    "colors_smooth = ['green' if not r['is_swapped'] else 'red' for r in results_smooth]\n",
    "axes[1, 0].scatter(range(n_runs), total_objs, c=colors_smooth, s=100, alpha=0.7, marker='o', label='Smooth')\n",
    "axes[1, 0].axhline(y=np.mean(total_objs), color='blue', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[1, 0].set_xlabel('Run number')\n",
    "axes[1, 0].set_ylabel('Total objective')\n",
    "axes[1, 0].set_title('Objective by Run (λ=1.0)')\n",
    "axes[1, 0].legend(['Mean', 'Correct order', 'Swapped order'])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom right: Summary statistics table\n",
    "summary_data = [\n",
    "    ['', 'No Reg', 'λ=1.0'],\n",
    "    ['Correct %', f'{n_correct/n_runs*100:.0f}%', f'{n_correct_smooth/n_runs*100:.0f}%'],\n",
    "    ['Swapped %', f'{n_swapped/n_runs*100:.0f}%', f'{n_swapped_smooth/n_runs*100:.0f}%'],\n",
    "    ['Mean Obj', f'{np.mean(errors):.4f}', f'{np.mean(total_objs):.4f}'],\n",
    "    ['Std Obj', f'{np.std(errors):.4f}', f'{np.std(total_objs):.4f}']\n",
    "]\n",
    "\n",
    "axes[1, 1].axis('tight')\n",
    "axes[1, 1].axis('off')\n",
    "table = axes[1, 1].table(cellText=summary_data, cellLoc='center', loc='center',\n",
    "                          colWidths=[0.3, 0.35, 0.35])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Color header row\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#40466e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color data rows alternating\n",
    "for i in range(1, len(summary_data)):\n",
    "    for j in range(3):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "axes[1, 1].set_title('Summary Statistics', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_pilot_smoothness_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a6f3c",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "**WITHOUT smoothness** (non-negativity only):\n",
    "- 40% correct, 60% swapped\n",
    "- Perfect ambiguity (p = 0.88)\n",
    "- Random initialization determines outcome\n",
    "\n",
    "**WITH smoothness** (λ = 1.0):\n",
    "- Results will show if smoothness breaks ambiguity\n",
    "- Compare selection rates, objectives, statistical significance\n",
    "- Answers: Does regularization help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f4bf6",
   "metadata": {},
   "source": [
    "## Part 5: Interpretation & Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**If most/all runs found correct order**:\n",
    "- Non-negativity constraint alone may be sufficient for this case\n",
    "- Simple ALS (without smoothness) already has some selection bias\n",
    "- Need to test harder cases (more overlap, similar components)\n",
    "\n",
    "**If runs split between permutations**:\n",
    "- Ambiguity exists at this level of overlap\n",
    "- Need to quantify objective differences\n",
    "- Test whether smoothness regularization improves selection\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Add smoothness regularization** to ALS\n",
    "   - Implement λ_C ||D²C||² term\n",
    "   - Test if this improves selection reliability\n",
    "\n",
    "2. **Test harder cases**:\n",
    "   - More overlap (30% separation)\n",
    "   - Similar SAXS profiles (harder to distinguish)\n",
    "   - Lower SNR (20, 10)\n",
    "\n",
    "3. **Compare with REGALS**:\n",
    "   - Install and test actual REGALS\n",
    "   - Compare selection reliability\n",
    "   - Document differences\n",
    "\n",
    "4. **Expand test matrix**:\n",
    "   - Systematic variation of overlap, SNR, similarity\n",
    "   - Build reliability map\n",
    "   - Identify high-risk scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28af092",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**This pilot notebook established**:\n",
    "\n",
    "✓ Synthetic data generation workflow  \n",
    "✓ Simple ALS implementation  \n",
    "✓ Permutation detection method  \n",
    "✓ Multi-start experimental protocol  \n",
    "✓ Statistical analysis framework  \n",
    "\n",
    "**Ready to scale up** to full study in [permutation_selection_reliability_study.md](permutation_selection_reliability_study.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
