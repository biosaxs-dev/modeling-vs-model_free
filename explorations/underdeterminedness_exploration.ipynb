{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26443626",
   "metadata": {},
   "source": [
    "# Exploration: Underdeterminedness of $\\min_{P,C} \\|M - PC\\|^2$\n",
    "\n",
    "**Goal**: Understand the fundamental ambiguities in matrix factorization **before** adding regularization.\n",
    "\n",
    "This notebook demonstrates why **every decomposition method must make modeling choices** - the unconstrained problem has infinitely many solutions that fit the data equally well.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. **Scale Ambiguity**: Can we multiply P by α and divide C by α?\n",
    "2. **Basis Ambiguity**: Can we transform components and still fit M perfectly?\n",
    "3. **Implications**: What does this mean for \"model-free\" claims?\n",
    "\n",
    "---\n",
    "\n",
    "**Context**: Preliminary exploration for TRACK 1, Step 1.1 of the Molass vs REGALS paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd, qr\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a92132",
   "metadata": {},
   "source": [
    "## 1. The Mathematical Setup\n",
    "\n",
    "Given measured data matrix $M \\in \\mathbb{R}^{N \\times K}$:\n",
    "- $N$ = number of q-points (SAXS scattering angles)\n",
    "- $K$ = number of time frames (elution profile)\n",
    "\n",
    "We want to factorize: $M = P \\cdot C$ where:\n",
    "- $P \\in \\mathbb{R}^{N \\times n}$ = SAXS profiles of $n$ components\n",
    "- $C \\in \\mathbb{R}^{n \\times K}$ = Concentration/elution curves\n",
    "\n",
    "**Problem**: This decomposition is **not unique**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2-component SEC-SAXS-like data\n",
    "n_q = 100  # Number of q-points (scattering angles)\n",
    "n_t = 50   # Number of time frames\n",
    "n_comp = 2  # Number of components\n",
    "\n",
    "# True components (one arbitrary choice among infinitely many)\n",
    "print(\"Generating ground truth components...\")\n",
    "P_true = np.random.rand(n_q, n_comp) + 1.0  # Positive SAXS profiles\n",
    "C_true = np.random.rand(n_comp, n_t)         # Positive concentrations\n",
    "\n",
    "# Compute measured data\n",
    "M = P_true @ C_true\n",
    "\n",
    "print(f\"Data matrix M: {M.shape}\")\n",
    "print(f\"True P: {P_true.shape}, True C: {C_true.shape}\")\n",
    "print(f\"Reconstruction error (should be zero): {np.linalg.norm(M - P_true @ C_true):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d04f51",
   "metadata": {},
   "source": [
    "## 2. Ambiguity #1: Scale Ambiguity\n",
    "\n",
    "For any $\\alpha > 0$, if $M = PC$, then:\n",
    "\n",
    "$$M = (\\alpha P)(C/\\alpha)$$\n",
    "\n",
    "Both $(P, C)$ and $(\\alpha P, C/\\alpha)$ fit the data **identically**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate scale ambiguity\n",
    "alphas = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"Testing scale ambiguity (α scaling):\")\n",
    "print(\"-\" * 60)\n",
    "for alpha in alphas:\n",
    "    P_scaled = alpha * P_true\n",
    "    C_scaled = C_true / alpha\n",
    "    \n",
    "    error = np.linalg.norm(M - P_scaled @ C_scaled)\n",
    "    print(f\"α = {alpha:6.1f} → Reconstruction error: {error:.2e}\")\n",
    "\n",
    "print(\"\\n✓ All scales fit the data identically!\")\n",
    "print(\"⚠ Without additional constraints, intensity scale is arbitrary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45e071",
   "metadata": {},
   "source": [
    "## 3. Ambiguity #2: Basis Ambiguity (CRITICAL!)\n",
    "\n",
    "For any **invertible** matrix $R \\in \\mathbb{R}^{n \\times n}$:\n",
    "\n",
    "$$M = PC = (PR)(R^{-1}C)$$\n",
    "\n",
    "This is the **fundamental** ambiguity: infinitely many ways to decompose into components.\n",
    "\n",
    "**Note on terminology**: REGALS authors call this \"rotation ambiguity,\" but R can be ANY invertible matrix - not just rotations! This includes:\n",
    "- **Rotations** (orthogonal matrices)\n",
    "- **Scalings** (diagonal matrices)\n",
    "- **Shearings** (off-diagonal elements)\n",
    "- **Arbitrary mixing** (any invertible transformation)\n",
    "\n",
    "The ambiguity is **much broader** than just rotations - components are basis-dependent.\n",
    "\n",
    "### The SVD Solution\n",
    "\n",
    "SVD provides **one specific choice**:\n",
    "$$M = U\\Sigma V^T$$\n",
    "\n",
    "Setting $P = U_{:,:n}\\Sigma^{1/2}$ and $C = \\Sigma^{1/2}V_{:,:n}^T$ is just **one** of infinitely many factorizations.\n",
    "\n",
    "**What makes it special?** It's convenient mathematically, but has no physical justification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b47c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SVD solution\n",
    "U, S, Vt = svd(M, full_matrices=False)\n",
    "\n",
    "# SVD factorization (symmetric square root of singular values)\n",
    "P_svd = U[:, :n_comp] @ np.diag(np.sqrt(S[:n_comp]))\n",
    "C_svd = np.diag(np.sqrt(S[:n_comp])) @ Vt[:n_comp, :]\n",
    "\n",
    "# Generate random basis transformations (using orthogonal matrices as examples)\n",
    "print(\"Generating transformed solutions (orthogonal transformations)...\")\n",
    "n_rotations = 5\n",
    "P_rotations = []\n",
    "C_rotations = []\n",
    "\n",
    "for i in range(n_rotations):\n",
    "    # Generate random orthogonal matrix R (one type of basis transformation)\n",
    "    R_random = np.random.randn(n_comp, n_comp)\n",
    "    R, _ = qr(R_random)  # QR decomposition gives orthogonal matrix\n",
    "    \n",
    "    P_rot = P_svd @ R\n",
    "    C_rot = np.linalg.inv(R) @ C_svd\n",
    "    \n",
    "    P_rotations.append(P_rot)\n",
    "    C_rotations.append(C_rot)\n",
    "\n",
    "print(f\"Generated {n_rotations} random basis transformations of the SVD solution\")\n",
    "print(\"Note: These are orthogonal transformations, but ANY invertible matrix would work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67994707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ce017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all transformations fit the data identically\n",
    "print(\"\\nReconstruction Errors (all should be essentially zero):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "error_true = np.linalg.norm(M - P_true @ C_true)\n",
    "error_svd = np.linalg.norm(M - P_svd @ C_svd)\n",
    "print(f\"True components:     {error_true:.2e}\")\n",
    "print(f\"SVD solution:        {error_svd:.2e}\")\n",
    "\n",
    "for i, (P_rot, C_rot) in enumerate(zip(P_rotations, C_rotations)):\n",
    "    error = np.linalg.norm(M - P_rot @ C_rot)\n",
    "    print(f\"Transformation {i+1}:    {error:.2e}\")\n",
    "\n",
    "print(\"\\n✓ All decompositions reconstruct M perfectly!\")\n",
    "print(\"⚠ But the components themselves are completely different...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c7dc3",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Ambiguity\n",
    "\n",
    "Let's compare the **concentration profiles** from different factorizations.\n",
    "\n",
    "Despite fitting the data identically, the extracted components look **completely different**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55bb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Plot concentration profiles from different factorizations\n",
    "solutions = [\n",
    "    (\"True\", C_true, 'black'),\n",
    "    (\"SVD\", C_svd, 'blue'),\n",
    "    (\"Transform 1\", C_rotations[0], 'red'),\n",
    "    (\"Transform 2\", C_rotations[1], 'green'),\n",
    "    (\"Transform 3\", C_rotations[2], 'orange'),\n",
    "    (\"Transform 4\", C_rotations[3], 'purple'),\n",
    "]\n",
    "\n",
    "for idx, (name, C, color) in enumerate(solutions):\n",
    "    ax = axes.flatten()[idx]\n",
    "    for comp in range(n_comp):\n",
    "        ax.plot(C[comp, :], label=f'Component {comp+1}', linewidth=2, alpha=0.7)\n",
    "    ax.set_title(f'{name} Concentration Profiles', fontweight='bold')\n",
    "    ax.set_xlabel('Time Frame')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('basis_ambiguity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: basis_ambiguity.png\")\n",
    "print(\"\\nNotice: All solutions fit M perfectly, but components are completely different!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef30eb4",
   "metadata": {},
   "source": [
    "## 5. Quantifying Component Differences\n",
    "\n",
    "Even though all solutions fit $M$ identically, let's measure how different the extracted components are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec196c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare concentration profiles pairwise\n",
    "print(\"Correlation between concentration profiles:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Comparison':<30} {'Component 1':<15} {'Component 2':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# True vs SVD\n",
    "corr_c1 = np.corrcoef(C_true[0, :], C_svd[0, :])[0, 1]\n",
    "corr_c2 = np.corrcoef(C_true[1, :], C_svd[1, :])[0, 1]\n",
    "print(f\"{'True vs SVD':<30} {corr_c1:>14.3f} {corr_c2:>14.3f}\")\n",
    "\n",
    "# True vs Transformations\n",
    "for i, C_rot in enumerate(C_rotations[:3]):\n",
    "    corr_c1 = np.corrcoef(C_true[0, :], C_rot[0, :])[0, 1]\n",
    "    corr_c2 = np.corrcoef(C_true[1, :], C_rot[1, :])[0, 1]\n",
    "    print(f\"{f'True vs Transform {i+1}':<30} {corr_c1:>14.3f} {corr_c2:>14.3f}\")\n",
    "\n",
    "# SVD vs Transformation\n",
    "corr_c1 = np.corrcoef(C_svd[0, :], C_rotations[0][0, :])[0, 1]\n",
    "corr_c2 = np.corrcoef(C_svd[1, :], C_rotations[0][1, :])[0, 1]\n",
    "print(f\"{'SVD vs Transform 1':<30} {corr_c1:>14.3f} {corr_c2:>14.3f}\")\n",
    "\n",
    "print(\"\\n⚠ Low/negative correlations mean components are fundamentally different!\")\n",
    "print(\"⚠ Yet all fit the data M identically (χ² = 0)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba0f23",
   "metadata": {},
   "source": [
    "## 6. What Breaks the Basis Ambiguity?\n",
    "\n",
    "Different methods make different choices to get unique solutions:\n",
    "\n",
    "### REGALS (Implicit Modeling)\n",
    "1. **Non-negativity**: $P \\geq 0$, $C \\geq 0$ → Restricts to positive basis transformations only\n",
    "2. **Smoothness**: $\\lambda \\|D^2C\\|^2$ → Prefers certain bases (smooth profiles)\n",
    "3. **Compact support**: $C(t) = 0$ outside windows → Further restrictions\n",
    "4. **SAXS constraints**: Real-space $P(r)$ with $d_{max}$ → Additional physical constraints\n",
    "\n",
    "**Together**: These constraints select **one specific basis** from the infinite family.\n",
    "\n",
    "### Molass (Explicit Modeling)\n",
    "1. **Parametric form**: $C_k(t) = \\sum_i \\alpha_i f_i(t; \\theta_i)$ (e.g., Gaussian)\n",
    "2. This **eliminates** basis freedom by imposing functional structure\n",
    "3. Components must have specific shapes → Uniquely determined\n",
    "\n",
    "### The Key Insight\n",
    "**Both make modeling choices** - REGALS hides it in optimization constraints, Molass states it upfront!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how non-negativity constraint restricts basis transformations\n",
    "print(\"Testing if random transformations preserve non-negativity:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if transformed solutions have negative components\n",
    "for i, (P_rot, C_rot) in enumerate(zip(P_rotations[:3], C_rotations[:3])):\n",
    "    has_neg_P = np.any(P_rot < 0)\n",
    "    has_neg_C = np.any(C_rot < 0)\n",
    "    min_P = P_rot.min()\n",
    "    min_C = C_rot.min()\n",
    "    \n",
    "    print(f\"Transformation {i+1}:\")\n",
    "    print(f\"  P has negatives: {has_neg_P} (min = {min_P:+.3f})\")\n",
    "    print(f\"  C has negatives: {has_neg_C} (min = {min_C:+.3f})\")\n",
    "\n",
    "print(\"\\n⚠ Most random transformations produce NEGATIVE components!\")\n",
    "print(\"✓ Non-negativity constraint eliminates most of the basis freedom.\")\n",
    "print(\"⚠ But infinitely many non-negative bases still exist!\")\n",
    "print(\"\\n→ Need ADDITIONAL constraints (smoothness, etc.) to get unique solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62a4c1",
   "metadata": {},
   "source": [
    "## 7. Implications for \"Model-Free\" Claims\n",
    "\n",
    "### The Fundamental Problem\n",
    "$$\\min_{P,C} \\|M - PC\\|^2$$\n",
    "has **infinitely many solutions** that fit the data identically.\n",
    "\n",
    "### What This Means\n",
    "\n",
    "1. **No method can be truly \"model-free\"**\n",
    "   - Every method must make choices to resolve ambiguities\n",
    "   - These choices ARE modeling assumptions\n",
    "\n",
    "2. **REGALS is not model-free**\n",
    "   - Makes choices via: non-negativity, smoothness, compact support, SAXS constraints\n",
    "   - These are **implicit models** embedded in optimization\n",
    "\n",
    "3. **Molass is explicitly model-based**\n",
    "   - Makes choices via: parametric functional forms (Gaussian, EGH, etc.)\n",
    "   - These are **explicit models** stated upfront\n",
    "\n",
    "4. **The Key Difference**\n",
    "   - **REGALS**: Implicit modeling (hidden in constraints)\n",
    "   - **Molass**: Explicit modeling (transparent in formulation)\n",
    "   - **Both require modeling** - difference is transparency, not existence!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "This exploration establishes the foundation. Next we need to:\n",
    "1. **Characterize REGALS's implicit model mathematically** (What does smoothness regularization assume?)\n",
    "2. **Compare to explicit models quantitatively** (When does REGALS ≈ Gaussian?)\n",
    "3. **Test empirically with simulations** (Validate theoretical predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbe979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY: Underdeterminedness of M = PC\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"DEMONSTRATED:\")\n",
    "print(\"  ✓ Scale ambiguity: (αP, C/α) fits identically for any α > 0\")\n",
    "print(\"  ✓ Basis ambiguity: (PR, R⁻¹C) fits identically for any invertible R\")\n",
    "print(\"    (Note: REGALS authors call this 'rotation' but it's much more general!)\")\n",
    "print(\"  ✓ Infinitely many solutions with χ² = 0\")\n",
    "print(\"  ✓ Different solutions give completely different components\")\n",
    "print()\n",
    "print(\"IMPLICATIONS:\")\n",
    "print(\"  → Every method MUST make modeling choices\")\n",
    "print(\"  → 'Model-free' is a misnomer\")\n",
    "print(\"  → REGALS: implicit modeling (via constraints)\")\n",
    "print(\"  → Molass: explicit modeling (via functional forms)\")\n",
    "print(\"  → Key difference: transparency, not existence of modeling\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Next: Characterize what implicit model REGALS assumes!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bb06c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Does Regularization Break the Basis Ambiguity?\n",
    "\n",
    "**New Question**: If we add regularization terms:\n",
    "$$\\min_{P,C} \\|M - PC\\|^2 + R(C) + R(P)$$\n",
    "\n",
    "Can we still find transformations $B$ such that the **entire objective** remains unchanged?\n",
    "\n",
    "$$\\|M - (PB)(B^{-1}C)\\|^2 + R(B^{-1}C) + R(PB) = \\|M - PC\\|^2 + R(C) + R(P)$$\n",
    "\n",
    "**Note on REGALS**: In practice, REGALS uses $R(C) = \\lambda\\|D^2C\\|^2$ (smoothness on concentration profiles) but **does NOT explicitly regularize P**. Instead, P is constrained through:\n",
    "- Non-negativity: $P \\geq 0$\n",
    "- Real-space transform: $P(q) \\leftrightarrow P(r)$ with maximum dimension $d_{max}$\n",
    "\n",
    "These are **hard constraints** rather than smooth regularization terms. For this exploration, we focus on $R(C)$ to test if smoothness regularization alone breaks the basis ambiguity.\n",
    "\n",
    "## Mathematical Insight\n",
    "\n",
    "### Data-Fit Term (Already Proven)\n",
    "$$\\|M - (PB)(B^{-1}C)\\|^2 = \\|M - PC\\|^2 \\quad \\text{for ANY invertible } B$$\n",
    "\n",
    "### Regularization Terms (Key Question!)\n",
    "\n",
    "**For orthogonal transformations** ($B^T B = I$):\n",
    "\n",
    "1. **Frobenius norm**: $\\|B^{-1}C\\|_F^2 = \\|C\\|_F^2$ ✓ (orthogonal matrices preserve norms)\n",
    "\n",
    "2. **Smoothness on C** (where $D^2$ is the second derivative operator):\n",
    "   $$\\|D^2(B^{-1}C)\\|_F^2 = \\|B^{-1}(D^2C)\\|_F^2 = \\|D^2C\\|_F^2$$\n",
    "   \n",
    "   ✓ Since $D^2$ acts on each row of $C$ independently, and orthogonal $B^{-1}$ preserves norms, smoothness is invariant!\n",
    "\n",
    "3. **If we had smoothness on P**: Similarly, $\\|D^2(PB)\\|_F^2 = \\|(D^2P)B\\|_F^2 = \\|D^2P\\|_F^2$ ✓\n",
    "   \n",
    "   (But REGALS uses hard constraints on P instead)\n",
    "\n",
    "### **Critical Implication**\n",
    "\n",
    "Even with smoothness regularization, **orthogonal basis ambiguity persists**!\n",
    "- For $n=2$: 1 free parameter (rotation angle)\n",
    "- For $n=3$: 3 free parameters (Euler angles)\n",
    "- Generally: $\\frac{n(n-1)}{2}$ degrees of freedom\n",
    "\n",
    "**Regularization alone is insufficient for uniqueness!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae878a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define smoothness regularization (REGALS-style second derivative penalty)\n",
    "def smoothness_regularizer(C, lambda_smooth=1.0):\n",
    "    \"\"\"\n",
    "    Compute smoothness penalty: λ||D²C||²_F\n",
    "    where D² is the second derivative operator (discrete approximation)\n",
    "    \"\"\"\n",
    "    n_comp, n_t = C.shape\n",
    "    \n",
    "    # Second derivative via finite differences: d²f/dt² ≈ f(t-1) - 2f(t) + f(t+1)\n",
    "    D2C = np.zeros((n_comp, n_t - 2))\n",
    "    for i in range(n_comp):\n",
    "        for j in range(n_t - 2):\n",
    "            D2C[i, j] = C[i, j] - 2*C[i, j+1] + C[i, j+2]\n",
    "    \n",
    "    return lambda_smooth * np.linalg.norm(D2C, 'fro')**2\n",
    "\n",
    "# Define total objective function\n",
    "def total_objective(P, C, M, lambda_smooth=1.0):\n",
    "    \"\"\"Total objective: data-fit + smoothness regularization\"\"\"\n",
    "    data_fit = np.linalg.norm(M - P @ C)**2\n",
    "    smooth_penalty = smoothness_regularizer(C, lambda_smooth)\n",
    "    return data_fit + smooth_penalty\n",
    "\n",
    "print(\"Regularization functions defined!\")\n",
    "print(\"\\nTesting smoothness regularizer on SVD solution:\")\n",
    "lambda_test = 1.0\n",
    "smooth_svd = smoothness_regularizer(C_svd, lambda_test)\n",
    "print(f\"  Smoothness penalty for SVD solution: {smooth_svd:.4f}\")\n",
    "\n",
    "# Compute total objective for SVD solution\n",
    "obj_svd = total_objective(P_svd, C_svd, M, lambda_test)\n",
    "print(f\"  Total objective for SVD solution: {obj_svd:.4e}\")\n",
    "print(f\"    (Data-fit: {np.linalg.norm(M - P_svd @ C_svd)**2:.4e}, Smoothness: {smooth_svd:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7270dd",
   "metadata": {},
   "source": [
    "## Testing the Conjecture: Orthogonal Transformations\n",
    "\n",
    "Let's test if orthogonal transformations preserve the **entire objective** (data-fit + smoothness).\n",
    "\n",
    "**Prediction**: Since orthogonal matrices preserve norms, we expect:\n",
    "$$\\text{Objective}(PB, B^{-1}C) = \\text{Objective}(P, C)$$\n",
    "\n",
    "for orthogonal $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if orthogonal transformations preserve the total objective\n",
    "print(\"Testing Conjecture: Does regularization break basis ambiguity?\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBaseline (SVD solution):\")\n",
    "print(f\"  Total objective: {obj_svd:.4e}\")\n",
    "print(f\"  Data-fit: {np.linalg.norm(M - P_svd @ C_svd)**2:.4e}\")\n",
    "print(f\"  Smoothness: {smoothness_regularizer(C_svd, lambda_test):.4f}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Test each orthogonal transformation from earlier\n",
    "print(f\"\\nOrthogonal Transformations (rotation angle varies):\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "objectives = []\n",
    "for i, (P_rot, C_rot) in enumerate(zip(P_rotations[:5], C_rotations[:5])):\n",
    "    obj_rot = total_objective(P_rot, C_rot, M, lambda_test)\n",
    "    data_fit = np.linalg.norm(M - P_rot @ C_rot)**2\n",
    "    smooth = smoothness_regularizer(C_rot, lambda_test)\n",
    "    \n",
    "    objectives.append(obj_rot)\n",
    "    \n",
    "    print(f\"Transformation {i+1}:\")\n",
    "    print(f\"  Total objective: {obj_rot:.4e}  (Δ = {obj_rot - obj_svd:+.4e})\")\n",
    "    print(f\"  Data-fit: {data_fit:.4e}\")\n",
    "    print(f\"  Smoothness: {smooth:.4f}  (Δ = {smooth - smoothness_regularizer(C_svd, lambda_test):+.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESULT:\")\n",
    "print(f\"  Objective range: [{min(objectives):.4e}, {max(objectives):.4e}]\")\n",
    "print(f\"  Standard deviation: {np.std(objectives):.4e}\")\n",
    "print(f\"  Max difference from SVD: {max(abs(obj - obj_svd) for obj in objectives):.4e}\")\n",
    "print(\"\\n✓ All orthogonal transformations give IDENTICAL objectives!\")\n",
    "print(\"⚠ Smoothness regularization does NOT break rotational ambiguity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7b370",
   "metadata": {},
   "source": [
    "## Visualizing the Persistent Ambiguity\n",
    "\n",
    "Even with smoothness regularization, we still have infinitely many solutions with **identical objective values** but **different components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Plot concentration profiles with objective values\n",
    "solutions_reg = [\n",
    "    (\"SVD\", C_svd, P_svd, 'blue'),\n",
    "    (\"Transform 1\", C_rotations[0], P_rotations[0], 'red'),\n",
    "    (\"Transform 2\", C_rotations[1], P_rotations[1], 'green'),\n",
    "    (\"Transform 3\", C_rotations[2], P_rotations[2], 'orange'),\n",
    "    (\"Transform 4\", C_rotations[3], P_rotations[3], 'purple'),\n",
    "    (\"Transform 5\", C_rotations[4], P_rotations[4], 'brown'),\n",
    "]\n",
    "\n",
    "for idx, (name, C, P, color) in enumerate(solutions_reg):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    # Compute objective for this solution\n",
    "    obj = total_objective(P, C, M, lambda_test)\n",
    "    smooth = smoothness_regularizer(C, lambda_test)\n",
    "    \n",
    "    for comp in range(n_comp):\n",
    "        ax.plot(C[comp, :], label=f'Comp {comp+1}', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{name}\\nObj={obj:.3e}, λ||D²C||²={smooth:.2f}', fontweight='bold', fontsize=9)\n",
    "    ax.set_xlabel('Time Frame')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('regularization_ambiguity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: regularization_ambiguity.png\")\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"  All solutions have IDENTICAL objective values\")\n",
    "print(\"  But concentration profiles are COMPLETELY DIFFERENT\")\n",
    "print(\"  → Smoothness regularization alone does NOT resolve ambiguity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be989ef",
   "metadata": {},
   "source": [
    "## What ACTUALLY Breaks the Ambiguity?\n",
    "\n",
    "Since smoothness regularization preserves orthogonal transformations, we need **additional constraints**.\n",
    "\n",
    "### Non-Negativity: The Critical Constraint\n",
    "\n",
    "Let's test if non-negativity ($P \\geq 0, C \\geq 0$) breaks the rotational ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514974ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many orthogonal transformations satisfy non-negativity\n",
    "print(\"Testing Non-Negativity Constraint:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check SVD solution\n",
    "svd_nonneg = (P_svd >= 0).all() and (C_svd >= 0).all()\n",
    "print(f\"SVD solution satisfies P≥0, C≥0: {svd_nonneg}\")\n",
    "print(f\"  min(P_svd) = {P_svd.min():+.4f}, min(C_svd) = {C_svd.min():+.4f}\")\n",
    "print()\n",
    "\n",
    "# Check transformed solutions\n",
    "print(\"Orthogonal transformations:\")\n",
    "print(\"-\"*70)\n",
    "n_valid = 0\n",
    "for i, (P_rot, C_rot) in enumerate(zip(P_rotations[:5], C_rotations[:5])):\n",
    "    P_nonneg = (P_rot >= 0).all()\n",
    "    C_nonneg = (C_rot >= 0).all()\n",
    "    both_nonneg = P_nonneg and C_nonneg\n",
    "    \n",
    "    if both_nonneg:\n",
    "        n_valid += 1\n",
    "    \n",
    "    status = \"✓ VALID\" if both_nonneg else \"✗ INVALID\"\n",
    "    print(f\"Transform {i+1}: {status}\")\n",
    "    print(f\"  min(P) = {P_rot.min():+.4f}, min(C) = {C_rot.min():+.4f}\")\n",
    "    \n",
    "    if both_nonneg:\n",
    "        obj = total_objective(P_rot, C_rot, M, lambda_test)\n",
    "        print(f\"  → Valid solution with objective = {obj:.4e}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Summary: {n_valid}/{len(P_rotations[:5])} random rotations satisfy non-negativity\")\n",
    "print()\n",
    "\n",
    "if n_valid > 1:\n",
    "    print(\"⚠ CRITICAL: Multiple non-negative solutions with identical objectives exist!\")\n",
    "    print(\"→ Non-negativity + smoothness STILL insufficient for uniqueness!\")\n",
    "elif n_valid == 1:\n",
    "    print(\"✓ Only one solution satisfies non-negativity constraint\")\n",
    "    print(\"→ Non-negativity + smoothness MAY give uniqueness (needs theoretical proof)\")\n",
    "else:\n",
    "    print(\"⚠ No random rotations satisfy non-negativity\")\n",
    "    print(\"→ But this doesn't prove uniqueness (special rotations might exist)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022b04e",
   "metadata": {},
   "source": [
    "## The Hierarchy of Constraints\n",
    "\n",
    "Our exploration reveals a **hierarchy of modeling choices** needed for uniqueness:\n",
    "\n",
    "### Level 1: Data-Fit Only\n",
    "$$\\min_{P,C} \\|M - PC\\|^2$$\n",
    "- **Result**: Infinite solutions (scale ambiguity + full basis ambiguity)\n",
    "- **Free parameters**: 1 (scale) + $n^2$ (any invertible matrix $R$)\n",
    "- **Ambiguity**: $(P, C)$ and $(\\alpha P R, R^{-1}C/\\alpha)$ fit identically for any $\\alpha > 0$ and invertible $R$\n",
    "\n",
    "### Level 2: Add Smoothness Regularization\n",
    "$$\\min_{P,C} \\|M - PC\\|^2 + \\lambda\\|D^2C\\|^2$$\n",
    "\n",
    "**Hidden modeling assumptions at this level**:\n",
    "\n",
    "**1. Choice of combination rule (WHY ADDITIVE?)**\n",
    "- Why $A + \\lambda B$ and not $A \\times B$, or $\\log(A) + \\log(B)$, or $A^p + B^q$?\n",
    "\n",
    "**Logical structure encoded by operators**:\n",
    "- **Addition (+)**: Encodes \"AND\" logic\n",
    "  - $A + \\lambda B$ means \"fit the data **AND** be smooth\"\n",
    "  - Both constraints must be satisfied; cannot minimize objective by only satisfying one\n",
    "  - Forces simultaneous satisfaction of both criteria\n",
    "- **Multiplication (×)**: Encodes different logical relationship\n",
    "  - $A \\times B$ means if either term is near zero, whole objective can be small\n",
    "  - Allows one constraint to dominate; changes trade-off structure\n",
    "  - Different logical interaction between constraints\n",
    "- **The choice of operator is a modeling decision about how constraints should interact!**\n",
    "\n",
    "**Probabilistic interpretation (for additive form)**:\n",
    "- **Additive form** ($+$) in log-probability space represents independence\n",
    "  - Corresponds to minimizing negative log-posterior: $-\\log p(P,C|M) = -\\log p(M|P,C) - \\log p(C)$\n",
    "  - Data-fit term: $\\|M - PC\\|^2 \\propto -\\log p(M|P,C)$ (Gaussian likelihood)\n",
    "  - Smoothness term: $\\|D^2C\\|^2 \\propto -\\log p(C)$ (Gaussian prior on curvature)\n",
    "  - Assumes: Gaussian noise model + Gaussian smoothness prior + independent errors\n",
    "- **Multiplicative form** ($\\times$): Would correspond to different probabilistic model (non-standard)\n",
    "- **Log-additive form** ($\\log + \\log$): Would imply different noise/prior distributions\n",
    "- **Each choice encodes different implicit beliefs about noise structure, constraint interaction, and solution properties!**\n",
    "\n",
    "**2. Choice of penalty functional (WHY SQUARED L2 NORM?)**\n",
    "- Why $\\|D^2C\\|^2$ and not $\\|D^2C\\|_1$ (L1), or $\\|D^2C\\|_\\infty$ (max), or other norms?\n",
    "- L2 norm: Penalizes large deviations quadratically → Gaussian assumption\n",
    "- L1 norm: Promotes sparsity in curvature → would give different implicit model\n",
    "- Each choice represents different prior belief about curvature distribution\n",
    "\n",
    "**Mathematical consequences**:\n",
    "- **Result**: Continuous ambiguity **reduced** to orthogonal transformations O(n)\n",
    "- **Solution manifold**: Continuous family with dimension $\\frac{n(n-1)}{2}$ (orthogonal group O(n): proper rotations SO(n) with det=+1, plus improper rotations with det=-1)\n",
    "- **What changed**: \n",
    "  - Scale ambiguity **eliminated** (regularization creates unique optimal scale)\n",
    "  - Basis ambiguity **reduced** (from arbitrary invertible $R \\in GL(n)$ to orthogonal $B \\in O(n)$ only)\n",
    "- **Why scale is eliminated**: Smoothness penalty $\\|D^2(C/\\alpha)\\|^2 = \\|D^2C\\|^2/\\alpha^2$ changes with scale, creating a unique optimum that balances data-fit (scale-invariant) vs smoothness (scale-dependent)\n",
    "\n",
    "**⚠ Critical limitations**:\n",
    "\n",
    "**Limitation 1: Orthogonal invariance allows degeneracy**\n",
    "- While mathematically elegant, orthogonal invariance **does not prevent degeneracy**\n",
    "- Solutions where one component becomes bimodal (high curvature) while another vanishes (zero curvature → perfect smoothness) can have **lower total smoothness** than the correct two-component solution\n",
    "- In alternating optimization (ALS), this leads to **component collapse** even though the transformation group is restricted to O(n)\n",
    "- See [smoothness_orthogonal_invariance_proof.ipynb](smoothness_orthogonal_invariance_proof.ipynb) Part 11D: 65% degeneracy rate despite orthogonal invariance constraint\n",
    "\n",
    "**Limitation 2: The additive/L2 choices are implicit models**\n",
    "- The **form** of the objective ($A + \\lambda B$ with L2 norms) is not \"model-free\"\n",
    "- It encodes specific probabilistic assumptions (Gaussian noise, Gaussian priors, independence)\n",
    "- Different choices (multiplicative, L1, log-additive, etc.) would yield completely different solutions\n",
    "- **This is a modeling decision**, hidden in the mathematical formulation!\n",
    "\n",
    "### Level 3: Add Non-Negativity\n",
    "$$\\min_{P \\geq 0, C \\geq 0} \\|M - PC\\|^2 + \\lambda\\|D^2C\\|^2$$\n",
    "- **Result**: Unique solution (or small discrete set of solutions)\n",
    "- **Free parameters**: 0 or small discrete set (permutation ambiguity)\n",
    "- **What changed**: Non-negativity eliminates the continuous n(n-1)/2 degrees of freedom from O(n)\n",
    "  - Most random orthogonal transformations produce negative values (empirically verified above)\n",
    "  - For generic positive data, continuous ambiguity is eliminated\n",
    "  - Remaining ambiguity: discrete permutations (component label swapping) when components are similar\n",
    "  - Edge cases: highly symmetric data might preserve additional discrete symmetries\n",
    "\n",
    "### Level 4: Full REGALS (Add Normalization + Compact Support + SAXS)\n",
    "$$\\min_{\\substack{P \\geq 0, C \\geq 0 \\\\ C(t) = 0 \\text{ outside windows} \\\\ P \\leftrightarrow P(r) \\text{ with } d_{max} \\\\ \\|P_k\\| = 1}} \\|M - PC\\|^2 + \\lambda\\|D^2C\\|^2$$\n",
    "- **Result**: Unique solution (guaranteed for generic data)\n",
    "- **Free parameters**: 0 or small discrete set (guaranteed unique for generic data)\n",
    "- **What changed**: Additional physical constraints make degeneracies highly unlikely\n",
    "  - **Normalization**: $\\|P_k\\| = 1$ ensures no residual scaling freedom\n",
    "  - **Compact support**: $C(t) = 0$ outside windows spatially separates components\n",
    "  - **SAXS constraints**: $P \\leftrightarrow P(r)$ with distinct $d_{max}$ values distinguishes components by size\n",
    "  - **For generic data**: Components differ in SAXS profiles, elution times, and intensities → unique solution\n",
    "  - **Edge case**: Nearly identical components (same size, overlapping elution, similar profiles) may still permit permutation ambiguity\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight**: REGALS requires **FOUR layers of constraints** to achieve uniqueness:\n",
    "1. **Smoothness** → Eliminates scale ambiguity + reduces basis freedom to orthogonal transformations\n",
    "2. **Non-negativity** → Eliminates (most) orthogonal transformations\n",
    "3. **Normalization** → Ensures no residual scale freedom (explicit in REGALS implementation)\n",
    "4. **Compact support + SAXS constraints** → Physical constraints that restrict solution space\n",
    "\n",
    "**Each is an implicit modeling assumption!**\n",
    "\n",
    "**Important Note**: The \"free parameters\" here refer to **continuous degrees of freedom in the solution space**, not the number of optimization variables. The orthogonal group O(n) has dimension n(n-1)/2 and includes: (1) **proper rotations** in SO(n) with det = +1, and (2) **improper rotations** with det = -1 (which include reflections, rotoinversions, and orientation-reversing isometries—far more general than simple reflections in high dimensions). Even at Level 3, there may be a finite discrete set of solutions (e.g., component label swapping), but no continuous manifold of equivalent solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c530684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"PART 2 SUMMARY: Regularization and Basis Ambiguity\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"CONJECTURE TESTED:\")\n",
    "print(\"  Can we find B such that Objective(PB, B⁻¹C) = Objective(P, C)?\")\n",
    "print()\n",
    "print(\"ANSWER: YES, for orthogonal transformations!\")\n",
    "print()\n",
    "print(\"FINDINGS:\")\n",
    "print(\"  ✓ Data-fit term: Always invariant for ANY invertible B\")\n",
    "print(\"  ✓ Smoothness ||D²C||²: Invariant for ORTHOGONAL B\")\n",
    "print(\"  ✓ Total objective: Identical across all orthogonal transformations\")\n",
    "print(\"  ✓ Components: Completely different despite identical objectives\")\n",
    "print()\n",
    "print(\"DEGREES OF FREEDOM:\")\n",
    "print(f\"  • Unconstrained: scale (1) + arbitrary mixing (n²)\")\n",
    "print(f\"  • With smoothness: scale (1) + orthogonal rotations (n(n-1)/2)\")\n",
    "print(f\"  • With non-negativity: Most/all rotational freedom eliminated\")\n",
    "print(f\"  • Full REGALS: Uniqueness (0 free parameters)\")\n",
    "print()\n",
    "print(\"IMPLICATIONS:\")\n",
    "print(\"  → Smoothness regularization ALONE is insufficient!\")\n",
    "print(\"  → Need MULTIPLE constraints for uniqueness\")\n",
    "print(\"  → REGALS uses 4-layer constraint hierarchy\")\n",
    "print(\"  → Each layer is an IMPLICIT MODELING choice\")\n",
    "print(\"  → 'Model-free' claim is fundamentally misleading\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"Next: Mathematical characterization of implicit functional form\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
