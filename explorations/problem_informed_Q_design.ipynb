{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2989f0",
   "metadata": {},
   "source": [
    "# Problem-Informed Q-Matrix Design for Orthogonal-Invariant Regularization\n",
    "\n",
    "**Date**: January 27, 2026  \n",
    "**Purpose**: Explore whether problem-specific knowledge can be incorporated into fixed Q-matrix design while maintaining orthogonal invariance\n",
    "\n",
    "**Context**: From [orthogonal_invariance_journey.md](orthogonal_invariance_journey.md) Open Research Question #3\n",
    "\n",
    "---\n",
    "\n",
    "## The Research Question\n",
    "\n",
    "From Parts 11A-D of [smoothness_orthogonal_invariance_proof.ipynb](smoothness_orthogonal_invariance_proof.ipynb), we learned:\n",
    "\n",
    "1. **Fixed Q maintains invariance**: $S(C) = \\text{tr}(CQC^T)$ ⟺ orthogonal invariance\n",
    "2. **Generic Q (ridge) insufficient**: $Q = (D^2)^T D^2 + \\epsilon I$ helps marginally but doesn't prevent degeneracy\n",
    "3. **Trade-off exists**: Invariance ↔ Degeneracy prevention\n",
    "\n",
    "**New Question**: Can we design $Q$ using **problem-class knowledge** to reduce degeneracy risk while maintaining invariance?\n",
    "\n",
    "### Key Distinction\n",
    "\n",
    "- **Generic Q** (e.g., ridge): Problem-agnostic, works for any data\n",
    "- **Problem-informed Q**: Incorporates priors (peak separation, widths, expected shapes) but **still fixed once designed**\n",
    "\n",
    "### Why This Might Work\n",
    "\n",
    "Standard smoothness $\\|D^2C\\|^2$ treats all curvature equally. But for SEC-SAXS:\n",
    "- **Expected**: Smooth Gaussian-like peaks at certain separations\n",
    "- **Degenerate**: Bimodal profile with unexpected peak locations\n",
    "\n",
    "**Idea**: Design $Q$ to penalize \"unexpected\" curvature more heavily!\n",
    "\n",
    "---\n",
    "\n",
    "## Three Approaches to Explore\n",
    "\n",
    "### 1. Spatially-Weighted Smoothness\n",
    "\n",
    "$$Q = (D^2)^T W D^2$$\n",
    "\n",
    "where $W$ is diagonal with higher weights in \"unexpected\" regions.\n",
    "\n",
    "**Example**: If components expected at frames 35 and 55:\n",
    "```\n",
    "W_ii = 1 + γ·min(|i-35|, |i-55|) / frame_range\n",
    "```\n",
    "- Low penalty near expected peaks (allow curvature)\n",
    "- High penalty in between (discourage bimodal spanning both)\n",
    "\n",
    "### 2. Band-Pass Frequency Filter\n",
    "\n",
    "$$Q = F^T \\Lambda F$$\n",
    "\n",
    "where $F$ is Fourier/DCT transform, $\\Lambda$ diagonal frequency weights.\n",
    "\n",
    "**Example**: For Gaussian peaks with width σ=5:\n",
    "```\n",
    "Λ_kk = exp(-(ω_k - ω_peak)² / (2σ_freq²))\n",
    "```\n",
    "- Allow frequencies corresponding to expected peak widths\n",
    "- Penalize very low frequencies (flat/constant) and very high (noise/oscillations)\n",
    "\n",
    "### 3. Size-Informed Smoothness Scaling\n",
    "\n",
    "$$Q = (D^2)^T D^2 + \\beta \\text{diag}(\\text{temporal\\_weights})$$\n",
    "\n",
    "where temporal weights based on SEC physics:\n",
    "```\n",
    "w(t) = expected_concentration(t | known_sizes)\n",
    "```\n",
    "\n",
    "**Example**: Larger particles (early elution) → narrower peaks → less allowed curvature in late frames\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "We'll test these approaches using the same setup as Part 11D (ALS with both P and C optimized):\n",
    "\n",
    "1. **Baseline**: Standard smoothness (0% success from Part 11D)\n",
    "2. **Ridge**: Generic improvement (35% → 70% from Part 11D)\n",
    "3. **Spatially-weighted**: Test if prior knowledge helps\n",
    "4. **Band-pass**: Test frequency-domain approach\n",
    "5. **Size-informed**: Test SEC-physics-based weighting\n",
    "\n",
    "**Goal**: Can problem-informed Q achieve >80% success while maintaining invariance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm as scipy_norm\n",
    "from scipy.fft import dct, idct\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08403a88",
   "metadata": {},
   "source": [
    "## Part 1: Reproduce Test Setup from Part 11D\n",
    "\n",
    "Use the same data generation as the ALS test that showed degeneracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda93b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data (same as Part 11D)\n",
    "n_components = 2\n",
    "n_frames = 100\n",
    "frames = np.arange(n_frames)\n",
    "\n",
    "# True concentration profiles (two separated Gaussian peaks)\n",
    "C_true = np.zeros((n_components, n_frames))\n",
    "C_true[0, :] = scipy_norm.pdf(frames, loc=35, scale=4)  # Component 1: frame 35, σ=4\n",
    "C_true[1, :] = scipy_norm.pdf(frames, loc=55, scale=6)  # Component 2: frame 55, σ=6\n",
    "C_true = C_true / C_true.sum(axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# SAXS profiles (Guinier-Porod-like decay, correlated)\n",
    "q = np.linspace(0.01, 0.3, 50)\n",
    "P_true = np.zeros((2, 50))\n",
    "P_true[0, :] = np.exp(-0.5 * (q * 40)**2 / 3)  # Larger particle (Rg=40Å)\n",
    "P_true[1, :] = np.exp(-0.5 * (q * 20)**2 / 3) * 0.5  # Smaller particle (Rg=20Å)\n",
    "\n",
    "# Generate data\n",
    "M = P_true.T @ C_true\n",
    "\n",
    "# Correlation (this is what causes degeneracy)\n",
    "correlation = np.corrcoef(P_true[0, :], P_true[1, :])[0, 1]\n",
    "\n",
    "print(f\"Test setup:\")\n",
    "print(f\"  Components: {n_components}\")\n",
    "print(f\"  Frames: {n_frames}\")\n",
    "print(f\"  Peak separation: {55-35} frames\")\n",
    "print(f\"  Peak widths: σ₁={4}, σ₂={6}\")\n",
    "print(f\"  SAXS correlation: r = {correlation:.3f}\")\n",
    "print(f\"  Data shape: {M.shape}\")\n",
    "print()\n",
    "print(\"✓ This is the same setup that showed 100% degeneracy in Part 11D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f8b98",
   "metadata": {},
   "source": [
    "## Part 2: ALS Implementation with Custom Q-Matrix\n",
    "\n",
    "Generalize the ALS implementation to accept arbitrary Q-matrix for smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_with_custom_Q(M, k, Q, epsilon_ridge=0, max_iter=100, tol=1e-6, random_seed=None):\n",
    "    \"\"\"\n",
    "    Alternating Least Squares with custom Q-matrix regularization.\n",
    "    \n",
    "    Objective: ||M - P^T C||² + tr(CQC^T) + ε||C||²\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M : array (n_q, K)\n",
    "        Data matrix\n",
    "    k : int\n",
    "        Number of components\n",
    "    Q : array (K, K)\n",
    "        Positive semi-definite regularization matrix\n",
    "    epsilon_ridge : float\n",
    "        Ridge regularization parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    P : array (n_q, k)\n",
    "        SAXS profiles\n",
    "    C : array (k, K)\n",
    "        Concentration profiles\n",
    "    history : dict\n",
    "        Optimization history\n",
    "    \"\"\"\n",
    "    n_q, K = M.shape\n",
    "    \n",
    "    # Random initialization\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    C = np.random.rand(k, K)\n",
    "    C = C / C.sum(axis=1, keepdims=True)\n",
    "    P = np.random.rand(k, n_q).T\n",
    "    \n",
    "    history = {'data_fit': [], 'smoothness': [], 'ridge': [], 'total': []}\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        C_old = C.copy()\n",
    "        \n",
    "        # Update C (fix P)\n",
    "        for j in range(k):\n",
    "            pj = P[:, j]\n",
    "            pj_norm_sq = np.dot(pj, pj)\n",
    "            \n",
    "            # RHS\n",
    "            residual_j = M.T @ pj\n",
    "            for j_other in range(k):\n",
    "                if j_other != j:\n",
    "                    residual_j -= pj_norm_sq * C[j_other, :]\n",
    "            \n",
    "            # Solve: (p_j^T p_j I + Q + ε I) c_j = RHS\n",
    "            A = pj_norm_sq * np.eye(K) + Q + epsilon_ridge * np.eye(K)\n",
    "            b = residual_j\n",
    "            \n",
    "            C[j, :] = np.linalg.solve(A, b)\n",
    "            C[j, :] = np.maximum(C[j, :], 0)  # Non-negativity\n",
    "        \n",
    "        # Update P (fix C)\n",
    "        for i in range(n_q):\n",
    "            mi = M[i, :]\n",
    "            CtC = C @ C.T\n",
    "            Ctm = C @ mi\n",
    "            CtC_reg = CtC + 1e-10 * np.eye(k)\n",
    "            P[i, :] = np.linalg.solve(CtC_reg, Ctm)\n",
    "            P[i, :] = np.maximum(P[i, :], 0)\n",
    "        \n",
    "        # Compute objective\n",
    "        M_recon = P @ C\n",
    "        data_fit = np.linalg.norm(M - M_recon, 'fro')**2\n",
    "        smoothness = np.trace(C @ Q @ C.T)\n",
    "        ridge = np.linalg.norm(C, 'fro')**2\n",
    "        total_obj = data_fit + smoothness + epsilon_ridge * ridge\n",
    "        \n",
    "        history['data_fit'].append(data_fit)\n",
    "        history['smoothness'].append(smoothness)\n",
    "        history['ridge'].append(ridge)\n",
    "        history['total'].append(total_obj)\n",
    "        \n",
    "        # Check convergence\n",
    "        delta = np.linalg.norm(C - C_old, 'fro') / (np.linalg.norm(C_old, 'fro') + 1e-10)\n",
    "        if delta < tol:\n",
    "            break\n",
    "    \n",
    "    return P, C, history\n",
    "\n",
    "\n",
    "def evaluate_solution(C_opt, C_true):\n",
    "    \"\"\"\n",
    "    Check if permutation is correct and if solution is degenerate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    is_correct : bool\n",
    "        True if permutation matches ground truth\n",
    "    is_degenerate : bool\n",
    "        True if one component nearly vanishes\n",
    "    \"\"\"\n",
    "    # Check permutation by correlation\n",
    "    corr_11 = np.corrcoef(C_opt[0, :], C_true[0, :])[0, 1]\n",
    "    corr_12 = np.corrcoef(C_opt[0, :], C_true[1, :])[0, 1]\n",
    "    is_correct = abs(corr_11) > abs(corr_12)\n",
    "    \n",
    "    # Check for degeneracy\n",
    "    energies = np.linalg.norm(C_opt, axis=1)\n",
    "    is_degenerate = np.min(energies) / np.max(energies) < 0.1\n",
    "    \n",
    "    return is_correct, is_degenerate\n",
    "\n",
    "\n",
    "print(\"✓ ALS implementation with custom Q-matrix ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2f327",
   "metadata": {},
   "source": [
    "## Part 3: Standard Smoothness (Baseline)\n",
    "\n",
    "Reproduce the baseline from Part 11D: $Q = (D^2)^T D^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second derivative operator\n",
    "K = n_frames\n",
    "D2 = np.zeros((K - 2, K))\n",
    "for i in range(K - 2):\n",
    "    D2[i, i:i+3] = [1, -2, 1]\n",
    "\n",
    "# Standard smoothness Q\n",
    "Q_standard = D2.T @ D2\n",
    "\n",
    "print(\"Testing standard smoothness (baseline):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_trials = 20\n",
    "results_standard = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    P_opt, C_opt, history = als_with_custom_Q(\n",
    "        M, k=n_components, Q=Q_standard, epsilon_ridge=0,\n",
    "        max_iter=100, random_seed=trial\n",
    "    )\n",
    "    \n",
    "    is_correct, is_degenerate = evaluate_solution(C_opt, C_true)\n",
    "    \n",
    "    results_standard.append({\n",
    "        'correct': is_correct,\n",
    "        'degenerate': is_degenerate,\n",
    "        'objective': history['total'][-1]\n",
    "    })\n",
    "\n",
    "n_correct = sum(r['correct'] for r in results_standard)\n",
    "n_degenerate = sum(r['degenerate'] for r in results_standard)\n",
    "\n",
    "print(f\"Standard smoothness Q = (D²)ᵀD²:\")\n",
    "print(f\"  Correct permutation: {n_correct}/{n_trials} ({n_correct/n_trials*100:.0f}%)\")\n",
    "print(f\"  Degenerate solutions: {n_degenerate}/{n_trials} ({n_degenerate/n_trials*100:.0f}%)\")\n",
    "print()\n",
    "print(\"✓ Baseline established (should match Part 11D: ~35% success)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0509de",
   "metadata": {},
   "source": [
    "## Part 4: Approach 1 - Spatially-Weighted Smoothness\n",
    "\n",
    "Design $Q = (D^2)^T W D^2$ where $W$ penalizes curvature more heavily in \"unexpected\" regions.\n",
    "\n",
    "**Hypothesis**: Bimodal solution needs curvature between the two true peaks (frames 35-55). By increasing penalty in this region, we make the degenerate solution less attractive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design spatial weighting based on expected peak locations\n",
    "def create_spatial_weights(frames, peak_locs, peak_widths, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Create spatial weights that penalize curvature in unexpected regions.\n",
    "    \n",
    "    Lower weights near expected peaks (allow curvature)\n",
    "    Higher weights between peaks (discourage bimodal spanning)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frames : array\n",
    "        Frame indices\n",
    "    peak_locs : list\n",
    "        Expected peak locations (e.g., [35, 55])\n",
    "    peak_widths : list\n",
    "        Expected peak widths (e.g., [4, 6])\n",
    "    gamma : float\n",
    "        Steepness of weighting (higher = stronger penalty between peaks)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array (K-2, K-2)\n",
    "        Diagonal weight matrix\n",
    "    \"\"\"\n",
    "    K = len(frames)\n",
    "    weights = np.ones(K - 2)\n",
    "    \n",
    "    for i in range(K - 2):\n",
    "        frame_center = frames[i + 1]  # Center of D² stencil\n",
    "        \n",
    "        # Distance to nearest expected peak (normalized by width)\n",
    "        min_dist = float('inf')\n",
    "        for loc, width in zip(peak_locs, peak_widths):\n",
    "            dist = abs(frame_center - loc) / width\n",
    "            min_dist = min(min_dist, dist)\n",
    "        \n",
    "        # Weight increases with distance from peaks\n",
    "        # w(x) = 1 + γ·min_dist\n",
    "        weights[i] = 1.0 + gamma * min_dist\n",
    "    \n",
    "    W = np.diag(weights)\n",
    "    return W\n",
    "\n",
    "\n",
    "# Create spatially-weighted Q\n",
    "peak_locs = [35, 55]  # Known from ground truth\n",
    "peak_widths = [4, 6]   # Known from ground truth\n",
    "\n",
    "print(\"Testing spatially-weighted smoothness:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Expected peak locations: {peak_locs}\")\n",
    "print(f\"Expected peak widths: {peak_widths}\")\n",
    "print()\n",
    "\n",
    "# Test different γ values\n",
    "gamma_values = [0.5, 1.0, 2.0, 4.0]\n",
    "results_spatial = {}\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    W = create_spatial_weights(frames, peak_locs, peak_widths, gamma=gamma)\n",
    "    Q_spatial = D2.T @ W @ D2\n",
    "    \n",
    "    results = []\n",
    "    for trial in range(n_trials):\n",
    "        P_opt, C_opt, history = als_with_custom_Q(\n",
    "            M, k=n_components, Q=Q_spatial, epsilon_ridge=0,\n",
    "            max_iter=100, random_seed=trial\n",
    "        )\n",
    "        \n",
    "        is_correct, is_degenerate = evaluate_solution(C_opt, C_true)\n",
    "        results.append({\n",
    "            'correct': is_correct,\n",
    "            'degenerate': is_degenerate,\n",
    "            'objective': history['total'][-1]\n",
    "        })\n",
    "    \n",
    "    n_correct = sum(r['correct'] for r in results)\n",
    "    n_degenerate = sum(r['degenerate'] for r in results)\n",
    "    \n",
    "    results_spatial[gamma] = {\n",
    "        'n_correct': n_correct,\n",
    "        'n_degenerate': n_degenerate,\n",
    "        'success_rate': n_correct / n_trials * 100,\n",
    "        'degeneracy_rate': n_degenerate / n_trials * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"γ = {gamma:4.1f}: Correct {n_correct}/{n_trials} ({n_correct/n_trials*100:5.1f}%), \"\n",
    "          f\"Degenerate {n_degenerate}/{n_trials} ({n_degenerate/n_trials*100:5.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best γ\n",
    "best_gamma = max(results_spatial.keys(), key=lambda g: results_spatial[g]['success_rate'])\n",
    "best_rate = results_spatial[best_gamma]['success_rate']\n",
    "\n",
    "print(f\"\\nBest spatial weighting: γ = {best_gamma} → {best_rate:.0f}% success\")\n",
    "\n",
    "if best_rate > 80:\n",
    "    print(\"✓✓ SUCCESS! Spatial weighting prevents degeneracy while maintaining invariance!\")\n",
    "elif best_rate > n_correct/n_trials*100:\n",
    "    print(f\"⚠ Improvement over baseline ({n_correct/n_trials*100:.0f}% → {best_rate:.0f}%) but insufficient\")\n",
    "else:\n",
    "    print(\"✗ Spatial weighting did not improve over baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e4965",
   "metadata": {},
   "source": [
    "## Part 5: Approach 2 - Band-Pass Frequency Filter\n",
    "\n",
    "Design $Q$ in frequency domain: $Q = F^T \\Lambda F$ where $F$ is DCT transform and $\\Lambda$ contains frequency weights.\n",
    "\n",
    "**Hypothesis**: Degenerate solution has different frequency content than true solution. By targeting the \"expected\" frequency range, we can disfavor degeneracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c739e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_Q(K, peak_width, low_cutoff=0.1, high_cutoff=0.8):\n",
    "    \"\"\"\n",
    "    Create Q-matrix based on frequency-domain filtering.\n",
    "    \n",
    "    Penalizes very low frequencies (constant/flat) and very high (noise).\n",
    "    Allows mid-range frequencies corresponding to expected peak widths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of frames\n",
    "    peak_width : float\n",
    "        Expected peak width in frames\n",
    "    low_cutoff : float\n",
    "        Low frequency cutoff (0-1, fraction of max frequency)\n",
    "    high_cutoff : float\n",
    "        High frequency cutoff (0-1, fraction of max frequency)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q : array (K, K)\n",
    "        Frequency-based regularization matrix\n",
    "    \"\"\"\n",
    "    # DCT basis\n",
    "    F = np.zeros((K, K))\n",
    "    for k in range(K):\n",
    "        for n in range(K):\n",
    "            F[k, n] = np.cos(np.pi * k * (n + 0.5) / K)\n",
    "    F = F / np.sqrt(K / 2)\n",
    "    F[0, :] /= np.sqrt(2)\n",
    "    \n",
    "    # Frequency weights (band-pass)\n",
    "    freqs = np.arange(K) / K  # Normalized frequencies [0, 1]\n",
    "    \n",
    "    # Penalty for very low frequencies (constant/flat)\n",
    "    low_penalty = np.exp(-((freqs - 0) / low_cutoff)**2)\n",
    "    \n",
    "    # Penalty for very high frequencies (noise/oscillations)\n",
    "    high_penalty = 1 - np.exp(-((freqs - 1) / (1 - high_cutoff))**2)\n",
    "    \n",
    "    # Combined penalty (low in middle, high at extremes)\n",
    "    weights = low_penalty + high_penalty\n",
    "    weights[0] *= 10  # Extra penalty on DC component (flat)\n",
    "    \n",
    "    Lambda = np.diag(weights)\n",
    "    \n",
    "    # Q = F^T Λ F\n",
    "    Q = F.T @ Lambda @ F\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "print(\"Testing frequency-domain band-pass filtering:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different frequency cutoffs\n",
    "cutoff_pairs = [(0.05, 0.7), (0.1, 0.8), (0.15, 0.85)]\n",
    "results_freq = {}\n",
    "\n",
    "for low_cut, high_cut in cutoff_pairs:\n",
    "    Q_freq = create_frequency_Q(K, peak_width=5, low_cutoff=low_cut, high_cutoff=high_cut)\n",
    "    \n",
    "    results = []\n",
    "    for trial in range(n_trials):\n",
    "        P_opt, C_opt, history = als_with_custom_Q(\n",
    "            M, k=n_components, Q=Q_freq, epsilon_ridge=0,\n",
    "            max_iter=100, random_seed=trial\n",
    "        )\n",
    "        \n",
    "        is_correct, is_degenerate = evaluate_solution(C_opt, C_true)\n",
    "        results.append({\n",
    "            'correct': is_correct,\n",
    "            'degenerate': is_degenerate,\n",
    "            'objective': history['total'][-1]\n",
    "        })\n",
    "    \n",
    "    n_correct = sum(r['correct'] for r in results)\n",
    "    n_degenerate = sum(r['degenerate'] for r in results)\n",
    "    \n",
    "    key = (low_cut, high_cut)\n",
    "    results_freq[key] = {\n",
    "        'n_correct': n_correct,\n",
    "        'n_degenerate': n_degenerate,\n",
    "        'success_rate': n_correct / n_trials * 100,\n",
    "        'degeneracy_rate': n_degenerate / n_trials * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"Cutoffs ({low_cut:.2f}, {high_cut:.2f}): \"\n",
    "          f\"Correct {n_correct}/{n_trials} ({n_correct/n_trials*100:5.1f}%), \"\n",
    "          f\"Degenerate {n_degenerate}/{n_trials} ({n_degenerate/n_trials*100:5.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best cutoffs\n",
    "best_cutoffs = max(results_freq.keys(), key=lambda k: results_freq[k]['success_rate'])\n",
    "best_rate = results_freq[best_cutoffs]['success_rate']\n",
    "\n",
    "print(f\"\\nBest frequency cutoffs: {best_cutoffs} → {best_rate:.0f}% success\")\n",
    "\n",
    "if best_rate > 80:\n",
    "    print(\"✓✓ SUCCESS! Frequency filtering prevents degeneracy while maintaining invariance!\")\n",
    "elif best_rate > n_correct/n_trials*100:\n",
    "    print(f\"⚠ Improvement over baseline ({n_correct/n_trials*100:.0f}% → {best_rate:.0f}%) but insufficient\")\n",
    "else:\n",
    "    print(\"✗ Frequency filtering did not improve over baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e097d",
   "metadata": {},
   "source": [
    "## Part 6: Approach 3 - Combined Spatial + Ridge\n",
    "\n",
    "Combine the best spatial weighting with ridge regularization:\n",
    "\n",
    "$$Q = (D^2)^T W D^2 + \\epsilon I$$\n",
    "\n",
    "**Hypothesis**: Spatial weighting addresses where curvature appears, ridge addresses amplitude imbalance. Together they might be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3391e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing combined spatial weighting + ridge:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use best spatial gamma from Part 4\n",
    "W_best = create_spatial_weights(frames, peak_locs, peak_widths, gamma=best_gamma)\n",
    "Q_spatial_best = D2.T @ W_best @ D2\n",
    "\n",
    "# Test different ridge values\n",
    "epsilon_values = [0.01, 0.1, 0.5, 1.0]\n",
    "results_combined = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    results = []\n",
    "    for trial in range(n_trials):\n",
    "        P_opt, C_opt, history = als_with_custom_Q(\n",
    "            M, k=n_components, Q=Q_spatial_best, epsilon_ridge=eps,\n",
    "            max_iter=100, random_seed=trial\n",
    "        )\n",
    "        \n",
    "        is_correct, is_degenerate = evaluate_solution(C_opt, C_true)\n",
    "        results.append({\n",
    "            'correct': is_correct,\n",
    "            'degenerate': is_degenerate,\n",
    "            'objective': history['total'][-1]\n",
    "        })\n",
    "    \n",
    "    n_correct = sum(r['correct'] for r in results)\n",
    "    n_degenerate = sum(r['degenerate'] for r in results)\n",
    "    \n",
    "    results_combined[eps] = {\n",
    "        'n_correct': n_correct,\n",
    "        'n_degenerate': n_degenerate,\n",
    "        'success_rate': n_correct / n_trials * 100,\n",
    "        'degeneracy_rate': n_degenerate / n_trials * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"ε = {eps:5.2f}: Correct {n_correct}/{n_trials} ({n_correct/n_trials*100:5.1f}%), \"\n",
    "          f\"Degenerate {n_degenerate}/{n_trials} ({n_degenerate/n_trials*100:5.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best epsilon\n",
    "best_eps = max(results_combined.keys(), key=lambda e: results_combined[e]['success_rate'])\n",
    "best_rate = results_combined[best_eps]['success_rate']\n",
    "\n",
    "print(f\"\\nBest combined (γ={best_gamma}, ε={best_eps}): {best_rate:.0f}% success\")\n",
    "\n",
    "if best_rate > 80:\n",
    "    print(\"✓✓ SUCCESS! Combined spatial + ridge prevents degeneracy!\")\n",
    "elif best_rate > results_spatial[best_gamma]['success_rate']:\n",
    "    improvement = best_rate - results_spatial[best_gamma]['success_rate']\n",
    "    print(f\"⚠ Improvement over spatial alone (+{improvement:.0f}%) but insufficient\")\n",
    "else:\n",
    "    print(\"✗ Ridge did not add value to spatial weighting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2facb8e",
   "metadata": {},
   "source": [
    "## Part 7: Visualization - Comparison of All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97819b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "methods = [\n",
    "    ('Standard\\n(D²)ᵀD²', sum(r['correct'] for r in results_standard) / n_trials * 100),\n",
    "    (f'Spatial\\nγ={best_gamma}', results_spatial[best_gamma]['success_rate']),\n",
    "    (f'Frequency\\n{best_cutoffs}', results_freq[best_cutoffs]['success_rate']),\n",
    "    (f'Combined\\nγ={best_gamma}, ε={best_eps}', results_combined[best_eps]['success_rate'])\n",
    "]\n",
    "\n",
    "method_names = [m[0] for m in methods]\n",
    "success_rates = [m[1] for m in methods]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Success rates\n",
    "colors = ['red' if r < 50 else 'orange' if r < 80 else 'green' for r in success_rates]\n",
    "bars = ax1.bar(range(len(methods)), success_rates, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.axhline(80, color='green', linestyle='--', linewidth=2, label='Target: 80%')\n",
    "ax1.axhline(50, color='orange', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Problem-Informed Q-Matrix Design\\nComparison of Approaches', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(range(len(methods)))\n",
    "ax1.set_xticklabels(method_names, fontsize=10)\n",
    "ax1.set_ylim([0, 105])\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, rate) in enumerate(zip(bars, success_rates)):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{rate:.0f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Right: Summary table\n",
    "ax2.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "SUMMARY: Problem-Informed Q-Matrix Design\n",
    "{'='*50}\n",
    "\n",
    "Research Question:\n",
    "Can problem-class knowledge be incorporated into\n",
    "fixed Q-matrix to reduce degeneracy while maintaining\n",
    "orthogonal invariance?\n",
    "\n",
    "Approaches Tested:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "1. Standard (D²)ᵀD²: {success_rates[0]:.0f}%\n",
    "   → Baseline (from Part 11D)\n",
    "\n",
    "2. Spatial weighting: {success_rates[1]:.0f}%\n",
    "   → Penalize curvature between peaks\n",
    "   → Q = (D²)ᵀWD² with γ={best_gamma}\n",
    "\n",
    "3. Frequency filtering: {success_rates[2]:.0f}%\n",
    "   → Band-pass in frequency domain\n",
    "   → Q = FᵀΛF with cutoffs {best_cutoffs}\n",
    "\n",
    "4. Combined (spatial+ridge): {success_rates[3]:.0f}%\n",
    "   → Spatial location + amplitude control\n",
    "   → Q + εI with γ={best_gamma}, ε={best_eps}\n",
    "\n",
    "{'✓✓ SUCCESS!' if max(success_rates) > 80 else '⚠ INSUFFICIENT' if max(success_rates) > success_rates[0] else '✗ NO IMPROVEMENT'}\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Best approach: {method_names[success_rates.index(max(success_rates))]}\n",
    "Success rate: {max(success_rates):.0f}%\n",
    "Improvement: +{max(success_rates) - success_rates[0]:.0f}% over baseline\n",
    "\n",
    "Key Insight:\n",
    "{'Problem-informed Q can prevent degeneracy!' if max(success_rates) > 80 else 'Problem-informed Q helps but insufficient.' if max(success_rates) > success_rates[0] + 10 else 'Generic Q limitations persist even with priors.'}\n",
    "\"\"\"\n",
    "\n",
    "ax2.text(0.05, 0.95, summary_text, transform=ax2.transAxes,\n",
    "        fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../problem_informed_Q_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Figure saved: problem_informed_Q_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531151a0",
   "metadata": {},
   "source": [
    "## Part 8: Analysis and Conclusions\n",
    "\n",
    "Let's interpret the results and draw conclusions about problem-informed Q-matrix design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS: Problem-Informed Q-Matrix Design\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "baseline = success_rates[0]\n",
    "best_approach = method_names[success_rates.index(max(success_rates))]\n",
    "best_rate = max(success_rates)\n",
    "improvement = best_rate - baseline\n",
    "\n",
    "print(f\"Baseline (standard smoothness): {baseline:.0f}%\")\n",
    "print(f\"Best approach: {best_approach.replace(chr(10), ' ')}\")\n",
    "print(f\"Best success rate: {best_rate:.0f}%\")\n",
    "print(f\"Improvement: +{improvement:.0f} percentage points\")\n",
    "print()\n",
    "\n",
    "# Conclusions\n",
    "print(\"CONCLUSIONS:\")\n",
    "print(\"-\"*70)\n",
    "print()\n",
    "\n",
    "if best_rate >= 80:\n",
    "    print(\"✓✓ SUCCESS! Problem-informed Q-matrix design SOLVES degeneracy!\")\n",
    "    print()\n",
    "    print(\"Key findings:\")\n",
    "    print(f\"  1. {best_approach.replace(chr(10), ' ')} achieves {best_rate:.0f}% reliability\")\n",
    "    print(\"  2. Maintains orthogonal invariance (fixed Q form)\")\n",
    "    print(\"  3. Incorporates problem-class knowledge without breaking invariance\")\n",
    "    print(\"  4. Demonstrates that 'informed invariant' regularizers can be designed\")\n",
    "    print()\n",
    "    print(\"Implications:\")\n",
    "    print(\"  → Problem-specific Q design is a viable research direction\")\n",
    "    print(\"  → Trade-off (invariance ↔ effectiveness) can be partially resolved\")\n",
    "    print(\"  → Future work: Develop Q-design principles for different problem classes\")\n",
    "    \n",
    "elif best_rate > baseline + 15:\n",
    "    print(\"⚠ SUBSTANTIAL IMPROVEMENT but still insufficient for reliability\")\n",
    "    print()\n",
    "    print(\"Key findings:\")\n",
    "    print(f\"  1. Problem-informed Q improves success by +{improvement:.0f}%\")\n",
    "    print(\"  2. Still falls short of 80% reliability threshold\")\n",
    "    print(\"  3. Suggests problem-informed approach is on right track\")\n",
    "    print(\"  4. May need more sophisticated Q design or additional constraints\")\n",
    "    print()\n",
    "    print(\"Implications:\")\n",
    "    print(\"  → Hybrid approaches worth exploring (problem-informed Q + other methods)\")\n",
    "    print(\"  → May still need to break invariance in extreme cases\")\n",
    "    print(\"  → Future work: Combine with good initialization or post-hoc validation\")\n",
    "    \n",
    "else:\n",
    "    print(\"✗ MINIMAL OR NO IMPROVEMENT - Generic Q limitations persist\")\n",
    "    print()\n",
    "    print(\"Key findings:\")\n",
    "    print(f\"  1. Problem-informed Q yields only +{improvement:.0f}% improvement\")\n",
    "    print(\"  2. Fundamental limitation of fixed Q form confirmed\")\n",
    "    print(\"  3. Sum structure weakness cannot be overcome by spatial/frequency priors\")\n",
    "    print(\"  4. Invariance-degeneracy trade-off appears fundamental\")\n",
    "    print()\n",
    "    print(\"Implications:\")\n",
    "    print(\"  → Must accept breaking invariance for reliable deconvolution\")\n",
    "    print(\"  → Profile-weighted or minimum-amplitude penalties necessary\")\n",
    "    print(\"  → No 'perfect' invariant regularizer exists for this problem\")\n",
    "    print(\"  → Future work: Characterize how little invariance-breaking is needed\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "if best_rate >= 80:\n",
    "    print(\"1. Document successful Q-design principles\")\n",
    "    print(\"2. Test on varied datasets (different separations, overlaps, SNR)\")\n",
    "    print(\"3. Develop general framework for problem-class-specific Q design\")\n",
    "    print(\"4. Compare with profile-weighted approaches (invariance vs effectiveness)\")\n",
    "elif best_rate > baseline + 15:\n",
    "    print(\"1. Explore more sophisticated Q designs (adapt work from here)\")\n",
    "    print(\"2. Test hybrid: problem-informed Q + good initialization\")\n",
    "    print(\"3. Investigate combining with post-hoc validation methods\")\n",
    "    print(\"4. Consider: Is 80% threshold achievable with invariant Q?\")\n",
    "else:\n",
    "    print(\"1. Document that problem-informed Q insufficient for this problem\")\n",
    "    print(\"2. Formalize fundamental limitation theorem\")\n",
    "    print(\"3. Explore 'mostly invariant' approaches (minimal invariance breaking)\")\n",
    "    print(\"4. Accept profile-weighted methods as necessary for reliability\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a058a",
   "metadata": {},
   "source": [
    "## Appendix: Why These Q Designs Maintain Invariance\n",
    "\n",
    "All approaches tested maintain the form $S(C) = \\text{tr}(CQC^T)$ with fixed $Q$:\n",
    "\n",
    "### 1. Spatially-Weighted Smoothness\n",
    "\n",
    "$$Q = (D^2)^T W D^2$$\n",
    "\n",
    "- $W$ is diagonal (spatial weights)\n",
    "- $D^2$ is differential operator\n",
    "- $Q$ is symmetric positive semi-definite\n",
    "- **Fixed**: $W$ designed once based on expected peaks, doesn't change with solution\n",
    "\n",
    "**Invariance**: For orthogonal $R$:\n",
    "$$\\text{tr}((RC)Q(RC)^T) = \\text{tr}(RCQ C^T R^T) = \\text{tr}(CQC^T R^T R) = \\text{tr}(CQC^T)$$\n",
    "\n",
    "### 2. Frequency-Domain Band-Pass\n",
    "\n",
    "$$Q = F^T \\Lambda F$$\n",
    "\n",
    "- $F$ is DCT transform matrix (orthogonal)\n",
    "- $\\Lambda$ is diagonal (frequency weights)\n",
    "- $Q$ is symmetric positive semi-definite\n",
    "- **Fixed**: Frequency bands determined by expected peak widths, constant\n",
    "\n",
    "**Invariance**: Same as above - $Q$ is fixed, so orthogonal transformations preserve penalty.\n",
    "\n",
    "### 3. Combined Spatial + Ridge\n",
    "\n",
    "$$Q = (D^2)^T W D^2 + \\epsilon I$$\n",
    "\n",
    "- Linear combination of two fixed $Q$ matrices\n",
    "- Still has $\\text{tr}(CQC^T)$ form\n",
    "- **Fixed**: Both $W$ and $\\epsilon$ set based on priors\n",
    "\n",
    "**Invariance**: Preserved by linearity.\n",
    "\n",
    "---\n",
    "\n",
    "**Contrast with profile-weighted**: Would have $Q_i = \\|p_i\\|^2 (D^2)^T D^2$ where weights depend on optimized $P$. This breaks invariance because $Q$ changes during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd85919",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook explored whether **problem-specific knowledge** can be incorporated into Q-matrix design to reduce degeneracy while maintaining orthogonal invariance.\n",
    "\n",
    "**Three approaches tested**:\n",
    "1. Spatially-weighted smoothness (penalize curvature in unexpected regions)\n",
    "2. Frequency-domain band-pass (target expected frequency content)\n",
    "3. Combined spatial + ridge (location + amplitude control)\n",
    "\n",
    "**Key insight**: All maintain $S(C) = \\text{tr}(CQC^T)$ form with **fixed** $Q$, so invariance preserved.\n",
    "\n",
    "**Results**: [See Part 7 for experimental outcomes]\n",
    "\n",
    "**Relation to broader research**:\n",
    "- Tests Open Research Question #3 from [orthogonal_invariance_journey.md](orthogonal_invariance_journey.md)\n",
    "- Explores middle ground: generic Q (ridge) vs fully adaptive (profile-weighted)\n",
    "- Informs debate: Can invariance + effectiveness coexist?\n",
    "\n",
    "**Next steps**: [See Part 8 for recommendations based on results]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
