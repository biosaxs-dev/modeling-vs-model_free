{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd52e8f",
   "metadata": {},
   "source": [
    "# Stage 8 Experiment: Log-Additive + Frequency Q\n",
    "\n",
    "**Critical Research Question**: Does frequency-domain Q work with log-additive objectives?\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We seek optimal low-rank factorization M ≈ PC where:\n",
    "1. **Good reconstruction**: $\\|M - PC\\|_F^2$ is small\n",
    "2. **Smooth SAXS profiles**: P profiles are well-behaved (usually not explicitly regularized)\n",
    "3. **Smooth elution curves**: C profiles are smooth, measured by $\\text{tr}(CQC^T)$\n",
    "\n",
    "## Context\n",
    "\n",
    "From previous stages:\n",
    "- **Stage 5**: Additive + frequency Q → **90% success** (breakthrough!)\n",
    "  - Objective: $f = \\|M - PC\\|^2 + \\lambda \\cdot \\text{tr}(CQC^T)$ with frequency Q\n",
    "- **Stage 7**: Log-additive + generic smoothness → **25% success** (only operator with success)\n",
    "  - Objective: $f = \\log(\\|M - PC\\|^2) + \\lambda \\cdot \\log(\\text{tr}(CQC^T))$ with generic Q\n",
    "\n",
    "**Hypothesis**: Log-additive + frequency Q → **>50% success?**\n",
    "\n",
    "If yes: We get the \"best of both worlds\":\n",
    "- Adaptive gradients from log-additive\n",
    "- Problem-informed regularization from frequency Q\n",
    "\n",
    "## Objective Formulation\n",
    "\n",
    "**Additive operator** (Stage 5):\n",
    "$$f_{\\text{add}} = \\|M - PC\\|_F^2 + \\lambda \\cdot \\text{tr}(CQC^T)$$\n",
    "\n",
    "**Log-additive operator** (Stage 7, this notebook):\n",
    "$$f_{\\text{log}} = \\log(\\|M - PC\\|_F^2) + \\lambda \\cdot \\log(\\text{tr}(CQC^T))$$\n",
    "\n",
    "Where:\n",
    "- $\\text{tr}(CQC^T)$ measures smoothness of C (concentration profiles)\n",
    "- Q can be generic $(D^2)^T D^2$ or frequency-domain (Stage 5)\n",
    "- Log-additive creates adaptive penalty: $\\nabla_C \\propto \\frac{1}{\\text{tr}(CQC^T)}$\n",
    "\n",
    "**Key Question**: Do adaptive gradients interact constructively with frequency filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from molass.SAXS.Models.Simple import guinier_porod\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Stage 8: Testing log-additive + frequency Q combination\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5ea27",
   "metadata": {},
   "source": [
    "## Section 1: Generate Synthetic SEC-SAXS Data\n",
    "\n",
    "**Realistic SEC-SAXS Setup**:\n",
    "\n",
    "- **C profiles** (elution/concentration): Gaussian peaks from SEC column- This matches actual experimental SEC-SAXS data structure\n",
    "- **P profiles** (SAXS): Guinier-Porod models with physical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic SEC-SAXS dataset: M = PC where P, C are known\n",
    "K = 80  # Number of frames (elution time dimension)\n",
    "n_q = 50  # Number of q-points (scattering vector dimension)\n",
    "n_components = 2\n",
    "\n",
    "frames = np.arange(K)\n",
    "\n",
    "# Ground truth C (concentration/elution profiles) - SEC physics\n",
    "def gaussian_peak(x, center, width, height=1.0):\n",
    "    return height * np.exp(-((x - center)**2) / (2 * width**2))\n",
    "\n",
    "C_true = np.zeros((n_components, K))\n",
    "C_true[0, :] = gaussian_peak(frames, center=35, width=4)  # Component 1: larger particle, elutes early, narrower\n",
    "C_true[1, :] = gaussian_peak(frames, center=55, width=6)  # Component 2: smaller particle, elutes late, broader\n",
    "\n",
    "# Normalize\n",
    "C_true[0, :] = C_true[0, :] / C_true[0, :].sum()\n",
    "C_true[1, :] = C_true[1, :] / C_true[1, :].sum()\n",
    "\n",
    "# Ground truth P (SAXS profiles) - Guinier-Porod models\n",
    "q = np.linspace(0.01, 0.3, n_q)  # Typical SAXS q-range (Å⁻¹)\n",
    "\n",
    "# Component 1: LARGER particle → Rg = 40 Å, d = 4 (sphere)\n",
    "G1 = 1.0  # Guinier prefactor\n",
    "Rg1 = 40.0  # Radius of gyration (Å) - larger particle\n",
    "d1 = 4.0  # Porod exponent (sphere)\n",
    "p1_true = guinier_porod(q, G1, Rg1, d1)\n",
    "\n",
    "# Component 2: SMALLER particle → Rg = 20 Å, d = 4 (sphere)\n",
    "G2 = 1.0  # Same scale\n",
    "Rg2 = 20.0  # Radius of gyration (Å) - smaller particle\n",
    "d2 = 4.0  # Porod exponent (sphere)\n",
    "p2_true = guinier_porod(q, G2, Rg2, d2)\n",
    "\n",
    "P_true = np.vstack([p1_true, p2_true]).T  # (n_q, n_components)\n",
    "\n",
    "# Generate observed data\n",
    "M_clean = P_true @ C_true\n",
    "noise_level = M_clean.mean() / 100  # SNR = 100\n",
    "noise = np.random.randn(n_q, K) * noise_level\n",
    "M = M_clean + noise\n",
    "M = np.maximum(M, 0)  # Ensure non-negativity\n",
    "\n",
    "print(f\"Data shape: {M.shape} (q-points × frames)\")\n",
    "print(f\"Components: {n_components}\")\n",
    "print(f\"C_true peaks at frames: 35 (σ=4), 55 (σ=6)\")\n",
    "print(f\"P_true: Component 1 Rg={Rg1}Å, Component 2 Rg={Rg2}Å\")\n",
    "print(f\"SNR: {M_clean.mean() / noise_level:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92872f",
   "metadata": {},
   "source": [
    "## Section 2: Create Frequency-Domain Q\n",
    "\n",
    "Exact implementation from Stage 5 (where it achieved 90% with additive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0107300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_Q(K, peak_width, low_cutoff=0.1, high_cutoff=0.8):\n",
    "    \"\"\"\n",
    "    Create Q-matrix based on frequency-domain filtering.\n",
    "    \n",
    "    Penalizes very low frequencies (constant/flat) and very high (noise).\n",
    "    Allows mid-range frequencies corresponding to expected peak widths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of frames\n",
    "    peak_width : float\n",
    "        Expected peak width in frames\n",
    "    low_cutoff : float\n",
    "        Low frequency cutoff (0-1, fraction of max frequency)\n",
    "    high_cutoff : float\n",
    "        High frequency cutoff (0-1, fraction of max frequency)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q : array (K, K)\n",
    "        Frequency-based regularization matrix\n",
    "    \"\"\"\n",
    "    # DCT basis\n",
    "    F = np.zeros((K, K))\n",
    "    for k in range(K):\n",
    "        for n in range(K):\n",
    "            F[k, n] = np.cos(np.pi * k * (n + 0.5) / K)\n",
    "    F = F / np.sqrt(K / 2)\n",
    "    F[0, :] /= np.sqrt(2)\n",
    "    \n",
    "    # Frequency weights (band-pass)\n",
    "    freqs = np.arange(K) / K  # Normalized frequencies [0, 1]\n",
    "    \n",
    "    # Penalty for very low frequencies (constant/flat)\n",
    "    low_penalty = np.exp(-((freqs - 0) / low_cutoff)**2)\n",
    "    \n",
    "    # Penalty for very high frequencies (noise/oscillations)\n",
    "    high_penalty = 1 - np.exp(-((freqs - 1) / (1 - high_cutoff))**2)\n",
    "    \n",
    "    # Combined penalty (low in middle, high at extremes)\n",
    "    weights = low_penalty + high_penalty\n",
    "    weights[0] *= 10  # Extra penalty on DC component (flat)\n",
    "    \n",
    "    Lambda = np.diag(weights)\n",
    "    \n",
    "    # Q = F^T Λ F\n",
    "    Q = F.T @ Lambda @ F\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Create frequency Q with optimal parameters from Stage 5\n",
    "Q_freq = create_frequency_Q(K, peak_width=5, low_cutoff=0.05, high_cutoff=0.7)\n",
    "\n",
    "print(f\"Frequency Q shape: {Q_freq.shape}\")\n",
    "print(f\"Cutoffs: low=0.05, high=0.7 (optimal from Stage 5)\")\n",
    "print(f\"\\nThis Q achieved 90% success with additive objective.\")\n",
    "print(f\"Will it work with log-additive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea64dcf",
   "metadata": {},
   "source": [
    "## Section 3: ALS with Log-Additive + Frequency Q\n",
    "\n",
    "**Correct Objective** (consistent with Stage 7):\n",
    "$$f = \\log(\\|M - PC\\|_F^2) + \\lambda \\cdot \\log(\\text{tr}(CQC^T))$$\n",
    "\n",
    "Where:\n",
    "- First term: Log of reconstruction error (not raw squared error)\n",
    "- Second term: Log of smoothness penalty on C\n",
    "- This is the **log-additive operator** applied to (reconstruction, smoothness)\n",
    "\n",
    "**Gradient structure**:\n",
    "$$\\frac{\\partial f}{\\partial C} = \\frac{-2P^T(M - PC)}{\\|M - PC\\|_F^2} + \\frac{\\lambda}{\\text{tr}(CQC^T)} \\cdot 2CQ$$\n",
    "\n",
    "Both terms have **adaptive inverse scaling** with their respective penalty magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_log_additive_frequency(M, k, Q, lam=0.1, max_iter=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    ALS with log-additive objective and frequency-domain Q.\n",
    "    \n",
    "    Objective: log(||M - PC||²) + λ·log(tr(CQC^T))\n",
    "    \n",
    "    This combines:\n",
    "    - Adaptive gradients from log-additive (Stage 7)\n",
    "    - Problem-informed regularization from frequency Q (Stage 5)\n",
    "    \n",
    "    Note: Simplified to focus on C regularization only (P not regularized)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    n, m = M.shape\n",
    "    \n",
    "    # Initialize with SVD + small noise\n",
    "    U, s, Vt = svd(M, full_matrices=False)\n",
    "    P = np.abs(U[:, :k] @ np.diag(np.sqrt(s[:k]))) + 0.01\n",
    "    C = np.abs(np.diag(np.sqrt(s[:k])) @ Vt[:k, :]) + 0.01\n",
    "    \n",
    "    history = {'recon': [], 'pen_C': [], 'total': []}\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Update P - standard ALS without regularization\n",
    "        # (could add log penalty on P, but focusing on C for degeneracy)\n",
    "        CCT = C @ C.T\n",
    "        P = M @ C.T @ np.linalg.inv(CCT + 1e-8 * np.eye(k))\n",
    "        P = np.maximum(P, 1e-10)  # Ensure positivity\n",
    "        \n",
    "        # Update C with log-additive objective\n",
    "        # Objective: log(||M-PC||²) + λ·log(tr(CQC^T))\n",
    "        # Gradient wrt C:\n",
    "        #   ∂f/∂C = -2/(||M-PC||²)·P^T(M-PC) + λ/(tr(CQC^T))·2CQ\n",
    "        # \n",
    "        # For ALS update, we use gradient descent step or\n",
    "        # approximate closed form by treating adaptive weights as constants\n",
    "        \n",
    "        PTP = P.T @ P\n",
    "        PTM = P.T @ M\n",
    "        \n",
    "        # Current objective values for adaptive weights\n",
    "        recon_error = np.linalg.norm(M - P @ C, 'fro')**2\n",
    "        CQC = np.trace(C @ Q @ C.T)\n",
    "        \n",
    "        # Adaptive regularization weights (inverse scaling)\n",
    "        alpha_recon = 1.0 / (recon_error + 1e-10)\n",
    "        alpha_smooth = lam / (CQC + 1e-10)\n",
    "        \n",
    "        # Update each row of C independently\n",
    "        # Balance: alpha_recon·(P^T P C = P^T M) + alpha_smooth·(C Q = 0)\n",
    "        # Rearranged: (alpha_recon·P^T P + alpha_smooth·Q) C = alpha_recon·P^T M\n",
    "        C_new = np.zeros_like(C)\n",
    "        for i in range(k):\n",
    "            # RHS: reconstruction contribution from other components\n",
    "            rhs = alpha_recon * PTM[i, :]\n",
    "            for j in range(k):\n",
    "                if j != i:\n",
    "                    rhs -= alpha_recon * PTP[i, j] * C[j, :]\n",
    "            \n",
    "            # LHS: (alpha_recon·PTP[i,i]·I + alpha_smooth·Q)\n",
    "            lhs_matrix = alpha_recon * PTP[i, i] * np.eye(Q.shape[0]) + alpha_smooth * Q\n",
    "            C_new[i, :] = np.linalg.solve(lhs_matrix + 1e-8 * np.eye(Q.shape[0]), rhs)\n",
    "        \n",
    "        C = np.maximum(C_new, 1e-10)  # Ensure positivity\n",
    "        \n",
    "        # Compute objective components (log-additive form)\n",
    "        recon = np.linalg.norm(M - P @ C, 'fro')**2\n",
    "        smoothness = np.trace(C @ Q @ C.T)\n",
    "        \n",
    "        log_recon = np.log(recon + 1e-10)\n",
    "        log_smooth = np.log(smoothness + 1e-10)\n",
    "        total = log_recon + lam * log_smooth\n",
    "        \n",
    "        history['recon'].append(recon)\n",
    "        history['pen_C'].append(smoothness)\n",
    "        history['total'].append(total)\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0:\n",
    "            rel_change = abs(history['total'][-1] - history['total'][-2]) / (abs(history['total'][-2]) + 1e-10)\n",
    "            if rel_change < 1e-6:\n",
    "                break\n",
    "    \n",
    "    return P, C, history\n",
    "\n",
    "print(\"ALS algorithm with log-additive + frequency Q defined\")\n",
    "print(\"Key features:\")\n",
    "print(\"  - Objective: log(||M-PC||²) + λ·log(tr(CQC^T))\")\n",
    "print(\"  - Both terms have adaptive inverse scaling\")\n",
    "print(\"  - Reconstruction weight: 1/||M-PC||²\")\n",
    "print(\"  - Smoothness weight: λ/tr(CQC^T)\")\n",
    "print(\"  - Frequency Q from Stage 5: band-pass filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fd838",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation Functions\n",
    "\n",
    "Same evaluation as Stage 7 for direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70171d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_solution(C_opt, C_true, corr_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Evaluate if recovered C matches ground truth.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    is_correct : bool\n",
    "        True if correlations > threshold (modulo permutation)\n",
    "    is_degenerate : bool\n",
    "        True if both components are nearly identical (all correlations > 0.95)\n",
    "    \"\"\"\n",
    "    from scipy.stats import pearsonr\n",
    "    \n",
    "    k = C_opt.shape[0]\n",
    "    \n",
    "    # Check for degeneracy (all components identical)\n",
    "    corr_matrix = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            corr_matrix[i, j] = abs(pearsonr(C_opt[i, :], C_opt[j, :])[0])\n",
    "    \n",
    "    # If all off-diagonal correlations are very high, it's degenerate\n",
    "    off_diag = corr_matrix[np.triu_indices(k, k=1)]\n",
    "    is_degenerate = np.all(off_diag > 0.95)\n",
    "    \n",
    "    if is_degenerate:\n",
    "        return False, True\n",
    "    \n",
    "    # Check correlation with ground truth (try both permutations)\n",
    "    corr_direct = [\n",
    "        abs(pearsonr(C_opt[0, :], C_true[0, :])[0]),\n",
    "        abs(pearsonr(C_opt[1, :], C_true[1, :])[0])\n",
    "    ]\n",
    "    \n",
    "    corr_swapped = [\n",
    "        abs(pearsonr(C_opt[0, :], C_true[1, :])[0]),\n",
    "        abs(pearsonr(C_opt[1, :], C_true[0, :])[0])\n",
    "    ]\n",
    "    \n",
    "    is_correct = (np.all(np.array(corr_direct) > corr_threshold) or\n",
    "                  np.all(np.array(corr_swapped) > corr_threshold))\n",
    "    \n",
    "    return is_correct, False\n",
    "\n",
    "print(\"Evaluation functions defined\")\n",
    "print(\"  - Correct: correlation > 0.95 (modulo permutation)\")\n",
    "print(\"  - Degenerate: both components nearly identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363dd84",
   "metadata": {},
   "source": [
    "## Section 5: Multi-Start Experiment\n",
    "\n",
    "Test log-additive + frequency Q with 20 random initializations.\n",
    "\n",
    "**Comparison baselines**:\n",
    "- Stage 5 (additive + frequency Q): 90%\n",
    "- Stage 7 (log-additive + generic smoothness): 25%\n",
    "\n",
    "**Hypothesis**: >50% success (best of both worlds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad02fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 20\n",
    "lambda_val = 0.1\n",
    "\n",
    "print(f\"Running multi-start experiment: {n_trials} trials\")\n",
    "print(f\"Objective: log-additive + frequency Q\")\n",
    "print(f\"λ = {lambda_val}\")\n",
    "print(f\"Q = frequency (cutoffs: 0.05, 0.7 from Stage 5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    P_opt, C_opt, history = als_log_additive_frequency(\n",
    "        M, k=n_components, Q=Q_freq, lam=lambda_val, \n",
    "        max_iter=100, random_seed=trial\n",
    "    )\n",
    "    \n",
    "    is_correct, is_degenerate = evaluate_solution(C_opt, C_true)\n",
    "    \n",
    "    results.append({\n",
    "        'trial': trial,\n",
    "        'correct': is_correct,\n",
    "        'degenerate': is_degenerate,\n",
    "        'final_objective': history['total'][-1],\n",
    "        'n_iterations': len(history['total']),\n",
    "        'C_opt': C_opt.copy()\n",
    "    })\n",
    "    \n",
    "    status = \"✓ CORRECT\" if is_correct else (\"✗ DEGENERATE\" if is_degenerate else \"✗ incorrect\")\n",
    "    print(f\"Trial {trial:2d}: {status}  (obj={history['total'][-1]:8.2f}, iters={len(history['total']):3d})\")\n",
    "\n",
    "# Summary statistics\n",
    "n_correct = sum(r['correct'] for r in results)\n",
    "n_degenerate = sum(r['degenerate'] for r in results)\n",
    "success_rate = n_correct / n_trials * 100\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRESULTS SUMMARY:\")\n",
    "print(f\"  Correct: {n_correct}/{n_trials} ({success_rate:.0f}%)\")\n",
    "print(f\"  Degenerate: {n_degenerate}/{n_trials} ({n_degenerate/n_trials*100:.0f}%)\")\n",
    "print(f\"  Other failures: {n_trials - n_correct - n_degenerate}/{n_trials}\")\n",
    "\n",
    "print(f\"\\nCOMPARISON TO BASELINES:\")\n",
    "print(f\"  Stage 5 (additive + frequency Q): 90%\")\n",
    "print(f\"  Stage 7 (log-additive + generic):  25%\")\n",
    "print(f\"  Stage 8 (log-additive + frequency): {success_rate:.0f}%\")\n",
    "\n",
    "if success_rate > 50:\n",
    "    print(f\"\\n✓✓ HYPOTHESIS CONFIRMED! Best of both worlds achieved!\")\n",
    "    print(f\"   Log-additive's adaptive gradients + frequency Q's problem-informed regularization = {success_rate:.0f}%\")\n",
    "elif success_rate > 25:\n",
    "    print(f\"\\n⚠ PARTIAL SUCCESS: Better than generic smoothness ({success_rate:.0f}% vs 25%)\")\n",
    "    print(f\"   But not as good as additive + frequency Q (90%)\")\n",
    "    print(f\"   Possible explanation: Adaptive gradients reduce Q importance\")\n",
    "else:\n",
    "    print(f\"\\n✗ NO IMPROVEMENT: Same as or worse than log-additive + generic ({success_rate:.0f}% vs 25%)\")\n",
    "    print(f\"   Possible explanation: Frequency Q incompatible with log-additive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6858db",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Representative Solutions\n",
    "\n",
    "Compare correct vs incorrect solutions to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find one correct and one incorrect solution\n",
    "correct_trials = [r for r in results if r['correct']]\n",
    "incorrect_trials = [r for r in results if not r['correct'] and not r['degenerate']]\n",
    "degenerate_trials = [r for r in results if r['degenerate']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Ground truth\n",
    "ax = axes[0]\n",
    "ax.plot(frames, C_true[0, :], 'b-', linewidth=2, label='Component 1')\n",
    "ax.plot(frames, C_true[1, :], 'r-', linewidth=2, label='Component 2')\n",
    "ax.set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Concentration')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correct solution (if any)\n",
    "ax = axes[1]\n",
    "if correct_trials:\n",
    "    C_correct = correct_trials[0]['C_opt']\n",
    "    ax.plot(frames, C_correct[0, :], 'b--', linewidth=2, alpha=0.7, label='Comp 1 (recovered)')\n",
    "    ax.plot(frames, C_correct[1, :], 'r--', linewidth=2, alpha=0.7, label='Comp 2 (recovered)')\n",
    "    ax.set_title(f'Correct Solution\\n(Trial {correct_trials[0][\"trial\"]})', fontsize=12, fontweight='bold')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No correct solutions', ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "    ax.set_title('Correct Solution (None found)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Concentration')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Incorrect or degenerate solution\n",
    "ax = axes[2]\n",
    "if degenerate_trials:\n",
    "    C_bad = degenerate_trials[0]['C_opt']\n",
    "    ax.plot(frames, C_bad[0, :], 'g-', linewidth=2, label='Comp 1')\n",
    "    ax.plot(frames, C_bad[1, :], 'm-', linewidth=2, alpha=0.7, label='Comp 2')\n",
    "    ax.set_title(f'Degenerate Solution\\n(Trial {degenerate_trials[0][\"trial\"]})', fontsize=12, fontweight='bold')\n",
    "elif incorrect_trials:\n",
    "    C_bad = incorrect_trials[0]['C_opt']\n",
    "    ax.plot(frames, C_bad[0, :], 'g-', linewidth=2, label='Comp 1')\n",
    "    ax.plot(frames, C_bad[1, :], 'm-', linewidth=2, alpha=0.7, label='Comp 2')\n",
    "    ax.set_title(f'Incorrect Solution\\n(Trial {incorrect_trials[0][\"trial\"]})', fontsize=12, fontweight='bold')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'All solutions correct!', ha='center', va='center', transform=ax.transAxes, fontsize=14, color='green')\n",
    "    ax.set_title('Failure Mode (None found!)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Concentration')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a56a4",
   "metadata": {},
   "source": [
    "## Section 7: Frequency Analysis of Solutions\n",
    "\n",
    "Examine frequency content of correct vs incorrect solutions.\n",
    "\n",
    "**Key question**: Does frequency Q successfully enforce expected frequency structure in log-additive context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74166e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import dct\n",
    "\n",
    "def compute_frequency_spectrum(C):\n",
    "    \"\"\"Compute DCT frequency spectrum for each component.\"\"\"\n",
    "    spectra = []\n",
    "    for i in range(C.shape[0]):\n",
    "        spectrum = dct(C[i, :], norm='ortho')\n",
    "        spectra.append(spectrum)\n",
    "    return np.array(spectra)\n",
    "\n",
    "if correct_trials and (incorrect_trials or degenerate_trials):\n",
    "    # Get frequency spectra\n",
    "    freq_true = compute_frequency_spectrum(C_true)\n",
    "    freq_correct = compute_frequency_spectrum(correct_trials[0]['C_opt'])\n",
    "    \n",
    "    if degenerate_trials:\n",
    "        freq_bad = compute_frequency_spectrum(degenerate_trials[0]['C_opt'])\n",
    "        bad_label = 'Degenerate'\n",
    "    else:\n",
    "        freq_bad = compute_frequency_spectrum(incorrect_trials[0]['C_opt'])\n",
    "        bad_label = 'Incorrect'\n",
    "    \n",
    "    # Plot frequency analysis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    freqs = np.arange(K) / K\n",
    "    \n",
    "    for ax, spectrum, title in zip(axes, \n",
    "                                     [freq_true, freq_correct, freq_bad],\n",
    "                                     ['Ground Truth', 'Correct Solution', f'{bad_label} Solution']):\n",
    "        ax.plot(freqs, np.abs(spectrum[0, :]), 'b-', label='Component 1', linewidth=2)\n",
    "        ax.plot(freqs, np.abs(spectrum[1, :]), 'r-', label='Component 2', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Mark cutoff regions\n",
    "        ax.axvspan(0, 0.05, alpha=0.2, color='red', label='Low cutoff (penalized)')\n",
    "        ax.axvspan(0.7, 1.0, alpha=0.2, color='orange', label='High cutoff (penalized)')\n",
    "        \n",
    "        ax.set_xlabel('Normalized Frequency')\n",
    "        ax.set_ylabel('|DCT Coefficient|')\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_yscale('log')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if ax == axes[0]:\n",
    "            ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFREQUENCY ANALYSIS:\")\n",
    "    print(\"- Red shaded: DC component (flat, should be penalized)\")\n",
    "    print(\"- Orange shaded: High frequencies (noise, should be penalized)\")\n",
    "    print(\"- Middle range: Expected peak frequencies (should be allowed)\")\n",
    "    print(\"\\nObservation: Do correct solutions have less energy in penalized regions?\")\n",
    "else:\n",
    "    print(\"Cannot perform frequency analysis - need both correct and incorrect solutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b844b",
   "metadata": {},
   "source": [
    "## Section 8: Analysis and Interpretation\n",
    "\n",
    "### Findings\n",
    "\n",
    "**Result: 0% success (0/20 trials)** ❌\n",
    "\n",
    "This is a **critical negative result**:\n",
    "- **Stage 5** (additive + frequency Q): 90% ✓\n",
    "- **Stage 7** (log-additive + generic Q): 25% ✓  \n",
    "- **Stage 8** (log-additive + frequency Q): **0%** ✗\n",
    "\n",
    "**Observations**:\n",
    "1. All 20 trials failed to recover correct solution\n",
    "2. All converged to identical objective value (1.46) at max iterations (100)\n",
    "3. Recovered profiles show:\n",
    "   - Peak locations approximately correct (frames 35, 55)\n",
    "   - Peak shapes distorted with artifacts (extra bumps)\n",
    "   - Components appear overly coupled/correlated\n",
    "4. Algorithm appears stuck in poor local minimum\n",
    "**This negative result is scientifically valuable** - it defines boundaries of the 3D design space (Operator × Q_form × Q_design).\n",
    "### Implications for Stage 8 Framework\n",
    "**Immediate priorities**:\n",
    "\n",
    "1. **Understand the incompatibility mechanism**:\n",
    "   - Does log(tr(CQC^T)) distort frequency structure?\n",
    "   - Does adaptive scaling λ/tr(CQC^T) interfere with frequency filtering?\n",
    "   - Visualize optimization trajectory to understand convergence failure\n",
    "\n",
    "2. **Test fixed form theorem** (Stage 8 Question #1):\n",
    "   - Current: $S(C) = \\text{tr}(CQC^T)$ works for additive\n",
    "   - Hypothesis: Log-additive may need $S_{\\text{log}}(C) = \\text{tr}(\\log(C) \\cdot Q \\cdot \\log(C)^T)$\n",
    "   - Or: $S_{\\text{log}}(C) = (\\text{tr}(CQC^T))^{\\alpha}$ with α ≠ 1\n",
    "\n",
    "3. **Design operator-specific Q** (Stage 8 Question #3):\n",
    "   - Frequency Q needs adaptation for log-additive context\n",
    "   - May need to design Q in log-space rather than linear space\n",
    "   - Explore: $Q_{\\text{log}} = F^T \\Lambda_{\\text{log}} F$ where Λ_log accounts for log scaling\n",
    "\n",
    "4. **Test other Q designs with log-additive**:\n",
    "   - Spatial weighting (from Stage 5)\n",
    "   - Combined spatial + ridge\n",
    "   - Simple ridge regularization (baseline)\n",
    "**If success >50%**:\n",
    "**Boundary established**: Not all (Operator, Q_form, Q_design) combinations are compatible!\n",
    "\n",
    "\n",
    "- Next: Understand why this combination works (gradient flow analysis)Then frequency Q design may need adaptation for log-additive context.\n",
    "\n",
    "### Connection to Fixed Form Theorem (Stage 8 Question #1)\n",
    "- Next: Test if other problem-informed Q designs (spatial weighting) also work with log-additive\n",
    "\n",
    "\n",
    "$$S_{\\text{log}}(C) = \\log \\text{tr}(CQC^T) \\quad \\text{or} \\quad S_{\\text{log}}(C) = \\text{tr}(\\log C \\cdot Q \\cdot \\log C^T)$$\n",
    "\n",
    "Current Q assumes $S(C) = \\text{tr}(CQC^T)$ from Stage 4's additive objective.\n",
    "**If success ~25-40%**:If log-additive requires different form:\n",
    "\n",
    "\n",
    "- Partial improvement suggests some benefit from frequency Q\n",
    "\n",
    "If log-additive requires different form:\n",
    "- Next: Analyze why improvement is limited (adaptive gradients reduce Q importance?)Current Q assumes $S(C) = \\text{tr}(CQC^T)$ from Stage 4's additive objective.\n",
    "\n",
    "$$S_{\\text{log}}(C) = \\log \\text{tr}(CQC^T) \\quad \\text{or} \\quad S_{\\text{log}}(C) = \\text{tr}(\\log C \\cdot Q \\cdot \\log C^T)$$\n",
    "- Next: Try tuning λ specifically for log-additive + frequency Q combination\n",
    "\n",
    "\n",
    "### Connection to Fixed Form Theorem (Stage 8 Question #1)\n",
    "\n",
    "Then frequency Q design may need adaptation for log-additive context.\n",
    "**If success ≤25%**:\n",
    "\n",
    "- No benefit from frequency Q in log-additive context- Next: Design Q specifically for log-additive (Stage 8 Question #3)\n",
    "- Next: Investigate incompatibility (does log transform change frequency structure?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29fb84",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Experiment**: Log-additive + frequency Q (Stage 8, Question #2)\n",
    "\n",
    "**Baseline comparisons**:\n",
    "- Additive + frequency Q (Stage 5): 90%\n",
    "- Log-additive + generic Q (Stage 7): 25%\n",
    "\n",
    "**Result**: **0% success (0/20 trials)** ❌\n",
    "\n",
    "**Interpretation**: \n",
    "- **Incompatibility confirmed**: Frequency Q does NOT work with log-additive objectives\n",
    "- Performs WORSE than log-additive + generic Q (0% vs 25%)\n",
    "\n",
    "- Suggests destructive interference between:4. Map compatible vs incompatible regions of design space systematically\n",
    "\n",
    "  - Adaptive gradients (∂f/∂B = λ/B) from log-additive3. Design log-specific Q: Adapt frequency filtering for log-space\n",
    "\n",
    "  - Frequency-domain constraints from Q2. Test fixed form theorem: Does log-additive need different S(C) form?\n",
    "\n",
    "- Log transform $\\log(\\text{tr}(CQC^T))$ may distort frequency properties1. Understand incompatibility mechanism (why 0% vs 25%?)\n",
    "\n",
    "**Next steps**: \n",
    "\n",
    "**Critical insight**: The 3D design space (Operator × Q_form × Q_design) has **incompatible regions**. Not all combinations work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
