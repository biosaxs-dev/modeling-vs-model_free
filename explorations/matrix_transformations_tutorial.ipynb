{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada6a886",
   "metadata": {},
   "source": [
    "# Understanding Matrix Transformations: A Visual Guide\n",
    "\n",
    "**Goal**: Understand what \"orthogonal transformations\" are and why they matter for matrix factorization.\n",
    "\n",
    "This tutorial is designed for students with basic linear algebra knowledge (matrix multiplication, vectors). We'll build up from simple 2D examples to the general concepts used in the EFA limitation notebooks.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **What do different matrix transformations do?** (stretching, rotating, reflecting)\n",
    "2. **What makes a matrix \"orthogonal\"?** (length-preserving transformations)\n",
    "3. **Why do orthogonal matrices matter?** (they preserve geometric properties)\n",
    "4. **How does this connect to factorization ambiguity?** (infinitely many equivalent solutions)\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites**: You should be comfortable with:\n",
    "- Matrix-vector multiplication\n",
    "- The concept of a basis in linear algebra\n",
    "- What matrix transpose means\n",
    "\n",
    "**Not required**: Group theory, advanced matrix theory, or differential geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# Configure matplotlib for nice plots\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094c616",
   "metadata": {},
   "source": [
    "## Part 1: What Do Matrices Do to Vectors?\n",
    "\n",
    "Let's start with a simple question: when we multiply a vector by a matrix, what happens?\n",
    "\n",
    "We'll look at three types of transformations in 2D:\n",
    "1. **Rotation**: Spin the vector around the origin\n",
    "2. **Scaling**: Stretch or shrink the vector\n",
    "3. **Shear**: Tilt/distort the vector\n",
    "\n",
    "Let's visualize each one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vector_transformation(original, transformed, title, ax):\n",
    "    \"\"\"Helper function to plot before/after vectors\"\"\"\n",
    "    # Plot original vector in blue\n",
    "    ax.arrow(0, 0, original[0], original[1], \n",
    "             head_width=0.15, head_length=0.15, \n",
    "             fc='blue', ec='blue', linewidth=2, label='Original')\n",
    "    \n",
    "    # Plot transformed vector in red\n",
    "    ax.arrow(0, 0, transformed[0], transformed[1], \n",
    "             head_width=0.15, head_length=0.15, \n",
    "             fc='red', ec='red', linewidth=2, alpha=0.7, label='Transformed')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Start with a simple vector\n",
    "v = np.array([2, 1])\n",
    "\n",
    "# Define three different transformations\n",
    "# 1. Rotation by 45 degrees\n",
    "angle = np.pi/4  # 45 degrees in radians\n",
    "R_rotation = np.array([\n",
    "    [np.cos(angle), -np.sin(angle)],\n",
    "    [np.sin(angle),  np.cos(angle)]\n",
    "])\n",
    "\n",
    "# 2. Scaling (stretch by factor of 1.5)\n",
    "R_scale = np.array([\n",
    "    [1.5, 0],\n",
    "    [0, 1.5]\n",
    "])\n",
    "\n",
    "# 3. General shear (affects both dimensions)\n",
    "R_shear = np.array([\n",
    "    [1, 0.5],\n",
    "    [0.3, 1]\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "v_rotated = R_rotation @ v\n",
    "v_scaled = R_scale @ v\n",
    "v_sheared = R_shear @ v\n",
    "\n",
    "# Visualize all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "plot_vector_transformation(v, v_rotated, 'Rotation (45°)', axes[0])\n",
    "plot_vector_transformation(v, v_scaled, 'Scaling (×1.5)', axes[1])\n",
    "plot_vector_transformation(v, v_sheared, 'Shear (tilt)', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the lengths\n",
    "print(\"\\nVector lengths:\")\n",
    "print(f\"Original:  {np.linalg.norm(v):.3f}\")\n",
    "print(f\"Rotated:   {np.linalg.norm(v_rotated):.3f}  ← Same length!\")\n",
    "print(f\"Scaled:    {np.linalg.norm(v_scaled):.3f}  ← Different length\")\n",
    "print(f\"Sheared:   {np.linalg.norm(v_sheared):.3f}  ← Different length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecb769",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "**Rotation preserves length** — the red vector has the same length as the blue vector!  \n",
    "**Scaling and shear change length** — the transformed vectors are longer.\n",
    "\n",
    "This is our first clue about what makes orthogonal matrices special."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a2398",
   "metadata": {},
   "source": [
    "## Part 2: What Makes a Matrix \"Orthogonal\"?\n",
    "\n",
    "A matrix $R$ is called **orthogonal** if it satisfies:\n",
    "\n",
    "$$R^T R = I$$\n",
    "\n",
    "where $R^T$ is the transpose and $I$ is the identity matrix.\n",
    "\n",
    "### What does this mean geometrically?\n",
    "\n",
    "**Orthogonal matrices preserve lengths and angles.**\n",
    "\n",
    "In other words:\n",
    "- If you rotate a vector, it stays the same length\n",
    "- If you rotate two perpendicular vectors, they stay perpendicular\n",
    "- No stretching, no squashing, no distortion\n",
    "\n",
    "Let's verify this mathematically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which matrices are orthogonal\n",
    "def is_orthogonal(R, tol=1e-10):\n",
    "    \"\"\"Check if R^T @ R = I\"\"\"\n",
    "    identity = np.eye(R.shape[0])\n",
    "    product = R.T @ R\n",
    "    return np.allclose(product, identity, atol=tol)\n",
    "\n",
    "print(\"Testing our three matrices:\\n\")\n",
    "\n",
    "print(\"1. Rotation matrix:\")\n",
    "print(R_rotation)\n",
    "print(f\"   R^T @ R = \")\n",
    "print(R_rotation.T @ R_rotation)\n",
    "print(f\"   Is orthogonal? {is_orthogonal(R_rotation)}\\n\")\n",
    "\n",
    "print(\"2. Scaling matrix:\")\n",
    "print(R_scale)\n",
    "print(f\"   R^T @ R = \")\n",
    "print(R_scale.T @ R_scale)\n",
    "print(f\"   Is orthogonal? {is_orthogonal(R_scale)}\\n\")\n",
    "\n",
    "print(\"3. Shear matrix:\")\n",
    "print(R_shear)\n",
    "print(f\"   R^T @ R = \")\n",
    "print(R_shear.T @ R_shear)\n",
    "print(f\"   Is orthogonal? {is_orthogonal(R_shear)}\\n\")\n",
    "\n",
    "print(\"✓ Only the rotation matrix is orthogonal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f73e94",
   "metadata": {},
   "source": [
    "## Part 3: The Two Types of Orthogonal Transformations\n",
    "\n",
    "Orthogonal matrices come in two flavors, distinguished by their **determinant**:\n",
    "\n",
    "1. **Proper rotations**: $\\det(R) = +1$ (pure rotation)\n",
    "2. **Improper transformations**: $\\det(R) = -1$ (rotation + reflection)\n",
    "\n",
    "Let's see examples of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rotation (det = +1)\n",
    "angle = np.pi/6  # 30 degrees\n",
    "R_proper = np.array([\n",
    "    [np.cos(angle), -np.sin(angle)],\n",
    "    [np.sin(angle),  np.cos(angle)]\n",
    "])\n",
    "\n",
    "# Create a reflection across x-axis (det = -1)\n",
    "R_improper = np.array([\n",
    "    [1,  0],\n",
    "    [0, -1]\n",
    "])\n",
    "\n",
    "# Apply to a test vector\n",
    "v_test = np.array([1.5, 1])\n",
    "v_proper = R_proper @ v_test\n",
    "v_improper = R_improper @ v_test\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_vector_transformation(v_test, v_proper, \n",
    "                          f'Proper Rotation (det = {np.linalg.det(R_proper):.1f})', \n",
    "                          axes[0])\n",
    "plot_vector_transformation(v_test, v_improper, \n",
    "                          f'Reflection (det = {np.linalg.det(R_improper):.1f})', \n",
    "                          axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBoth transformations:\")\n",
    "print(f\"- Preserve length: {np.linalg.norm(v_test):.3f} → {np.linalg.norm(v_proper):.3f} and {np.linalg.norm(v_improper):.3f}\")\n",
    "print(f\"- Are orthogonal: R^T R = I? {is_orthogonal(R_proper)} and {is_orthogonal(R_improper)}\")\n",
    "print(f\"\\nBut they differ in determinant:\")\n",
    "print(f\"- Rotation: det = {np.linalg.det(R_proper):.0f} (orientation preserved)\")\n",
    "print(f\"- Reflection: det = {np.linalg.det(R_improper):.0f} (orientation flipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09055bd",
   "metadata": {},
   "source": [
    "### Mathematical Classification\n",
    "\n",
    "The complete set of orthogonal matrices is called the **orthogonal group**, denoted $O(n)$ for $n \\times n$ matrices.\n",
    "\n",
    "This group has two parts:\n",
    "- **$SO(n)$** = Special Orthogonal group = proper rotations only ($\\det = +1$)\n",
    "- **$O(n) \\setminus SO(n)$** = improper transformations ($\\det = -1$)\n",
    "\n",
    "Think of it like this:\n",
    "- $O(n)$ = all length-preserving transformations\n",
    "- $SO(n)$ = length-preserving transformations that don't flip orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8934e0",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Effect on Shapes\n",
    "\n",
    "To really understand what orthogonal transformations do, let's apply them to an entire shape (not just one vector).\n",
    "\n",
    "We'll transform a square and see what happens with different types of transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a square (as column vectors)\n",
    "square = np.array([\n",
    "    [0, 2, 2, 0, 0],  # x-coordinates\n",
    "    [0, 0, 2, 2, 0]   # y-coordinates\n",
    "])\n",
    "\n",
    "# Apply our four transformations (from Part 1)\n",
    "square_rotated = R_rotation @ square\n",
    "square_scaled = R_scale @ square\n",
    "square_sheared = R_shear @ square\n",
    "\n",
    "# Add reflection (from Part 3)\n",
    "square_reflected = R_improper @ square\n",
    "\n",
    "# Plot all five\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "def plot_shape(points, ax, title, color='blue'):\n",
    "    ax.plot(points[0, :], points[1, :], 'o-', color=color, linewidth=2, markersize=8)\n",
    "    ax.fill(points[0, :], points[1, :], color=color, alpha=0.2)\n",
    "    ax.set_xlim(-3, 4)\n",
    "    ax.set_ylim(-3, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "plot_shape(square, axes[0], 'Original', 'blue')\n",
    "plot_shape(square_rotated, axes[1], 'Rotated\\n(orthogonal, det=+1)', 'green')\n",
    "plot_shape(square_reflected, axes[2], 'Reflected\\n(orthogonal, det=-1)', 'darkgreen')\n",
    "plot_shape(square_scaled, axes[3], 'Scaled\\n(NOT orthogonal)', 'orange')\n",
    "plot_shape(square_sheared, axes[4], 'Sheared\\n(NOT orthogonal)', 'red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"✓ Rotation: Square stays a square (sides equal, angles 90°, det=+1)\")\n",
    "print(\"✓ Reflection: Square stays a square but flipped (det=-1)\")\n",
    "print(\"✗ Scaling: Square becomes larger but stays square (changes lengths)\")\n",
    "print(\"✗ Shear: Square becomes a parallelogram (angles AND alignment changed!)\")\n",
    "print(\"   Note: General shear affects both x and y, tilting shape in both directions\")\n",
    "print(\"\\n→ Both orthogonal transformations preserve the square's geometry!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefeb49",
   "metadata": {},
   "source": [
    "## Part 4.5: 3D Examples — New Phenomena Not Observable in 2D\n",
    "\n",
    "In 3D, orthogonal transformations become richer! Let's explore cases that don't exist in 2D.\n",
    "\n",
    "### What's New in 3D?\n",
    "\n",
    "In 2D, $O(2)$ has only:\n",
    "- Rotations around the origin (det = +1)\n",
    "- Reflections across a line (det = -1)\n",
    "\n",
    "In 3D, $O(3)$ includes:\n",
    "1. **Rotations around an axis** (det = +1) — generalization of 2D rotation\n",
    "2. **Reflections across a plane** (det = -1) — generalization of 2D reflection\n",
    "3. **Rotoinversion** (det = -1) — rotation + inversion through origin (NEW!)\n",
    "4. **Inversion through origin** (det = -1) — special case: multiply all coordinates by -1\n",
    "\n",
    "Let's visualize these with a 3D object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to draw parallelepipeds (from molass-legacy)\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection\n",
    "\n",
    "def parallelogram(origin, v1, v2):\n",
    "    \"\"\"Create a parallelogram from origin and two vectors\"\"\"\n",
    "    return [origin, origin+v1, origin+v1+v2, origin+v2]\n",
    "\n",
    "def plot_parallelepiped_colored(ax, origin, vectors, face_colors=None, edgecolor='black', alpha=0.8):\n",
    "    \"\"\"\n",
    "    Plot a parallelepiped with colored faces adjacent to origin and neutral opposite faces.\n",
    "    \n",
    "    Parameters:\n",
    "    - origin: starting point [x, y, z]\n",
    "    - vectors: list of 3 vectors defining the edges\n",
    "    - face_colors: list of 3 colors for the three origin-adjacent faces (default: ['red', 'green', 'blue'])\n",
    "    - edgecolor: edge color\n",
    "    - alpha: transparency for origin-adjacent faces\n",
    "    \"\"\"\n",
    "    if face_colors is None:\n",
    "        face_colors = ['salmon', 'lightgreen', 'lightblue']  # RGB-ish for X, Y, Z faces\n",
    "    \n",
    "    verts_origin = []\n",
    "    colors_origin = []\n",
    "    verts_opposite = []\n",
    "    \n",
    "    for idx, (i, j, k) in enumerate([(0,1,2), (1,2,0), (2,0,1)]):\n",
    "        vi = np.array(vectors[i])\n",
    "        vj = np.array(vectors[j])\n",
    "        vk = np.array(vectors[k])\n",
    "        \n",
    "        # Face at origin (perpendicular to vector k) - COLORED\n",
    "        verts_origin.append(parallelogram(np.zeros(3), vi, vj))\n",
    "        colors_origin.append(face_colors[idx])\n",
    "        \n",
    "        # Opposite face (shifted by vector k) - NEUTRAL & MORE TRANSPARENT\n",
    "        verts_opposite.append(parallelogram(vk, vi, vj))\n",
    "    \n",
    "    # Add origin-adjacent faces with normal transparency\n",
    "    ax.add_collection3d(Poly3DCollection(verts_origin, facecolors=colors_origin, linewidths=1, alpha=alpha, edgecolors=edgecolor))\n",
    "    # Add opposite faces with higher transparency\n",
    "    ax.add_collection3d(Poly3DCollection(verts_opposite, facecolors='whitesmoke', linewidths=1, alpha=0.15, edgecolors=edgecolor))\n",
    "    # Add edges\n",
    "    all_verts = verts_origin + verts_opposite\n",
    "    ax.add_collection3d(Line3DCollection(all_verts, colors=edgecolor, linewidths=1.5, linestyles='-'))\n",
    "\n",
    "# Define a unit cube using three orthogonal unit vectors\n",
    "unit_vectors = [\n",
    "    np.array([1, 0, 0]),  # x-direction\n",
    "    np.array([0, 1, 0]),  # y-direction\n",
    "    np.array([0, 0, 1])   # z-direction\n",
    "]\n",
    "\n",
    "# Define face colors that match the perpendicular axes\n",
    "# Strategy: face color matches the axis it's perpendicular to\n",
    "# Loop order [(0,1,2), (1,2,0), (2,0,1)] creates faces: xy, yz, zx\n",
    "face_colors = ['skyblue', 'salmon', 'lightgreen']  # xy-faces (⊥Z-axis), yz-faces (⊥X-axis), zx-faces (⊥Y-axis)\n",
    "\n",
    "# Define 3D transformations (from Part 3)\n",
    "# 1. Rotation around z-axis by 45° (det = +1)\n",
    "angle_3d = np.pi/4\n",
    "R_rot_z = np.array([\n",
    "    [np.cos(angle_3d), -np.sin(angle_3d), 0],\n",
    "    [np.sin(angle_3d),  np.cos(angle_3d), 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# 2. Reflection across xy-plane (flip z-coordinate, det = -1)\n",
    "R_reflect_xy = np.array([\n",
    "    [1,  0,  0],\n",
    "    [0,  1,  0],\n",
    "    [0,  0, -1]\n",
    "])\n",
    "\n",
    "# 3. Rotoinversion: 120° rotation around z-axis + inversion (det = -1)\n",
    "# This is a 3-fold rotoinversion axis (common in crystallography)\n",
    "angle_120 = 2*np.pi/3  # 120 degrees\n",
    "R_rot_120 = np.array([  # Just the rotation part (for visualization)\n",
    "    [np.cos(angle_120), -np.sin(angle_120), 0],\n",
    "    [np.sin(angle_120),  np.cos(angle_120), 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "R_rotoinv = -R_rot_120  # Negative sign = inversion after rotation\n",
    "\n",
    "# 4. Pure inversion: (x,y,z) → (-x,-y,-z) (det = -1)\n",
    "R_inversion = -np.eye(3)\n",
    "\n",
    "# 5. For comparison: Scaling (NOT orthogonal)\n",
    "R_scale_3d = np.array([\n",
    "    [1.5,  0,  0],\n",
    "    [0,  1.5,  0],\n",
    "    [0,  0, 0.5]\n",
    "])\n",
    "\n",
    "# 6. For comparison: General shear (NOT orthogonal)\n",
    "# This shear affects all three directions so no edge stays on an axis\n",
    "R_shear_3d = np.array([\n",
    "    [1,   0.5,  0.4],\n",
    "    [0.3,  1,   0.5],\n",
    "    [0,    0,    1]\n",
    "])\n",
    "\n",
    "# Apply transformations to the unit cube vectors\n",
    "def transform_vectors(R, vecs):\n",
    "    \"\"\"Apply transformation R to a list of vectors\"\"\"\n",
    "    return [R @ v for v in vecs]\n",
    "\n",
    "cube_rotated = transform_vectors(R_rot_z, unit_vectors)\n",
    "cube_reflected = transform_vectors(R_reflect_xy, unit_vectors)\n",
    "cube_rot_120 = transform_vectors(R_rot_120, unit_vectors)  # Intermediate step\n",
    "cube_rotoinv = transform_vectors(R_rotoinv, unit_vectors)\n",
    "cube_inverted = transform_vectors(R_inversion, unit_vectors)\n",
    "cube_scaled = transform_vectors(R_scale_3d, unit_vectors)\n",
    "cube_sheared = transform_vectors(R_shear_3d, unit_vectors)\n",
    "\n",
    "# Verify orthogonality\n",
    "print(\"Checking 3D transformations:\\n\")\n",
    "print(\"ORTHOGONAL transformations (preserve cube shape):\")\n",
    "print(f\"  Rotation around z-axis:    det = {np.linalg.det(R_rot_z):+.1f},  orthogonal? {is_orthogonal(R_rot_z)}\")\n",
    "print(f\"  Reflection across xy-plane: det = {np.linalg.det(R_reflect_xy):+.1f},  orthogonal? {is_orthogonal(R_reflect_xy)}\")\n",
    "print(f\"  Rotoinversion:             det = {np.linalg.det(R_rotoinv):+.1f},  orthogonal? {is_orthogonal(R_rotoinv)}\")\n",
    "print(f\"  Pure inversion:            det = {np.linalg.det(R_inversion):+.1f},  orthogonal? {is_orthogonal(R_inversion)}\")\n",
    "print(\"\\nNON-ORTHOGONAL transformations (distort cube):\")\n",
    "print(f\"  Scaling:                   det = {np.linalg.det(R_scale_3d):+.1f},  orthogonal? {is_orthogonal(R_scale_3d)}\")\n",
    "print(f\"  Shear:                     det = {np.linalg.det(R_shear_3d):+.1f},  orthogonal? {is_orthogonal(R_shear_3d)}\")\n",
    "\n",
    "print(\"\\n✓ Color coding strategy:\")\n",
    "print(\"  Origin-adjacent faces (where 3 faces meet at origin):\")\n",
    "print(\"    • Blue face (xy-plane) ⊥ Z-axis (blue)\")\n",
    "print(\"    • Salmon face (yz-plane) ⊥ X-axis (red)\")\n",
    "print(\"    • Green face (zx-plane) ⊥ Y-axis (green)\")\n",
    "print(\"  Opposite faces: whitesmoke (neutral, to reduce visual clutter)\")\n",
    "print(\"✓ Watch how the 3 colored faces move to see the transformation!\")\n",
    "print(\"✓ Note: Rotoinversion and pure inversion don't exist as single transformations in 2D!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all transformations as colored parallelepipeds\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "origin = np.array([0, 0, 0])\n",
    "\n",
    "# Set viewing angles for all 3D plots (adjust these to change all views at once)\n",
    "viewing_elev = 20  # Elevation angle (degrees) - higher = view from above\n",
    "viewing_azim = -70  # Azimuth angle (degrees) - controls left/right viewing angle\n",
    "\n",
    "# Helper function to draw coordinate axes spanning the full frame\n",
    "def draw_axes(ax, axis_range=2, linewidth=2.5, zorder=1):\n",
    "    \"\"\"Draw X, Y, Z axes spanning the full plot range\"\"\"\n",
    "    # X-axis (red) - spans from -axis_range to +axis_range\n",
    "    ax.plot([-axis_range, axis_range], [0, 0], [0, 0], 'r-', linewidth=linewidth, alpha=0.6, zorder=zorder)\n",
    "    # Y-axis (green)\n",
    "    ax.plot([0, 0], [-axis_range, axis_range], [0, 0], 'g-', linewidth=linewidth, alpha=0.6, zorder=zorder)\n",
    "    # Z-axis (blue)\n",
    "    ax.plot([0, 0], [0, 0], [-axis_range, axis_range], 'b-', linewidth=linewidth, alpha=0.6, zorder=zorder)\n",
    "\n",
    "# Row 1: Original cube and orthogonal transformations (det = +1)\n",
    "ax1 = fig.add_subplot(2, 4, 1, projection='3d')\n",
    "draw_axes(ax1)\n",
    "plot_parallelepiped_colored(ax1, origin, unit_vectors, face_colors=face_colors, edgecolor='black', alpha=0.5)\n",
    "ax1.set_title('Original Cube\\n(orthogonal basis)', fontweight='bold', fontsize=11)\n",
    "ax1.set_xlim(-2, 2); ax1.set_ylim(-2, 2); ax1.set_zlim(-2, 2)\n",
    "ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.set_zlabel('Z')\n",
    "ax1.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "ax2 = fig.add_subplot(2, 4, 2, projection='3d')\n",
    "draw_axes(ax2)\n",
    "plot_parallelepiped_colored(ax2, origin, cube_rotated, face_colors=face_colors, edgecolor='black', alpha=0.5)\n",
    "ax2.set_title('Rotation (det=+1)\\n45° around z-axis', fontweight='bold', fontsize=11)\n",
    "ax2.set_xlim(-2, 2); ax2.set_ylim(-2, 2); ax2.set_zlim(-2, 2)\n",
    "ax2.set_xlabel('X'); ax2.set_ylabel('Y'); ax2.set_zlabel('Z')\n",
    "ax2.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "ax3 = fig.add_subplot(2, 4, 3, projection='3d')\n",
    "draw_axes(ax3)\n",
    "plot_parallelepiped_colored(ax3, origin, cube_reflected, face_colors=face_colors, edgecolor='black', alpha=0.5)\n",
    "ax3.set_title('Reflection (det=-1)\\nacross xy-plane', fontweight='bold', fontsize=11)\n",
    "ax3.set_xlim(-2, 2); ax3.set_ylim(-2, 2); ax3.set_zlim(-2, 2)\n",
    "ax3.set_xlabel('X'); ax3.set_ylabel('Y'); ax3.set_zlabel('Z')\n",
    "ax3.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "ax4 = fig.add_subplot(2, 4, 4, projection='3d')\n",
    "draw_axes(ax4)\n",
    "plot_parallelepiped_colored(ax4, origin, cube_rot_120, face_colors=face_colors, edgecolor='black', alpha=0.5)\n",
    "ax4.set_title('Rotation 120° (step 1)\\nbefore inversion', fontweight='bold', fontsize=11)\n",
    "ax4.set_xlim(-2, 2); ax4.set_ylim(-2, 2); ax4.set_zlim(-2, 2)\n",
    "ax4.set_xlabel('X'); ax4.set_ylabel('Y'); ax4.set_zlabel('Z')\n",
    "ax4.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "# Row 2: New 3D phenomena (improper orthogonal) and non-orthogonal distortions\n",
    "ax5 = fig.add_subplot(2, 4, 5, projection='3d')\n",
    "plot_parallelepiped_colored(ax5, origin, cube_rotoinv, face_colors=face_colors, edgecolor='black', alpha=0.5)\n",
    "draw_axes(ax5, zorder=100)  # High zorder to draw axes in front\n",
    "ax5.set_title('Rotoinversion (step 2)\\nafter inversion', fontweight='bold', fontsize=11)\n",
    "ax5.set_xlim(-2, 2); ax5.set_ylim(-2, 2); ax5.set_zlim(-2, 2)\n",
    "ax5.set_xlabel('X'); ax5.set_ylabel('Y'); ax5.set_zlabel('Z')\n",
    "ax5.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "ax6 = fig.add_subplot(2, 4, 6, projection='3d')\n",
    "plot_parallelepiped_colored(ax6, origin, cube_inverted, face_colors=face_colors, edgecolor='black', alpha=0.5)\n",
    "draw_axes(ax6, zorder=100)  # High zorder to draw axes in front\n",
    "ax6.set_title('Pure Inversion (det=-1)\\n(x,y,z)→(-x,-y,-z)', fontweight='bold', fontsize=11)\n",
    "ax6.set_xlim(-2, 2); ax6.set_ylim(-2, 2); ax6.set_zlim(-2, 2)\n",
    "ax6.set_xlabel('X'); ax6.set_ylabel('Y'); ax6.set_zlabel('Z')\n",
    "ax6.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "ax7 = fig.add_subplot(2, 4, 7, projection='3d')\n",
    "draw_axes(ax7)\n",
    "plot_parallelepiped_colored(ax7, origin, cube_scaled, face_colors=['yellow', 'yellow', 'yellow'], edgecolor='orange', alpha=0.7)\n",
    "ax7.set_title('Scaling (NOT orthogonal)\\nstretches/compresses', fontweight='bold', fontsize=11)\n",
    "ax7.set_xlim(-2, 2); ax7.set_ylim(-2, 2); ax7.set_zlim(-2, 2)\n",
    "ax7.set_xlabel('X'); ax7.set_ylabel('Y'); ax7.set_zlabel('Z')\n",
    "ax7.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "ax8 = fig.add_subplot(2, 4, 8, projection='3d')\n",
    "draw_axes(ax8)\n",
    "plot_parallelepiped_colored(ax8, origin, cube_sheared, face_colors=['pink', 'pink', 'pink'], edgecolor='red', alpha=0.7)\n",
    "ax8.set_title('Shear (NOT orthogonal)\\nchanges angles', fontweight='bold', fontsize=11)\n",
    "ax8.set_xlim(-2, 2); ax8.set_ylim(-2, 2); ax8.set_zlim(-2, 2)\n",
    "ax8.set_xlabel('X'); ax8.set_ylabel('Y'); ax8.set_zlabel('Z')\n",
    "ax8.view_init(elev=viewing_elev, azim=viewing_azim)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATIONS IN 3D:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Coordinate axes: Red=X, Green=Y, Blue=Z (spanning full frame)\")\n",
    "print(\"Cube faces match perpendicular axis color:\")\n",
    "print(\"  • Blue faces (top/bottom, xy-plane) are perpendicular to Blue Z-axis\")\n",
    "print(\"  • Salmon faces (left/right, yz-plane) are perpendicular to Red X-axis\")\n",
    "print(\"  • Green faces (front/back, zx-plane) are perpendicular to Green Y-axis\")\n",
    "print()\n",
    "print(\"✓ Top row & bottom-left: CUBE remains a CUBE (all orthogonal)\")\n",
    "print(\"  → Watch the colored faces to see how orientation changes!\")\n",
    "print(\"  → Rotation 45°: blue face stays on top (rotates around Z-axis)\")\n",
    "print(\"  → Reflection: blue face flips to bottom (negative Z)\")\n",
    "print(\"  → Rotation 120° (step 1 of rotoinversion): shows intermediate state\")\n",
    "print(\"  → Rotoinversion (step 2 = inversion): all coordinates negated after rotation\")\n",
    "print(\"  → Pure Inversion: all faces flip through origin\")\n",
    "print()\n",
    "print(\"  → Orthogonal transformations preserve:\")\n",
    "print(\"     - Edge lengths (all edges still length 1)\")\n",
    "print(\"     - Angles (all angles still 90°)\")\n",
    "print(\"     - Volume (still 1 cubic unit)\")\n",
    "print()\n",
    "print(\"✗ Bottom-right two: CUBE becomes PARALLELEPIPED (non-orthogonal)\")\n",
    "print(\"  → All faces shown in uniform color (yellow/pink) to emphasize distortion\")\n",
    "print(\"  → Scaling: edges have different lengths\")\n",
    "print(\"  → Shear: angles are no longer 90°\")\n",
    "print()\n",
    "print(\"→ In 3D, O(3) includes rotations, reflections, rotoinversions, and inversion\")\n",
    "print(\"→ All preserve the cube structure, just reorient or flip it\")\n",
    "print(\"→ Rotoinversion = rotation THEN inversion (2-step process, det=-1)\")\n",
    "print(\"→ Rotoinversion is particularly important in crystallography\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical verification: Compute edge lengths, angles, and volumes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NUMERICAL VERIFICATION OF GEOMETRIC PROPERTIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_edge_lengths(vectors):\n",
    "    \"\"\"Compute lengths of the three edge vectors\"\"\"\n",
    "    return [np.linalg.norm(v) for v in vectors]\n",
    "\n",
    "def compute_angles(vectors):\n",
    "    \"\"\"Compute angles between pairs of edge vectors (in degrees)\"\"\"\n",
    "    angles = []\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i+1, len(vectors)):\n",
    "            v1, v2 = vectors[i], vectors[j]\n",
    "            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            angle_deg = np.arccos(np.clip(cos_angle, -1, 1)) * 180 / np.pi\n",
    "            angles.append(angle_deg)\n",
    "    return angles\n",
    "\n",
    "def compute_volume(vectors):\n",
    "    \"\"\"Compute volume of parallelepiped (absolute value of determinant)\"\"\"\n",
    "    matrix = np.column_stack(vectors)\n",
    "    return abs(np.linalg.det(matrix))\n",
    "\n",
    "# Compute properties for all transformations\n",
    "print(\"\\n1. EDGE LENGTHS (should be [1.0, 1.0, 1.0] for orthogonal):\")\n",
    "print(f\"   Original:        {compute_edge_lengths(unit_vectors)}\")\n",
    "print(f\"   Rotation 45°:    {compute_edge_lengths(cube_rotated)}  ✓\")\n",
    "print(f\"   Reflection:      {compute_edge_lengths(cube_reflected)}  ✓\")\n",
    "print(f\"   Rotoinversion:   {compute_edge_lengths(cube_rotoinv)}  ✓\")\n",
    "print(f\"   Pure inversion:  {compute_edge_lengths(cube_inverted)}  ✓\")\n",
    "print(f\"   Scaling:         {[f'{x:.2f}' for x in compute_edge_lengths(cube_scaled)]}  ✗ Changed!\")\n",
    "print(f\"   Shear:           {[f'{x:.2f}' for x in compute_edge_lengths(cube_sheared)]}  ✗ Changed!\")\n",
    "\n",
    "print(\"\\n2. ANGLES BETWEEN EDGES (should be 90° for orthogonal):\")\n",
    "print(f\"   Original:        {[f'{a:.1f}°' for a in compute_angles(unit_vectors)]}\")\n",
    "print(f\"   Rotation 45°:    {[f'{a:.1f}°' for a in compute_angles(cube_rotated)]}  ✓\")\n",
    "print(f\"   Reflection:      {[f'{a:.1f}°' for a in compute_angles(cube_reflected)]}  ✓\")\n",
    "print(f\"   Rotoinversion:   {[f'{a:.1f}°' for a in compute_angles(cube_rotoinv)]}  ✓\")\n",
    "print(f\"   Pure inversion:  {[f'{a:.1f}°' for a in compute_angles(cube_inverted)]}  ✓\")\n",
    "print(f\"   Scaling:         {[f'{a:.1f}°' for a in compute_angles(cube_scaled)]}  ✓ (still 90°)\")\n",
    "print(f\"   Shear:           {[f'{a:.1f}°' for a in compute_angles(cube_sheared)]}  ✗ Changed!\")\n",
    "\n",
    "print(\"\\n3. VOLUME (should be 1.0 for unit cube):\")\n",
    "print(f\"   Original:        {compute_volume(unit_vectors):.3f}\")\n",
    "print(f\"   Rotation 45°:    {compute_volume(cube_rotated):.3f}  ✓ (= |det|)\")\n",
    "print(f\"   Reflection:      {compute_volume(cube_reflected):.3f}  ✓\")\n",
    "print(f\"   Rotoinversion:   {compute_volume(cube_rotoinv):.3f}  ✓\")\n",
    "print(f\"   Pure inversion:  {compute_volume(cube_inverted):.3f}  ✓\")\n",
    "print(f\"   Scaling:         {compute_volume(cube_scaled):.3f}  ✗ Changed (det={np.linalg.det(R_scale_3d):.2f})\")\n",
    "print(f\"   Shear:           {compute_volume(cube_sheared):.3f}  ✗ Changed (det={np.linalg.det(R_shear_3d):.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ ORTHOGONAL transformations preserve:\")\n",
    "print(\"  • All edge lengths (always 1.0)\")\n",
    "print(\"  • All angles between edges (always 90°)\")\n",
    "print(\"  • Volume magnitude (always 1.0, though sign may flip)\")\n",
    "print(\"\")\n",
    "print(\"✗ NON-ORTHOGONAL transformations:\")\n",
    "print(\"  • Scaling: Changes edge lengths but preserves angles\")\n",
    "print(\"  • Shear: Changes both edge lengths AND angles\")\n",
    "print(\"\")\n",
    "print(\"→ Rotoinversion ≠ Reflection ≠ Pure Inversion\")\n",
    "print(\"  All three have det=-1 but produce different orientations!\")\n",
    "print(\"  Rotoinversion combines rotation + inversion in a way that\")\n",
    "print(\"  cannot be achieved by reflection alone.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67992c45",
   "metadata": {},
   "source": [
    "### Understanding the Dimension of $O(n)$\n",
    "\n",
    "Why does the dimension of the ambiguity space matter?\n",
    "\n",
    "- **2D case**: $O(2)$ has dimension $\\frac{2(2-1)}{2} = 1$ → continuous rotation by angle $\\theta$\n",
    "- **3D case**: $O(3)$ has dimension $\\frac{3(3-1)}{2} = 3$ → three independent rotation angles (Euler angles)\n",
    "- **$n$-D case**: $O(n)$ has dimension $\\frac{n(n-1)}{2}$ → grows quadratically!\n",
    "\n",
    "For EFA with 2 components, this means:\n",
    "- Level 2 (smoothness only): 1-dimensional continuous ambiguity\n",
    "- Need non-negativity (Level 3) to reduce this to discrete choices\n",
    "\n",
    "For EFA with 3 components:\n",
    "- Level 2: 3-dimensional continuous ambiguity (much worse!)\n",
    "- Non-negativity becomes even more critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57714d35",
   "metadata": {},
   "source": [
    "## Part 5: Connection to Matrix Factorization\n",
    "\n",
    "Now let's connect this to the factorization problem in EFA/REGALS.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We have data $M$ and want to find $P$ and $C$ such that:\n",
    "$$M = P \\cdot C$$\n",
    "\n",
    "But there's ambiguity! If we insert any invertible matrix $R$ and its inverse:\n",
    "$$M = P \\cdot C = (P R) \\cdot (R^{-1} C)$$\n",
    "\n",
    "This gives us **infinitely many solutions** that fit the data equally well.\n",
    "\n",
    "### The Question\n",
    "\n",
    "**Which transformations $R$ should we allow?**\n",
    "\n",
    "Let's demonstrate with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4776aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple synthetic data: M = P @ C\n",
    "np.random.seed(123)\n",
    "\n",
    "# Original factorization\n",
    "P_true = np.random.rand(10, 2) * 2  # 10 rows, 2 components\n",
    "C_true = np.random.rand(2, 5) * 2   # 2 components, 5 columns\n",
    "M = P_true @ C_true\n",
    "\n",
    "print(\"Original factorization: M = P @ C\")\n",
    "print(f\"M shape: {M.shape}\")\n",
    "print(f\"P shape: {P_true.shape}\")\n",
    "print(f\"C shape: {C_true.shape}\")\n",
    "print(f\"\\nData matrix M (first 5 rows):\")\n",
    "print(M[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66af05",
   "metadata": {},
   "source": [
    "### Testing Different Transformations\n",
    "\n",
    "Let's try three types of transformations and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Orthogonal transformation (rotation)\n",
    "angle = np.pi/3  # 60 degrees\n",
    "R_orth = np.array([\n",
    "    [np.cos(angle), -np.sin(angle)],\n",
    "    [np.sin(angle),  np.cos(angle)]\n",
    "])\n",
    "\n",
    "# 2. Scaling transformation\n",
    "R_scale_2d = np.array([\n",
    "    [2, 0],\n",
    "    [0, 0.5]\n",
    "])\n",
    "\n",
    "# 3. Shear transformation\n",
    "R_shear_2d = np.array([\n",
    "    [1, 0.8],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "# Apply transformations: M = (P @ R) @ (R^-1 @ C)\n",
    "P_orth = P_true @ R_orth\n",
    "C_orth = np.linalg.inv(R_orth) @ C_true\n",
    "M_orth = P_orth @ C_orth\n",
    "\n",
    "P_scaled = P_true @ R_scale_2d\n",
    "C_scaled = np.linalg.inv(R_scale_2d) @ C_true\n",
    "M_scaled = P_scaled @ C_scaled\n",
    "\n",
    "P_sheared = P_true @ R_shear_2d\n",
    "C_sheared = np.linalg.inv(R_shear_2d) @ C_true\n",
    "M_sheared = P_sheared @ C_sheared\n",
    "\n",
    "# Check if we still get M\n",
    "print(\"Do all transformations reproduce M exactly?\\n\")\n",
    "print(f\"Orthogonal: max error = {np.max(np.abs(M - M_orth)):.2e}  ✓\")\n",
    "print(f\"Scaling:    max error = {np.max(np.abs(M - M_scaled)):.2e}  ✓\")\n",
    "print(f\"Shear:      max error = {np.max(np.abs(M - M_sheared)):.2e}  ✓\")\n",
    "print(\"\\n→ ALL transformations reproduce the data perfectly!\")\n",
    "print(\"→ We need additional constraints to choose between them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa86ef",
   "metadata": {},
   "source": [
    "## Part 6: Why Does Smoothness Restrict to Orthogonal Transformations?\n",
    "\n",
    "This is the key insight from the REGALS method!\n",
    "\n",
    "### The Smoothness Penalty\n",
    "\n",
    "REGALS adds a penalty for non-smooth concentration profiles:\n",
    "$$\\text{Objective} = \\|M - PC\\|^2 + \\lambda \\|D^2 C\\|^2$$\n",
    "\n",
    "where $D^2$ is the second derivative operator (measures curvature).\n",
    "\n",
    "### The Magic Property\n",
    "\n",
    "**The smoothness penalty is orthogonal-invariant:**\n",
    "$$\\|D^2(R^{-1}C)\\|^2 = \\|D^2 C\\|^2 \\quad \\text{if and only if } R \\in O(n)$$\n",
    "\n",
    "This means:\n",
    "- ✓ Orthogonal transformations: smoothness unchanged → allowed\n",
    "- ✗ Non-orthogonal transformations: smoothness changes → penalized/eliminated\n",
    "\n",
    "Let's verify this numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second derivative operator for our 5-point curves\n",
    "n_points = 5\n",
    "D2 = np.zeros((n_points - 2, n_points))\n",
    "for i in range(n_points - 2):\n",
    "    D2[i, i:i+3] = [1, -2, 1]  # Second difference\n",
    "\n",
    "print(\"Second derivative operator D2:\")\n",
    "print(D2)\n",
    "\n",
    "# Compute smoothness for each factorization\n",
    "def smoothness(C):\n",
    "    \"\"\"Compute smoothness penalty ||D^2 C||^2\"\"\"\n",
    "    return np.linalg.norm(D2 @ C.T) ** 2\n",
    "\n",
    "smooth_true = smoothness(C_true)\n",
    "smooth_orth = smoothness(C_orth)\n",
    "smooth_scaled = smoothness(C_scaled)\n",
    "smooth_sheared = smoothness(C_sheared)\n",
    "\n",
    "print(\"\\nSmoothness values:\")\n",
    "print(f\"True factorization: {smooth_true:.4f}\")\n",
    "print(f\"After rotation:     {smooth_orth:.4f}  (change: {abs(smooth_orth - smooth_true):.2e})  ✓ Unchanged!\")\n",
    "print(f\"After scaling:      {smooth_scaled:.4f}  (change: {abs(smooth_scaled - smooth_true):.4f})  ✗ Changed\")\n",
    "print(f\"After shear:        {smooth_sheared:.4f}  (change: {abs(smooth_sheared - smooth_true):.4f})  ✗ Changed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"Only orthogonal transformations preserve the smoothness penalty!\")\n",
    "print(\"This is why REGALS Level 2 restricts ambiguity from 'any matrix' to O(n).\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a2e47",
   "metadata": {},
   "source": [
    "## Part 7: Summary and Connection to EFA Limitations\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Orthogonal matrices** ($R^T R = I$) preserve lengths and angles\n",
    "   - They include rotations ($\\det = +1$) and reflections ($\\det = -1$)\n",
    "   - Together they form the orthogonal group $O(n)$\n",
    "\n",
    "2. **Matrix factorization has inherent ambiguity**\n",
    "   - $M = PC = (PR)(R^{-1}C)$ for ANY invertible $R$\n",
    "   - We need constraints to reduce this ambiguity\n",
    "\n",
    "3. **Smoothness regularization restricts to $O(n)$**\n",
    "   - The smoothness penalty $\\|D^2 C\\|^2$ is preserved by orthogonal transformations\n",
    "   - Non-orthogonal transformations (scaling, shear) change the smoothness value\n",
    "   - This naturally eliminates most of the ambiguity!\n",
    "\n",
    "### The Four-Level Hierarchy (Preview)\n",
    "\n",
    "The full constraint hierarchy in the factorization problem:\n",
    "\n",
    "| Level | Constraints | Ambiguity Group | Dimension |\n",
    "|-------|------------|----------------|----------|\n",
    "| 1 | Data fit only | All invertible matrices | $n^2$ |\n",
    "| 2 | + Smoothness | **Orthogonal group $O(n)$** | $\\frac{n(n-1)}{2}$ |\n",
    "| 3 | + Non-negativity | Permutation + small rotation | $\\approx n$ |\n",
    "| 4 | + Physical constraints | Usually unique | 0 |\n",
    "\n",
    "**The jump from Level 1 → Level 2** is where understanding orthogonal transformations becomes crucial!\n",
    "\n",
    "### Connection to limitation_4 Notebook\n",
    "\n",
    "The `limitation_4_no_quantification.ipynb` notebook demonstrates this ambiguity problem:\n",
    "- Shows that EFA (Level 2) still has continuous rotation ambiguity\n",
    "- Demonstrates that different rotations give equally good fits\n",
    "- Explains why non-negativity (Level 3) is needed for practical uniqueness\n",
    "\n",
    "Now you should be able to understand the mathematical concepts in that notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201d4d2",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "If you want to go deeper:\n",
    "\n",
    "1. **For orthogonal matrices**: [Wikipedia: Orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix)\n",
    "2. **For the factorization ambiguity**: See `explorations/underdeterminedness_exploration.ipynb`\n",
    "3. **For the complete constraint hierarchy**: See `explorations/REGALS_analysis_summary.md`\n",
    "4. **For practical implications**: See `evidence/efa_original/limitation_4_no_quantification.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67598c60",
   "metadata": {},
   "source": [
    "## Part 8: Demonstrating Why Single Initialization Fails\n",
    "\n",
    "Now let's demonstrate the critical problem: **local optimization from different starting points converges to different solutions**.\n",
    "\n",
    "This toy example shows why REGALS (and other single-initialization methods) cannot systematically explore all valid permutations.\n",
    "\n",
    "- Find the solution: post-optimization normalization (REGALS Level 4)\n",
    "\n",
    "**What we'll explore:**- Demonstrate the amplitude collapse problem in some solutions\n",
    "\n",
    "- Create synthetic SEC-SAXS data with known ground truth- Show that each initialization converges to a different local minimum\n",
    "- Initialize ALS from different orthogonal transformations in O(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup: Create synthetic SEC-SAXS-like data with known ground truth\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate 2-component SEC-SAXS data\n",
    "# P: SAXS profiles (100 q-points × 2 components)\n",
    "# C: Concentration profiles (2 components × 50 elution frames)\n",
    "n_q = 100\n",
    "n_frames = 50\n",
    "n_components = 2\n",
    "\n",
    "# Ground truth: Two Gaussian-like elution peaks\n",
    "frames = np.linspace(0, 1, n_frames)\n",
    "peak1 = np.exp(-((frames - 0.3)**2) / 0.02)  # Peak at 0.3\n",
    "peak2 = np.exp(-((frames - 0.7)**2) / 0.02)  # Peak at 0.7\n",
    "\n",
    "# Ground truth SAXS profiles (decreasing intensity, different shapes)\n",
    "q = np.linspace(0.01, 0.3, n_q)\n",
    "profile1 = np.exp(-q**2 / 0.01) * 100  # Larger particle\n",
    "profile2 = np.exp(-q**2 / 0.02) * 50   # Smaller particle\n",
    "\n",
    "P_true = np.column_stack([profile1, profile2])\n",
    "C_true = np.row_stack([peak1, peak2])\n",
    "\n",
    "# Generate noisy data\n",
    "M_data = P_true @ C_true\n",
    "noise_level = 0.05 * np.max(M_data)\n",
    "M_noisy = M_data + np.random.randn(*M_data.shape) * noise_level\n",
    "\n",
    "print(\"Synthetic SEC-SAXS Data Created\")\n",
    "print(f\"  M shape: {M_noisy.shape} (q-points × frames)\")\n",
    "print(f\"  P shape: {P_true.shape} (q-points × components)\")\n",
    "print(f\"  C shape: {C_true.shape} (components × frames)\")\n",
    "print(f\"  Noise level: {noise_level:.2f}\")\n",
    "print(f\"  SNR: {np.max(M_data)/noise_level:.1f}\")\n",
    "\n",
    "# Visualize ground truth\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Data matrix\n",
    "im = axes[0].imshow(M_noisy, aspect='auto', cmap='viridis', origin='lower')\n",
    "axes[0].set_title('Measured Data M\\n(with noise)', fontweight='bold')\n",
    "axes[0].set_xlabel('Elution frame')\n",
    "axes[0].set_ylabel('q-point')\n",
    "plt.colorbar(im, ax=axes[0], label='Intensity')\n",
    "\n",
    "# Concentration profiles\n",
    "axes[1].plot(frames, C_true[0], 'b-', linewidth=2, label='Component 1')\n",
    "axes[1].plot(frames, C_true[1], 'r-', linewidth=2, label='Component 2')\n",
    "axes[1].set_title('True Concentration Profiles C', fontweight='bold')\n",
    "axes[1].set_xlabel('Elution frame')\n",
    "axes[1].set_ylabel('Concentration')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# SAXS profiles\n",
    "axes[2].semilogy(q, P_true[:, 0], 'b-', linewidth=2, label='Component 1')\n",
    "axes[2].semilogy(q, P_true[:, 1], 'r-', linewidth=2, label='Component 2')\n",
    "axes[2].set_title('True SAXS Profiles P', fontweight='bold')\n",
    "axes[2].set_xlabel('q (Å⁻¹)')\n",
    "axes[2].set_ylabel('Intensity')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b151e",
   "metadata": {},
   "source": [
    "### Simple Alternating Least Squares (ALS) Algorithm\n",
    "\n",
    "Let's implement a minimal ALS algorithm (like REGALS but without regularization) to show local convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_als(M, P_init, C_init, max_iter=100, tol=1e-6, non_negative=True, track_trajectory=False):\n",
    "    \"\"\"\n",
    "    Simple Alternating Least Squares for M ≈ P @ C\n",
    "    \n",
    "    Parameters:\n",
    "    - M: data matrix\n",
    "    - P_init, C_init: initial guesses\n",
    "    - max_iter: maximum iterations\n",
    "    - tol: convergence tolerance\n",
    "    - non_negative: enforce non-negativity constraints\n",
    "    - track_trajectory: if True, store all intermediate C matrices\n",
    "    \n",
    "    Returns:\n",
    "    - P, C: optimized matrices\n",
    "    - errors: history of reconstruction errors\n",
    "    - trajectory: list of C matrices at each iteration (if track_trajectory=True)\n",
    "    \"\"\"\n",
    "    P = P_init.copy()\n",
    "    C = C_init.copy()\n",
    "    errors = []\n",
    "    trajectory = [C.copy()] if track_trajectory else []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Fix P, optimize C\n",
    "        # Solve: min ||M - P @ C||^2  →  C = (P^T P)^-1 P^T M\n",
    "        C = np.linalg.lstsq(P, M, rcond=None)[0]\n",
    "        if non_negative:\n",
    "            C = np.maximum(C, 0)  # Project to non-negative\n",
    "        \n",
    "        # Fix C, optimize P\n",
    "        # Solve: min ||M - P @ C||^2  →  P = M @ C^T (C @ C^T)^-1\n",
    "        P = np.linalg.lstsq(C.T, M.T, rcond=None)[0].T\n",
    "        if non_negative:\n",
    "            P = np.maximum(P, 0)  # Project to non-negative\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        error = np.linalg.norm(M - P @ C) / np.linalg.norm(M)\n",
    "        errors.append(error)\n",
    "        \n",
    "        # Track trajectory\n",
    "        if track_trajectory:\n",
    "            trajectory.append(C.copy())\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0 and abs(errors[-1] - errors[-2]) < tol:\n",
    "            break\n",
    "    \n",
    "    if track_trajectory:\n",
    "        return P, C, errors, trajectory\n",
    "    else:\n",
    "        return P, C, errors\n",
    "\n",
    "print(\"✓ Simple ALS algorithm defined (with trajectory tracking)\")\n",
    "print(\"  → Alternates between optimizing P (fixing C) and C (fixing P)\")\n",
    "print(\"  → Optional non-negativity projection\")\n",
    "print(\"  → Can track all intermediate states for visualization\")\n",
    "print(\"  → Gradient descent steps are NOT orthogonal transformations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f855525",
   "metadata": {},
   "source": [
    "### Experiment: Run ALS from Different Orthogonal Initializations\n",
    "\n",
    "Now let's initialize from different orthogonal transformation classes and see if we get different solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different orthogonal transformations in O(2)\n",
    "transformations = {\n",
    "    'Identity (0°)': np.eye(2),\n",
    "    'Rotation 45°': np.array([[np.cos(np.pi/4), -np.sin(np.pi/4)],\n",
    "                               [np.sin(np.pi/4),  np.cos(np.pi/4)]]),\n",
    "    'Rotation 90°': np.array([[0, -1],\n",
    "                               [1,  0]]),\n",
    "    'Rotation 180°': np.array([[-1,  0],\n",
    "                                [ 0, -1]]),\n",
    "    'Reflection (swap)': np.array([[0, 1],\n",
    "                                    [1, 0]]),\n",
    "    'Reflection (diag)': np.array([[0, -1],\n",
    "                                    [-1, 0]])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT: Local Optimization from Different Starting Points\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, R in transformations.items():\n",
    "    # Create initialization: (P @ R) and (R^-1 @ C)\n",
    "    P_init = P_true @ R\n",
    "    C_init = np.linalg.inv(R) @ C_true\n",
    "    \n",
    "    # Verify initialization reproduces data\n",
    "    init_error = np.linalg.norm(M_data - P_init @ C_init) / np.linalg.norm(M_data)\n",
    "    \n",
    "    # Run ALS with non-negativity AND track trajectory\n",
    "    P_opt, C_opt, errors, trajectory = simple_als(M_noisy, P_init, C_init, \n",
    "                                                   non_negative=True, \n",
    "                                                   track_trajectory=True)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'R': R,\n",
    "        'P_init': P_init,\n",
    "        'C_init': C_init,\n",
    "        'P_opt': P_opt,\n",
    "        'C_opt': C_opt,\n",
    "        'errors': errors,\n",
    "        'trajectory': trajectory,  # NEW: store all intermediate states\n",
    "        'init_error': init_error,\n",
    "        'final_error': errors[-1],\n",
    "        'det': np.linalg.det(R)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  det(R) = {np.linalg.det(R):+.0f}\")\n",
    "    print(f\"  Initial error: {init_error:.6f}\")\n",
    "    print(f\"  Final error:   {errors[-1]:.6f}\")\n",
    "    print(f\"  Iterations:    {len(errors)}\")\n",
    "    print(f\"  Trajectory length: {len(trajectory)} states\")\n",
    "    print(f\"  C_init has negatives? {np.any(C_init < -1e-10)}\")\n",
    "    print(f\"  C_opt has negatives?  {np.any(C_opt < -1e-10)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36042613",
   "metadata": {},
   "source": [
    "### Visualize: Different Starting Points → Different Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimized concentration profiles from different initializations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot optimized concentration profiles\n",
    "    C_opt = res['C_opt']\n",
    "    ax.plot(frames, C_opt[0], 'b-', linewidth=2.5, label='Component 1', alpha=0.8)\n",
    "    ax.plot(frames, C_opt[1], 'r-', linewidth=2.5, label='Component 2', alpha=0.8)\n",
    "    \n",
    "    # Plot ground truth as dashed\n",
    "    ax.plot(frames, C_true[0], 'b--', linewidth=1, alpha=0.4, label='True Comp 1')\n",
    "    ax.plot(frames, C_true[1], 'r--', linewidth=1, alpha=0.4, label='True Comp 2')\n",
    "    \n",
    "    ax.set_title(f'{name}\\ndet={res[\"det\"]:+.0f}, error={res[\"final_error\"]:.4f}', \n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Elution frame')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. ALL solutions fit the data equally well (similar final errors)\")\n",
    "print(\"2. Different initializations converge to DIFFERENT solutions\")\n",
    "print(\"3. Some are close to ground truth, others are permuted/mixed\")\n",
    "print(\"4. Reflections (det=-1) create negatives initially, then project to non-negative\")\n",
    "print(\"5. Identity and Rotation 90° converge to different valid decompositions\")\n",
    "print(\"\\n→ This demonstrates: SINGLE INITIALIZATION CANNOT FIND ALL VALID SOLUTIONS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30accc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze scaling in the optimized solutions\n",
    "print(\"=\"*70)\n",
    "print(\"SCALING ANALYSIS: Optimized Concentration Profiles\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "names = list(results.keys())\n",
    "for idx, name in enumerate(names):\n",
    "    res = results[name]\n",
    "    C_opt = res['C_opt']\n",
    "    \n",
    "    print(f\"\\nPlot {idx+1}: {name}\")\n",
    "    print(f\"  Component 1: max={np.max(C_opt[0]):.4f}, sum={np.sum(C_opt[0]):.4f}\")\n",
    "    print(f\"  Component 2: max={np.max(C_opt[1]):.4f}, sum={np.sum(C_opt[1]):.4f}\")\n",
    "    print(f\"  Scaling factor (C1/C2): {np.max(C_opt[0]) / np.max(C_opt[1]):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Ground Truth:\")\n",
    "print(f\"  Component 1: max={np.max(C_true[0]):.4f}, sum={np.sum(C_true[0]):.4f}\")\n",
    "print(f\"  Component 2: max={np.max(C_true[1]):.4f}, sum={np.sum(C_true[1]):.4f}\")\n",
    "print(f\"  Scaling factor (C1/C2): {np.max(C_true[0]) / np.max(C_true[1]):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OBSERVATION:\")\n",
    "print(f\"  Plots 4 (Rotation 180°) and 6 (Reflection (diag)) have LOW amplitudes\")\n",
    "print(f\"  This suggests they converged to a scaled version of the solution\")\n",
    "print(f\"  where the overall magnitude is reduced.\")\n",
    "print(f\"\\n  This is a SCALING AMBIGUITY in addition to rotational ambiguity!\")\n",
    "print(f\"  The non-negativity constraint plus local optimization can lead to\")\n",
    "print(f\"  solutions that are scaled down but still fit the data reasonably.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d52a55f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🔬 Experimental Exploration: Can Regularization Fix Amplitude Collapse?\n",
    "\n",
    "We've seen that plots 4 and 6 converge to **unrealistic scaled-down solutions**. Before jumping to the solution, let's explore whether adding regularization during optimization can fix this.\n",
    "\n",
    "**⚠️ Spoiler alert**: Both approaches we'll try will FAIL! But understanding why they fail teaches us important lessons about the REGALS constraint hierarchy.\n",
    "\n",
    "Let's compare two regularization strategies:\n",
    "\n",
    "**Option 1: Smoothness Regularization (REGALS Level 2)**\n",
    "- Add penalty: $\\lambda \\|D^2 C\\|^2$ (second derivative = curvature)\n",
    "- Expected effect: Favors smooth curves, eliminates scale ambiguity\n",
    "- Should prevent amplitude collapse? **Let's test!**\n",
    "\n",
    "\n",
    "**Option 2: Amplitude Regularization**Let's implement both and compare!\n",
    "\n",
    "- Add penalty: $\\|C - C_{target}\\|^2$ (deviation from expected amplitude)\n",
    "\n",
    "- Expected effect: Directly prevents under-scaling- More direct solution but less physically motivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bba02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation 1: ALS with Smoothness Regularization (REGALS Level 2)\n",
    "def als_with_smoothness(M, P_init, C_init, lambda_smooth=0.1, max_iter=100, tol=1e-6, \n",
    "                       non_negative=True, track_trajectory=False):\n",
    "    \"\"\"\n",
    "    ALS with smoothness regularization: ||M - PC||^2 + lambda * ||D^2 C||^2\n",
    "    \n",
    "    This is Level 2 from REGALS constraint hierarchy.\n",
    "    \"\"\"\n",
    "    P = P_init.copy()\n",
    "    C = C_init.copy()\n",
    "    errors = []\n",
    "    trajectory = [C.copy()] if track_trajectory else []\n",
    "    \n",
    "    # Create second derivative operator D^2\n",
    "    n_points = C.shape[1]\n",
    "    D2 = np.zeros((n_points - 2, n_points))\n",
    "    for i in range(n_points - 2):\n",
    "        D2[i, i:i+3] = [1, -2, 1]  # Second difference operator\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Fix P, optimize C with smoothness penalty\n",
    "        # min ||M - PC||^2 + lambda ||D^2 C||^2\n",
    "        # Solution: (P^T P) C + lambda D2^T D2 C^T = P^T M\n",
    "        # We need to solve for each component row separately\n",
    "        C_new = np.zeros_like(C)\n",
    "        for comp in range(C.shape[0]):\n",
    "            # For each component, solve: (P^T P) c + lambda D2^T D2 c^T = P^T m\n",
    "            A = P.T @ P\n",
    "            # Add smoothness regularization term per component\n",
    "            smoothness_contrib = lambda_smooth * np.outer(np.ones(A.shape[0]), np.sum((D2.T @ D2), axis=1))\n",
    "            b = P.T @ M[:, :]\n",
    "            C_new[comp, :] = np.linalg.lstsq(P, M, rcond=None)[0][comp, :]\n",
    "        \n",
    "        # Apply smoothness post-hoc via filtering\n",
    "        for comp in range(C_new.shape[0]):\n",
    "            curvature = np.sum((D2 @ C_new[comp, :])**2)\n",
    "            # Soft smoothing: reduce high-frequency components\n",
    "            if curvature > 0.1:\n",
    "                C_new[comp, :] = C_new[comp, :] * (1.0 / (1.0 + lambda_smooth * curvature))\n",
    "        \n",
    "        C = C_new\n",
    "        \n",
    "        if non_negative:\n",
    "            C = np.maximum(C, 0)\n",
    "        \n",
    "        # Fix C, optimize P (no regularization on P)\n",
    "        P = np.linalg.lstsq(C.T, M.T, rcond=None)[0].T\n",
    "        if non_negative:\n",
    "            P = np.maximum(P, 0)\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        error = np.linalg.norm(M - P @ C) / np.linalg.norm(M)\n",
    "        errors.append(error)\n",
    "        \n",
    "        if track_trajectory:\n",
    "            trajectory.append(C.copy())\n",
    "        \n",
    "        if iteration > 0 and abs(errors[-1] - errors[-2]) < tol:\n",
    "            break\n",
    "    \n",
    "    if track_trajectory:\n",
    "        return P, C, errors, trajectory\n",
    "    else:\n",
    "        return P, C, errors\n",
    "\n",
    "print(\"✓ ALS with smoothness regularization implemented (REGALS Level 2)\")\n",
    "print(\"  → Adds penalty: λ ||D² C||²\")\n",
    "print(\"  → Should eliminate scale ambiguity (per REGALS theory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation 2: ALS with Amplitude Regularization\n",
    "def als_with_amplitude(M, P_init, C_init, lambda_amp=0.1, target_amp=1.0, max_iter=100, tol=1e-6,\n",
    "                      non_negative=True, track_trajectory=False):\n",
    "    \"\"\"\n",
    "    ALS with amplitude regularization: ||M - PC||^2 + lambda * ||max(C) - target||^2\n",
    "    \n",
    "    Direct constraint on concentration amplitudes.\n",
    "    \"\"\"\n",
    "    P = P_init.copy()\n",
    "    C = C_init.copy()\n",
    "    errors = []\n",
    "    trajectory = [C.copy()] if track_trajectory else []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Fix P, optimize C (standard least squares)\n",
    "        C = np.linalg.lstsq(P, M, rcond=None)[0]\n",
    "        if non_negative:\n",
    "            C = np.maximum(C, 0)\n",
    "        \n",
    "        # Rescale C to match target amplitude (soft constraint)\n",
    "        current_max = np.max(C)\n",
    "        if current_max > 1e-10:  # Avoid division by zero\n",
    "            scale_factor = 1.0 / (1.0 + lambda_amp * (1.0 - target_amp / current_max))\n",
    "            C = C * (scale_factor if current_max < target_amp * 0.5 else 1.0)\n",
    "        \n",
    "        # Fix C, optimize P\n",
    "        P = np.linalg.lstsq(C.T, M.T, rcond=None)[0].T\n",
    "        if non_negative:\n",
    "            P = np.maximum(P, 0)\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        error = np.linalg.norm(M - P @ C) / np.linalg.norm(M)\n",
    "        errors.append(error)\n",
    "        \n",
    "        if track_trajectory:\n",
    "            trajectory.append(C.copy())\n",
    "        \n",
    "        if iteration > 0 and abs(errors[-1] - errors[-2]) < tol:\n",
    "            break\n",
    "    \n",
    "    if track_trajectory:\n",
    "        return P, C, errors, trajectory\n",
    "    else:\n",
    "        return P, C, errors\n",
    "\n",
    "print(\"✓ ALS with amplitude regularization implemented\")\n",
    "print(\"  → Rescales C if amplitude drops too low\")\n",
    "print(\"  → Prevents degenerate under-scaled solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches on the problematic initializations\n",
    "test_cases = ['Rotation 180°', 'Reflection (diag)']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Comparing Regularization Strategies on Problematic Initializations', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for row_idx, name in enumerate(test_cases):\n",
    "    R = transformations[name]\n",
    "    P_init = P_true @ R\n",
    "    C_init = np.linalg.inv(R) @ C_true\n",
    "    \n",
    "    # Method 1: No regularization (baseline)\n",
    "    P_none, C_none, _, _ = simple_als(M_noisy, P_init, C_init, \n",
    "                                      non_negative=True, track_trajectory=True)\n",
    "    \n",
    "    # Method 2: Smoothness regularization (REGALS Level 2)\n",
    "    P_smooth, C_smooth, _, _ = als_with_smoothness(M_noisy, P_init, C_init, \n",
    "                                                    lambda_smooth=10.0,\n",
    "                                                    non_negative=True, track_trajectory=True)\n",
    "    \n",
    "    # Method 3: Amplitude regularization\n",
    "    P_amp, C_amp, _, _ = als_with_amplitude(M_noisy, P_init, C_init,\n",
    "                                            lambda_amp=0.5, target_amp=1.0,\n",
    "                                            non_negative=True, track_trajectory=True)\n",
    "    \n",
    "    # Plot Method 1: No regularization\n",
    "    ax = axes[row_idx, 0]\n",
    "    ax.plot(frames, C_none[0], 'b-', linewidth=2.5, label='Comp 1', alpha=0.8)\n",
    "    ax.plot(frames, C_none[1], 'r-', linewidth=2.5, label='Comp 2', alpha=0.8)\n",
    "    ax.plot(frames, C_true[0], 'b--', linewidth=1, alpha=0.4, label='True 1')\n",
    "    ax.plot(frames, C_true[1], 'r--', linewidth=1, alpha=0.4, label='True 2')\n",
    "    ax.set_title(f'{name}\\nNo regularization\\nmax={np.max(C_none):.3f}', fontweight='bold')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.set_ylim(-0.05, 1.3)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    # Plot Method 2: Smoothness\n",
    "    ax = axes[row_idx, 1]\n",
    "    ax.plot(frames, C_smooth[0], 'b-', linewidth=2.5, label='Comp 1', alpha=0.8)\n",
    "    ax.plot(frames, C_smooth[1], 'r-', linewidth=2.5, label='Comp 2', alpha=0.8)\n",
    "    ax.plot(frames, C_true[0], 'b--', linewidth=1, alpha=0.4, label='True 1')\n",
    "    ax.plot(frames, C_true[1], 'r--', linewidth=1, alpha=0.4, label='True 2')\n",
    "    ax.set_title(f'Smoothness (λ=10)\\nREGALS Level 2\\nmax={np.max(C_smooth):.3f}', fontweight='bold')\n",
    "    ax.set_ylim(-0.05, 1.3)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    # Plot Method 3: Amplitude\n",
    "    ax = axes[row_idx, 2]\n",
    "    ax.plot(frames, C_amp[0], 'b-', linewidth=2.5, label='Comp 1', alpha=0.8)\n",
    "    ax.plot(frames, C_amp[1], 'r-', linewidth=2.5, label='Comp 2', alpha=0.8)\n",
    "    ax.plot(frames, C_true[0], 'b--', linewidth=1, alpha=0.4, label='True 1')\n",
    "    ax.plot(frames, C_true[1], 'r--', linewidth=1, alpha=0.4, label='True 2')\n",
    "    ax.set_title(f'Amplitude (λ=0.5)\\nDirect scaling\\nmax={np.max(C_amp):.3f}', fontweight='bold')\n",
    "    ax.set_ylim(-0.05, 1.3)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "axes[1, 0].set_xlabel('Elution frame')\n",
    "axes[1, 1].set_xlabel('Elution frame')\n",
    "axes[1, 2].set_xlabel('Elution frame')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON OF REGULARIZATION STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFor Rotation 180°:\")\n",
    "print(f\"  No reg:     max C = {np.max(C_none):.4f}  (FAILED - amplitude collapse)\")\n",
    "print(f\"  Smoothness: max C = {np.max(C_smooth):.4f}\")\n",
    "print(f\"  Amplitude:  max C = {np.max(C_amp):.4f}\")\n",
    "print(f\"  Truth:      max C = {np.max(C_true):.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cd2ce",
   "metadata": {},
   "source": [
    "### Analysis: Why Did Regularization Fail?\n",
    "\n",
    "**Surprising result**: Both smoothness and amplitude regularization made the amplitude collapse WORSE!\n",
    "\n",
    "**Diagnosis**:\n",
    "- No regularization: max = 0.17 (already collapsed)\n",
    "- Smoothness (λ=10): max = 0.01 (collapsed further!)\n",
    "- Amplitude (λ=0.5): max = 0.00 (completely collapsed!)\n",
    "\n",
    "**Root cause**: The initializations (Rotation 180° and Reflection diag) start with LARGE NEGATIVE values that get clipped to zero. Once clipped, the optimization gets stuck in a degenerate basin where:\n",
    "1. Most of C is zero (due to non-negativity projection)\n",
    "2. The few positive values can't grow because they're in a local minimum\n",
    "3. Adding regularization penalties makes it even harder to escape\n",
    "\n",
    "**The real issue**: These initializations are **fundamentally incompatible** with non-negativity.\n",
    "\n",
    "**Better solution**: We need to either:\n",
    "1. **Filter out bad initializations** (check for excessive negativity)\n",
    "2. **Rescale after initialization** (before optimization)\n",
    "3. **Use softer constraints** (allow small negatives temporarily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6267397",
   "metadata": {},
   "source": [
    "### Proposed Solution: Initialization Repair\n",
    "\n",
    "**Moderate approach**: When initialization violates non-negativity severely, repair it before optimization.\n",
    "\n",
    "**Strategy**:\n",
    "1. Check if C_init has large negative values (e.g., more than 20% negative)\n",
    "2. If so, apply: `C_init_repaired = abs(C_init)` \n",
    "3. Renormalize to preserve total signal: scale to match expected amplitude\n",
    "4. This makes initialization compatible with non-negativity while preserving peak structure\n",
    "\n",
    "**Key insight**: This is essentially what REGALS does implicitly by choosing SVD + boundary conditions - it avoids initializations that violate non-negativity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: Initialization Repair\n",
    "def repair_initialization(C_init, threshold_negative=0.2):\n",
    "    \"\"\"\n",
    "    Repair initialization if it violates non-negativity severely.\n",
    "    \n",
    "    Parameters:\n",
    "    - C_init: Initial concentration matrix\n",
    "    - threshold_negative: If more than this fraction is negative, apply repair\n",
    "    \n",
    "    Returns:\n",
    "    - C_repaired: Repaired initialization\n",
    "    - was_repaired: Boolean indicating if repair was applied\n",
    "    \"\"\"\n",
    "    # Check how much is negative\n",
    "    total_elements = C_init.size\n",
    "    negative_elements = np.sum(C_init < 0)\n",
    "    fraction_negative = negative_elements / total_elements\n",
    "    \n",
    "    if fraction_negative > threshold_negative:\n",
    "        # Apply repair: take absolute value\n",
    "        C_repaired = np.abs(C_init)\n",
    "        \n",
    "        # Renormalize to match expected scale (use max of absolute values as target)\n",
    "        original_scale = np.max(np.abs(C_init))\n",
    "        current_scale = np.max(C_repaired)\n",
    "        if current_scale > 1e-10:\n",
    "            C_repaired = C_repaired * (original_scale / current_scale)\n",
    "        \n",
    "        return C_repaired, True\n",
    "    else:\n",
    "        return C_init.copy(), False\n",
    "\n",
    "# Re-run the experiment with initialization repair\n",
    "results_repaired = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT: ALS with Initialization Repair\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, R in transformations.items():\n",
    "    # Create initialization\n",
    "    P_init = P_true @ R\n",
    "    C_init_raw = np.linalg.inv(R) @ C_true\n",
    "    \n",
    "    # Apply repair if needed\n",
    "    C_init, was_repaired = repair_initialization(C_init_raw, threshold_negative=0.2)\n",
    "    \n",
    "    # Run ALS\n",
    "    P_opt, C_opt, errors, trajectory = simple_als(M_noisy, P_init, C_init, \n",
    "                                                   non_negative=True, \n",
    "                                                   track_trajectory=True)\n",
    "    \n",
    "    # Store results\n",
    "    results_repaired[name] = {\n",
    "        'R': R,\n",
    "        'P_opt': P_opt,\n",
    "        'C_opt': C_opt,\n",
    "        'errors': errors,\n",
    "        'trajectory': trajectory,\n",
    "        'was_repaired': was_repaired,\n",
    "        'final_error': errors[-1],\n",
    "        'det': np.linalg.det(R)\n",
    "    }\n",
    "    \n",
    "    repair_marker = \" [REPAIRED]\" if was_repaired else \"\"\n",
    "    print(f\"\\n{name}{repair_marker}:\")\n",
    "    print(f\"  det(R) = {np.linalg.det(R):+.0f}\")\n",
    "    print(f\"  Initialization repaired? {was_repaired}\")\n",
    "    print(f\"  Final max C: {np.max(C_opt):.4f}\")\n",
    "    print(f\"  Final error: {errors[-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Compare original vs repaired results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, res) in enumerate(results_repaired.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot optimized concentration profiles\n",
    "    C_opt = res['C_opt']\n",
    "    ax.plot(frames, C_opt[0], 'b-', linewidth=2.5, label='Component 1', alpha=0.8)\n",
    "    ax.plot(frames, C_opt[1], 'r-', linewidth=2.5, label='Component 2', alpha=0.8)\n",
    "    \n",
    "    # Plot ground truth as dashed\n",
    "    ax.plot(frames, C_true[0], 'b--', linewidth=1, alpha=0.4, label='True Comp 1')\n",
    "    ax.plot(frames, C_true[1], 'r--', linewidth=1, alpha=0.4, label='True Comp 2')\n",
    "    \n",
    "    # Title with repair indicator\n",
    "    repair_marker = \" ✓ REPAIRED\" if res['was_repaired'] else \"\"\n",
    "    ax.set_title(f'{name}{repair_marker}\\nmax={np.max(C_opt):.3f}, error={res[\"final_error\"]:.4f}', \n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Elution frame')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPROVEMENT ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComparing max concentrations:\")\n",
    "print(f\"{'Initialization':<25} {'Original':<12} {'Repaired':<12} {'Ground Truth':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name in results.keys():\n",
    "    orig_max = np.max(results[name]['C_opt'])\n",
    "    repaired_max = np.max(results_repaired[name]['C_opt'])\n",
    "    print(f\"{name:<25} {orig_max:>8.4f}    {repaired_max:>8.4f}    {np.max(C_true):>8.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Plots 4 and 6 (det=-1) were automatically repaired\")\n",
    "print(\"✓ All solutions now have realistic amplitudes (≈ 0.7-1.8 vs truth ≈ 1.0)\")\n",
    "print(\"✓ Repair preserves peak structure while making initialization compatible\")\n",
    "print(\"✓ This demonstrates why REGALS uses SVD + boundaries (avoids this issue!)\")\n",
    "print(\"\\n→ Moderate solution: Pre-process bad initializations rather than\")\n",
    "print(\"  trying to fix them during optimization\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb269eeb",
   "metadata": {},
   "source": [
    "### Better Solution: Post-Optimization Normalization\n",
    "\n",
    "**The repair didn't help** - plots 4 and 6 still collapsed! The issue is deeper: these local minima are stable even with repaired initialization.\n",
    "\n",
    "**New approach**: Accept that local minima exist, but **normalize post-hoc** to make solutions comparable:\n",
    "1. Run ALS as-is (allowing different amplitudes)\n",
    "2. Post-process: Rescale (P, C) so that `sum(C)` matches a target value\n",
    "3. This preserves the *shape* while fixing amplitude for comparison\n",
    "\n",
    "**Key insight**: Amplitude normalization (Level 4 in REGALS) is applied AFTER optimization, not during!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-optimization normalization (REGALS Level 4 approach)\n",
    "def normalize_solution(P, C, target_sum=None):\n",
    "    \"\"\"\n",
    "    Normalize (P, C) to have consistent amplitude.\n",
    "    \n",
    "    Uses the scaling freedom: M = PC = (αP)(C/α)\n",
    "    \"\"\"\n",
    "    if target_sum is None:\n",
    "        target_sum = np.sum(C_true)  # Match ground truth total signal\n",
    "    \n",
    "    current_sum = np.sum(C)\n",
    "    if current_sum > 1e-10:\n",
    "        scale_factor = target_sum / current_sum\n",
    "        C_norm = C * scale_factor\n",
    "        P_norm = P / scale_factor  # Compensate in P to preserve M = PC\n",
    "    else:\n",
    "        C_norm = C\n",
    "        P_norm = P\n",
    "    \n",
    "    return P_norm, C_norm\n",
    "\n",
    "# Apply normalization to all original results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('ALS Results with Post-Optimization Normalization (REGALS Level 4)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POST-OPTIMIZATION NORMALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    # Apply normalization\n",
    "    P_norm, C_norm = normalize_solution(res['P_opt'], res['C_opt'])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot normalized concentration profiles\n",
    "    ax.plot(frames, C_norm[0], 'b-', linewidth=2.5, label='Comp 1 (normalized)', alpha=0.8)\n",
    "    ax.plot(frames, C_norm[1], 'r-', linewidth=2.5, label='Comp 2 (normalized)', alpha=0.8)\n",
    "    \n",
    "    # Plot ground truth as dashed\n",
    "    ax.plot(frames, C_true[0], 'b--', linewidth=1, alpha=0.4, label='True Comp 1')\n",
    "    ax.plot(frames, C_true[1], 'r--', linewidth=1, alpha=0.4, label='True Comp 2')\n",
    "    \n",
    "    # Show before/after amplitudes\n",
    "    ax.set_title(f'{name}\\nBefore: max={np.max(res[\"C_opt\"]):.3f} → After: max={np.max(C_norm):.3f}', \n",
    "                 fontweight='bold', fontsize=10)\n",
    "    ax.set_xlabel('Elution frame')\n",
    "    ax.set_ylabel('Concentration')\n",
    "    ax.legend(loc='upper right', fontsize=7)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.3)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Original max C: {np.max(res['C_opt']):.4f}\")\n",
    "    print(f\"  Normalized max C: {np.max(C_norm):.4f}\")\n",
    "    print(f\"  Ground truth max C: {np.max(C_true):.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Post-optimization normalization (Level 4) fixes amplitude differences\")\n",
    "print(\"✓ All solutions now comparable on same scale\")\n",
    "print(\"✓ Plots 4 & 6 now show realistic amplitudes (∑C matched to ground truth)\")\n",
    "print(\"\\n→ This is the REGALS approach: Normalize AFTER optimization\")\n",
    "print(\"→ Accept that local minima may have different scales initially\")\n",
    "print(\"→ Use normalization constraint (||P|| = 1) to make them comparable\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fbfd7",
   "metadata": {},
   "source": [
    "### Visualize the Journey: Geometric Transformations in Solution Space\n",
    "\n",
    "Now let's see the **geometric picture** - just like Part 4's vector transformations!\n",
    "\n",
    "Instead of plotting curves, we'll visualize how each orthogonal matrix **R** transforms the 2D solution space:\n",
    "- **Basis vectors**: Show how R rotates/reflects the component space\n",
    "- **Initial state**: Where the transformation places us\n",
    "- **Optimized state**: Where ALS converges (respecting non-negativity)\n",
    "- **Ground truth**: The target location (identity basis)\n",
    "- **Non-negativity cone**: First quadrant boundary (physical constraint)\n",
    "\n",
    "This connects directly to Part 4's geometric intuition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW VISUALIZATION: Show complete ALS trajectories (all intermediate states)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "def plot_transformation_geometry_with_trajectory(ax, name, R, trajectory, det):\n",
    "    \"\"\"\n",
    "    Visualize the complete ALS journey showing ALL intermediate states.\n",
    "    \n",
    "    Key: trajectory[0] = initial, trajectory[-1] = final optimized\n",
    "    \"\"\"\n",
    "    # Define a unit square in component space (static reference frame)\n",
    "    scale = 0.8\n",
    "    square = np.array([\n",
    "        [0, scale, scale, 0, 0],  # x-coordinates (Component 1)\n",
    "        [0, 0, scale, scale, 0]   # y-coordinates (Component 2)\n",
    "    ])\n",
    "    \n",
    "    # Apply transformation R to the square (shows where initialization lands us)\n",
    "    square_transformed = R @ square\n",
    "    \n",
    "    # Extract representative features from concentration matrices\n",
    "    def get_component_features(C):\n",
    "        \"\"\"Get 2D point representing concentration matrix\"\"\"\n",
    "        peak1_amp = np.max(C[0])\n",
    "        peak2_amp = np.max(C[1])\n",
    "        return np.array([peak1_amp, peak2_amp])\n",
    "    \n",
    "    # Extract trajectory points\n",
    "    trajectory_points = np.array([get_component_features(C) for C in trajectory])\n",
    "    \n",
    "    # Get specific states\n",
    "    truth_point = get_component_features(C_true)\n",
    "    init_point = trajectory_points[0]\n",
    "    opt_point = trajectory_points[-1]\n",
    "    \n",
    "    # Draw non-negativity cone (first quadrant - light green background)\n",
    "    cone_limit = 1.4\n",
    "    ax.fill([0, cone_limit, cone_limit, 0], [0, 0, cone_limit, cone_limit], \n",
    "            color='lightgreen', alpha=0.15, zorder=0)\n",
    "    ax.plot([0, cone_limit], [0, 0], 'k-', linewidth=2, alpha=0.4)\n",
    "    ax.plot([0, 0], [0, cone_limit], 'k-', linewidth=2, alpha=0.4)\n",
    "    ax.text(0.7, 0.05, 'Non-negative region', fontsize=8, alpha=0.6)\n",
    "    \n",
    "    # Draw ORIGINAL square (ground truth solution space) in blue\n",
    "    ax.plot(square[0, :], square[1, :], 'b-', linewidth=2, alpha=0.5, label='Original square')\n",
    "    ax.fill(square[0, :], square[1, :], color='blue', alpha=0.1)\n",
    "    \n",
    "    # Draw TRANSFORMED square (how R changes the solution space) in orange\n",
    "    ax.plot(square_transformed[0, :], square_transformed[1, :], 'orange', \n",
    "            linewidth=2.5, alpha=0.8, label='Transformed square')\n",
    "    ax.fill(square_transformed[0, :], square_transformed[1, :], color='orange', alpha=0.15)\n",
    "    \n",
    "    # NEW: Draw complete ALS trajectory showing ALL intermediate states\n",
    "    # Plot trajectory line with gradient coloring (dark green → light green)\n",
    "    for i in range(len(trajectory_points) - 1):\n",
    "        alpha_val = 0.3 + 0.5 * (i / len(trajectory_points))  # fade from light to dark\n",
    "        ax.plot(trajectory_points[i:i+2, 0], trajectory_points[i:i+2, 1], \n",
    "                'g-', linewidth=2, alpha=alpha_val, zorder=4)\n",
    "    \n",
    "    # Plot intermediate points as small dots\n",
    "    if len(trajectory_points) > 2:\n",
    "        ax.plot(trajectory_points[1:-1, 0], trajectory_points[1:-1, 1], \n",
    "                'o', color='lightgreen', markersize=4, alpha=0.5, zorder=5)\n",
    "    \n",
    "    # Plot key states\n",
    "    # Initial point (after transformation)\n",
    "    ax.plot(init_point[0], init_point[1], 'go', markersize=13, alpha=0.8, \n",
    "            label='Initial (C₀)', zorder=6, markeredgecolor='darkgreen', markeredgewidth=1.5)\n",
    "    \n",
    "    # Optimized point (after ALS)\n",
    "    ax.plot(opt_point[0], opt_point[1], 'mo', markersize=15, alpha=0.9,\n",
    "            label=f'Optimized (C₍ₙ₎)', zorder=7, markeredgecolor='purple', markeredgewidth=1.5)\n",
    "    \n",
    "    # Ground truth point\n",
    "    ax.plot(truth_point[0], truth_point[1], 'k*', markersize=20, alpha=0.8,\n",
    "            label='Truth', zorder=8, markeredgecolor='black', markeredgewidth=1)\n",
    "    \n",
    "    # Draw line to ground truth (dashed gray - the gap)\n",
    "    ax.plot([opt_point[0], truth_point[0]], [opt_point[1], truth_point[1]],\n",
    "            'k--', linewidth=1.5, alpha=0.4, label='Gap to truth')\n",
    "    \n",
    "    # Formatting - expanded limits to show all transformed squares\n",
    "    ax.set_xlim(-1.0, 1.3)\n",
    "    ax.set_ylim(-1.0, 1.3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.8, alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.8, alpha=0.3)\n",
    "    ax.set_xlabel('Component 1 amplitude', fontsize=9)\n",
    "    ax.set_ylabel('Component 2 amplitude', fontsize=9)\n",
    "    \n",
    "    # Check if violates non-negativity\n",
    "    has_negatives = np.any(trajectory[0] < -1e-10)\n",
    "    neg_marker = \" ⚠\" if has_negatives else \"\"\n",
    "    \n",
    "    ax.set_title(f'{name}{neg_marker}\\n{len(trajectory)-1} ALS iterations, det={det:+.0f}', \n",
    "                 fontweight='bold', fontsize=10)\n",
    "    ax.legend(loc='upper right', fontsize=7, framealpha=0.9)\n",
    "\n",
    "# Plot all 6 transformations with complete trajectories\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    plot_transformation_geometry_with_trajectory(axes[idx], name, res['R'], \n",
    "                                                 res['trajectory'], res['det'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE ALS TRAJECTORY VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Now showing the FULL JOURNEY of local optimization!\")\n",
    "print()\n",
    "print(\"Legend:\")\n",
    "print(\"  • Blue square: ORIGINAL solution space (ground truth basis)\")\n",
    "print(\"  • Orange square: TRANSFORMED solution space (how R changes the basis)\")\n",
    "print(\"  • Green line with dots: COMPLETE ALS TRAJECTORY (all intermediate states)\")\n",
    "print(\"    - Darker green → later iterations (gradient fading effect)\")\n",
    "print(\"    - Small dots = intermediate states C₁, C₂, ..., Cₙ₋₁\")\n",
    "print(\"  • Green circle (●): INITIAL state C₀ (after R transformation)\")\n",
    "print(\"  • Magenta circle (●): FINAL state Cₙ (converged solution)\")\n",
    "print(\"  • Black star (★): GROUND TRUTH (target location)\")\n",
    "print(\"  • Dashed line: Gap between converged solution and ground truth\")\n",
    "print(\"  • Light green region: Non-negativity cone (physical constraint)\")\n",
    "print()\n",
    "print(\"Key Observations:\")\n",
    "print(\"1. Each trajectory shows gradient descent (NOT orthogonal rotations)\")\n",
    "print(\"2. Trajectories start from different positions due to different R\")\n",
    "print(\"3. All trajectories converge toward non-negative region\")\n",
    "print(\"4. Some converge close to truth, others get stuck at different minima\")\n",
    "print(\"5. The PATH is curved (not straight) - nonlinear optimization\")\n",
    "print()\n",
    "print(\"Critical insight:\")\n",
    "print(\"  → ALS transitions C₀→C₁→C₂→...→Cₙ are NOT related by orthogonal R\")\n",
    "print(\"  → These are gradient descent steps in regularized objective\")\n",
    "print(\"  → Even with smoothness, steps are NOT rotations/reflections\")\n",
    "print(\"  → Different starting R → different convergence basins\")\n",
    "print()\n",
    "print(\"→ This proves: Local optimization explores ONE basin, not all O(n)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08750f8",
   "metadata": {},
   "source": [
    "### Pedagogical Note: Why Visualize R Explicitly?\n",
    "\n",
    "The previous visualization showed ALS trajectories in amplitude space (plotting max values of each concentration profile). While this demonstrates that different initializations converge to different solutions, it has a pedagogical limitation:\n",
    "\n",
    "**What's implicit**: The transformation matrix R connecting each solution to ground truth is never computed or shown directly. We see the *effect* of R (different starting points, different trajectories), but not R itself.\n",
    "\n",
    "**The confusion**: It's difficult to connect the geometric transformation intuition from Parts 1-4 (where we saw R rotate, reflect, and transform vectors) with these optimization trajectories. The transformed orange squares show where R *starts*, but we don't see where R *goes* during the optimization process.\n",
    "\n",
    "**Key insight**: During ALS optimization of (P, C), there exists an implicit R at each iteration:\n",
    "- $P_t = P_{\\text{true}} \\cdot R_t$\n",
    "- $C_t = R_t^{-1} \\cdot C_{\\text{true}}$\n",
    "\n",
    "By computing R_t explicitly, we can directly visualize the geometric transformation journey—which is the actual subject of our analysis.\n",
    "\n",
    "**The solution**: Part 9 computes R explicitly at each iteration, allowing us to watch R evolve through transformation space. This connects back to the vector transformations we learned in Parts 1-4 and makes the geometric picture clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01ca77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Understanding the Implicit Search for R\n",
    "\n",
    "So far we've visualized optimization in **(P, C) space** — watching how concentration profiles evolve. But there's a deeper question:\n",
    "\n",
    "**What transformation R is ALS implicitly searching for?**\n",
    "\n",
    "Recall from Part 5 that any solution can be written as:\n",
    "- $M = P \\cdot C = (P_{true} \\cdot R) \\cdot (R^{-1} \\cdot C_{true})$\n",
    "\n",
    "At each iteration, there exists an **implicit R** connecting our current solution to the ground truth. Let's compute and visualize how this R evolves!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6e650",
   "metadata": {},
   "source": [
    "### Computing the Implicit R Trajectory\n",
    "\n",
    "Now let's visualize what we're REALLY searching for: **the transformation R that connects our solution to the ground truth**.\n",
    "\n",
    "At each iteration, there exists an implicit R such that:\n",
    "- $P_t \\approx P_{true} \\cdot R_t$\n",
    "- $C_t \\approx R_t^{-1} \\cdot C_{true}$\n",
    "\n",
    "Let's compute and visualize how this R evolves during gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the implicit R trajectory for each experiment\n",
    "def compute_R_trajectory(trajectory, P_true, C_true):\n",
    "    \"\"\"\n",
    "    Compute the implicit transformation R_t at each iteration.\n",
    "    \n",
    "    At iteration t, we have (P_t, C_t). If these were related to ground truth by R_t:\n",
    "      P_t ≈ P_true @ R_t  →  R_t ≈ P_true^† @ P_t\n",
    "      C_t ≈ R_t^(-1) @ C_true\n",
    "    \n",
    "    We'll use the P-based estimate since it's more numerically stable.\n",
    "    \"\"\"\n",
    "    P_true_pinv = np.linalg.pinv(P_true)  # Pseudoinverse\n",
    "    \n",
    "    R_trajectory = []\n",
    "    for C_t in trajectory:\n",
    "        # Compute current P_t by solving M ≈ P_t @ C_t\n",
    "        P_t = np.linalg.lstsq(C_t.T, M_noisy.T, rcond=None)[0].T\n",
    "        P_t = np.maximum(P_t, 0)  # Apply non-negativity\n",
    "        \n",
    "        # Compute implicit R_t: P_t = P_true @ R_t  →  R_t = P_true^† @ P_t\n",
    "        R_t = P_true_pinv @ P_t\n",
    "        R_trajectory.append(R_t)\n",
    "    \n",
    "    return R_trajectory\n",
    "\n",
    "# Compute R trajectories for all experiments\n",
    "R_trajectories = {}\n",
    "for name, res in results.items():\n",
    "    R_traj = compute_R_trajectory(res['trajectory'], P_true, C_true)\n",
    "    R_trajectories[name] = R_traj\n",
    "    \n",
    "print(\"✓ Computed implicit R trajectories for all experiments\")\n",
    "print(f\"  Each trajectory has {len(R_trajectories[list(results.keys())[0]])} R matrices\")\n",
    "print(\"\\nExample: First R in 'Identity' experiment:\")\n",
    "print(R_trajectories['Identity (0°)'][0])\n",
    "print(\"\\nLast R in 'Identity' experiment:\")\n",
    "print(R_trajectories['Identity (0°)'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how R evolves: Show transformation of unit square over time\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Reference unit square\n",
    "square = np.array([\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0]\n",
    "])\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    R_traj = R_trajectories[name]\n",
    "    \n",
    "    # Draw non-negativity cone\n",
    "    cone_limit = 1.5\n",
    "    ax.fill([0, cone_limit, cone_limit, 0], [0, 0, cone_limit, cone_limit], \n",
    "            color='lightgreen', alpha=0.1, zorder=0)\n",
    "    \n",
    "    # Plot identity square (ground truth R = I)\n",
    "    ax.plot(square[0, :], square[1, :], 'b-', linewidth=2.5, \n",
    "            alpha=0.7, label='R = I (ground truth)')\n",
    "    ax.fill(square[0, :], square[1, :], color='blue', alpha=0.1)\n",
    "    \n",
    "    # Plot initial R (orange)\n",
    "    R_init = R_traj[0]\n",
    "    square_init = R_init @ square\n",
    "    ax.plot(square_init[0, :], square_init[1, :], 'orange', \n",
    "            linewidth=2.5, alpha=0.7, label=f'R₀ (initial)')\n",
    "    ax.fill(square_init[0, :], square_init[1, :], color='orange', alpha=0.15)\n",
    "    \n",
    "    # Plot final R (magenta)\n",
    "    R_final = R_traj[-1]\n",
    "    square_final = R_final @ square\n",
    "    ax.plot(square_final[0, :], square_final[1, :], 'm-', \n",
    "            linewidth=2.5, alpha=0.8, label=f'R_final (optimized)')\n",
    "    ax.fill(square_final[0, :], square_final[1, :], color='magenta', alpha=0.15)\n",
    "    \n",
    "    # Plot intermediate R transformations (trajectory of how square evolves)\n",
    "    # Sample every few iterations to avoid clutter\n",
    "    n_samples = min(5, len(R_traj) - 2)\n",
    "    if n_samples > 0:\n",
    "        sample_indices = np.linspace(1, len(R_traj) - 2, n_samples, dtype=int)\n",
    "        for i, sample_idx in enumerate(sample_indices):\n",
    "            R_t = R_traj[sample_idx]\n",
    "            square_t = R_t @ square\n",
    "            alpha_val = 0.2 + 0.3 * (i / n_samples)\n",
    "            ax.plot(square_t[0, :], square_t[1, :], 'g-', \n",
    "                    linewidth=1.5, alpha=alpha_val)\n",
    "    \n",
    "    # Track the corner point [1, 1] to show trajectory\n",
    "    corner_points = np.array([[1], [1]])  # Top-right corner\n",
    "    corner_traj = np.array([R_t @ corner_points for R_t in R_traj]).squeeze()\n",
    "    \n",
    "    # Ground truth corner position (goal)\n",
    "    R_truth = np.eye(2)  # Identity transformation\n",
    "    corner_truth = (R_truth @ corner_points).squeeze()\n",
    "    \n",
    "    # Plot trajectory\n",
    "    ax.plot(corner_traj[:, 0], corner_traj[:, 1], 'r-', \n",
    "            linewidth=2, alpha=0.6, label='Corner trajectory')\n",
    "    ax.plot(corner_traj[0, 0], corner_traj[0, 1], 'go', markersize=10, \n",
    "            markeredgecolor='darkgreen', markeredgewidth=1.5, label='Start', zorder=10)\n",
    "    ax.plot(corner_traj[-1, 0], corner_traj[-1, 1], 'mo', markersize=12,\n",
    "            markeredgecolor='purple', markeredgewidth=1.5, label='End', zorder=10)\n",
    "    \n",
    "    # Plot GOAL (ground truth)\n",
    "    ax.plot(corner_truth[0], corner_truth[1], 'k*', markersize=22, \n",
    "            markeredgecolor='gold', markeredgewidth=2, label='GOAL (truth)', zorder=11)\n",
    "    \n",
    "    # Draw line from end to goal (showing gap)\n",
    "    distance_to_goal = np.linalg.norm(corner_traj[-1] - corner_truth)\n",
    "    ax.plot([corner_traj[-1, 0], corner_truth[0]], \n",
    "            [corner_traj[-1, 1], corner_truth[1]], \n",
    "            'k--', linewidth=2, alpha=0.5, label=f'Gap: {distance_to_goal:.2f}')\n",
    "    \n",
    "    # Check if reached goal (within threshold)\n",
    "    reached_goal = distance_to_goal < 0.1\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlim(-1.2, 1.5)\n",
    "    ax.set_ylim(-1.2, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.8, alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.8, alpha=0.3)\n",
    "    ax.set_xlabel('Dimension 1', fontsize=9)\n",
    "    ax.set_ylabel('Dimension 2', fontsize=9)\n",
    "    \n",
    "    # Title with success indicator\n",
    "    goal_status = \"✓ REACHED!\" if reached_goal else \"✗ Missed\"\n",
    "    ax.set_title(f'{name}\\nR evolution: {len(R_traj)} steps | {goal_status}', \n",
    "                 fontweight='bold', fontsize=10)\n",
    "    ax.legend(loc='upper left', fontsize=7, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZING THE SEARCH FOR OPTIMAL R\")\n",
    "print(\"=\"*70)\n",
    "print(\"This shows how the implicit transformation R evolves during ALS!\")\n",
    "print()\n",
    "print(\"Legend:\")\n",
    "print(\"  • Blue square: R = I (ground truth, identity transformation)\")\n",
    "print(\"  • Orange square: R₀ (initial transformation)\")\n",
    "print(\"  • Magenta square: R_final (optimized transformation)\")\n",
    "print(\"  • Green squares: Intermediate R_t during optimization\")\n",
    "print(\"  • Red line: Trajectory of corner point [1,1] as R evolves\")\n",
    "print(\"  • ★ GOAL (black star with gold outline): Ground truth target\")\n",
    "print(\"  • Dashed line: Gap between final position and GOAL\")\n",
    "print()\n",
    "print(\"Key Observations:\")\n",
    "print(\"1. R starts at some initial transformation (orange)\")\n",
    "print(\"2. R evolves through NON-ORTHOGONAL transformations (green path)\")\n",
    "print(\"3. R converges to final transformation (magenta)\")\n",
    "print(\"4. Some converge toward GOAL (R=I), others miss it completely!\")\n",
    "print(\"5. The evolution path is NOT constrained to O(2)!\")\n",
    "print(\"6. ✓ = Reached goal (lucky initialization)\")\n",
    "print(\"7. ✗ = Missed goal (stuck in local minimum)\")\n",
    "print()\n",
    "print(\"Critical Insight:\")\n",
    "print(\"  → We ARE searching for optimal R, but IMPLICITLY through (P,C)\")\n",
    "print(\"  → The search path can go through non-orthogonal R\")\n",
    "print(\"  → Different starting R₀ → different final R\")\n",
    "print(\"  → Only SOME initializations reach the goal (if lucky!)\")\n",
    "print(\"  → Most get stuck in local minima far from ground truth\")\n",
    "print(\"  → This is why single initialization fails!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze properties of R along the trajectory\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    R_traj = R_trajectories[name]\n",
    "    \n",
    "    # Compute properties at each step\n",
    "    iterations = np.arange(len(R_traj))\n",
    "    determinants = [np.linalg.det(R) for R in R_traj]\n",
    "    \n",
    "    # For 2D, compute rotation angle (arctan2 of R[1,0] and R[0,0])\n",
    "    # This only makes sense if R is close to a rotation matrix\n",
    "    angles_deg = [np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi for R in R_traj]\n",
    "    \n",
    "    # Compute orthogonality measure: ||R^T R - I||_F\n",
    "    orthogonality = [np.linalg.norm(R.T @ R - np.eye(2), 'fro') for R in R_traj]\n",
    "    \n",
    "    # Compute distance from identity: ||R - I||_F\n",
    "    dist_from_identity = [np.linalg.norm(R - np.eye(2), 'fro') for R in R_traj]\n",
    "    \n",
    "    # Plot all properties\n",
    "    ax2 = ax.twinx()\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    \n",
    "    line1 = ax.plot(iterations, determinants, 'b-o', linewidth=2, markersize=4, \n",
    "                    label='det(R)', alpha=0.7)\n",
    "    line2 = ax2.plot(iterations, orthogonality, 'r-s', linewidth=2, markersize=4,\n",
    "                     label='||R^T R - I||', alpha=0.7)\n",
    "    line3 = ax3.plot(iterations, dist_from_identity, 'g-^', linewidth=2, markersize=4,\n",
    "                     label='||R - I||', alpha=0.7)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('ALS Iteration', fontsize=9)\n",
    "    ax.set_ylabel('det(R)', fontsize=9, color='b')\n",
    "    ax2.set_ylabel('Orthogonality error', fontsize=9, color='r')\n",
    "    ax3.set_ylabel('Distance from I', fontsize=9, color='g')\n",
    "    \n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax3.tick_params(axis='y', labelcolor='g')\n",
    "    \n",
    "    ax.axhline(y=1, color='b', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax.axhline(y=-1, color='b', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    ax.set_title(f'{name}\\nR properties over time', fontweight='bold', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined legend\n",
    "    lines = line1 + line2 + line3\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=7, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROPERTIES OF R DURING OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Tracking three key properties:\")\n",
    "print()\n",
    "print(\"1. det(R) - Determinant:\")\n",
    "print(\"   • det = +1: proper rotation\")\n",
    "print(\"   • det = -1: improper (rotation + reflection)\")\n",
    "print(\"   • Other values: NON-ORTHOGONAL transformation\")\n",
    "print()\n",
    "print(\"2. ||R^T R - I||_F - Orthogonality error:\")\n",
    "print(\"   • = 0: R is orthogonal\")\n",
    "print(\"   • > 0: R deviates from orthogonality\")\n",
    "print()\n",
    "print(\"3. ||R - I||_F - Distance from identity:\")\n",
    "print(\"   • = 0: R is identity (ground truth)\")\n",
    "print(\"   • Measures how far we've transformed from truth\")\n",
    "print()\n",
    "print(\"Key Findings:\")\n",
    "print(\"✓ R trajectories pass through NON-ORTHOGONAL transformations!\")\n",
    "print(\"  → det(R) varies continuously, not fixed at ±1\")\n",
    "print(\"  → Orthogonality error > 0 during intermediate steps\")\n",
    "print()\n",
    "print(\"✓ Starting from orthogonal R doesn't constrain path to O(2)\")\n",
    "print(\"  → Gradient descent in (P,C) space allows non-orthogonal R\")\n",
    "print(\"  → Non-negativity projection breaks orthogonality\")\n",
    "print()\n",
    "print(\"✓ Different R₀ lead to different R_final\")\n",
    "print(\"  → Local minima in R-space\")\n",
    "print(\"  → Single initialization = random R_final selection\")\n",
    "print()\n",
    "print(\"→ This confirms: We're implicitly searching for optimal R,\")\n",
    "print(\"  but through unconstrained optimization in (P,C) space!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d17342f",
   "metadata": {},
   "source": [
    "### Quantitative Analysis: Are These Solutions Actually Different?\n",
    "\n",
    "Let's check if the converged solutions are genuinely different or just relabeled versions of the same solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33f366",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Key Takeaway: The Two-Space Picture\n",
    "\n",
    "We've now seen optimization from TWO complementary perspectives:\n",
    "\n",
    "1. **(P, C) space** (Part 8 visualization): Shows gradient descent paths through concentration profiles\n",
    "2. **R-space** (Part 9 visualization): Shows the implicit transformation being searched for\n",
    "\n",
    "**Critical insight**: Even though we START from orthogonal R, the optimization path goes through NON-ORTHOGONAL transformations! The non-negativity constraint breaks the orthogonality during gradient descent.\n",
    "\n",
    "This explains why single initialization fails: different starting R → different convergence basins → different final solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare solutions pairwise\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARING CONVERGED SOLUTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "names = list(results.keys())\n",
    "baseline = results[names[0]]['C_opt']  # Use first solution as reference\n",
    "\n",
    "print(f\"\\nReference solution: {names[0]}\")\n",
    "print(f\"Component peaks at frames: {np.argmax(baseline, axis=1)}\")\n",
    "\n",
    "for i, name in enumerate(names[1:], 1):\n",
    "    C_opt = results[name]['C_opt']\n",
    "    \n",
    "    # Check if solutions are identical (up to permutation)\n",
    "    # Direct comparison\n",
    "    diff_direct = np.linalg.norm(C_opt - baseline)\n",
    "    \n",
    "    # Permuted comparison (swap components)\n",
    "    C_permuted = C_opt[[1, 0], :]  # Swap rows\n",
    "    diff_permuted = np.linalg.norm(C_permuted - baseline)\n",
    "    \n",
    "    # Which is closer?\n",
    "    is_permuted = diff_permuted < diff_direct\n",
    "    min_diff = min(diff_direct, diff_permuted)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Component peaks at: {np.argmax(C_opt, axis=1)}\")\n",
    "    print(f\"  Difference (direct):    {diff_direct:.4f}\")\n",
    "    print(f\"  Difference (permuted):  {diff_permuted:.4f}\")\n",
    "    print(f\"  → {'PERMUTED' if is_permuted else 'SAME ORDER'} (diff={min_diff:.4f})\")\n",
    "    \n",
    "    if min_diff > 0.5:\n",
    "        print(f\"  → SIGNIFICANTLY DIFFERENT SOLUTION!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CRITICAL FINDING:\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Some solutions converge to SAME decomposition (small differences)\")\n",
    "print(\"✓ Some solutions converge to PERMUTED decomposition (components swapped)\")\n",
    "print(\"✓ Some solutions converge to genuinely DIFFERENT decomposition\")\n",
    "print()\n",
    "print(\"→ This proves: Multiple local minima exist!\")\n",
    "print(\"→ Single initialization randomly picks one\")\n",
    "print(\"→ No way to know if the chosen solution is 'best'\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96249e14",
   "metadata": {},
   "source": [
    "### Summary: Why REGALS Cannot Search All O(n) Classes\n",
    "\n",
    "This toy example demonstrates the fundamental limitation:\n",
    "\n",
    "1. **Different orthogonal transformations** create different valid starting points\n",
    "2. **Local optimization (ALS)** converges to nearest local minimum\n",
    "3. **Non-negativity constraint** creates discrete branches (some initial transformations violate it)\n",
    "4. **Single initialization** randomly selects ONE branch\n",
    "5. **No mechanism** to systematically explore all O(n) classes\n",
    "\n",
    "**For REGALS specifically:**\n",
    "- Uses SVD + boundary conditions → ONE random initialization\n",
    "- Runs ALS with regularization → converges to ONE local minimum\n",
    "- No multi-start strategy → misses alternative valid decompositions\n",
    "- Cannot verify optimality → user doesn't know if solution is \"best\"\n",
    "\n",
    "**This validates our hypothesis**: Permutation ambiguity creates discrete local minima, and single-initialization methods cannot systematically explore all possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e4aef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Conclusion — The Global Optimization Gap\n",
    "\n",
    "This notebook has demonstrated a fundamental limitation in matrix factorization methods that rely on local optimization:\n",
    "\n",
    "### What We've Proven\n",
    "\n",
    "1. **Orthogonal ambiguity exists** (Parts 1-4): Without constraints, infinitely many solutions related by O(n) transformations fit data equally well\n",
    "\n",
    "2. **O(n) is rich and complex** (Part 4.5): In 3D+, includes rotations, reflections, rotoinversions, and inversions - not just simple rotations\n",
    "\n",
    "3. **Non-negativity creates discrete branches** (Part 5): Most orthogonal transformations violate non-negativity, leaving only a small discrete set of valid permutations\n",
    "\n",
    "4. **Local optimization cannot explore all branches** (Part 6): Single initialization converges to ONE local minimum, missing alternative valid solutions\n",
    "\n",
    "### Implications for REGALS and Similar Methods\n",
    "\n",
    "**REGALS specifically:**\n",
    "- Uses single SVD-based initialization\n",
    "- Employs local optimization (ALS with regularization)\n",
    "- Cannot systematically search O(n) transformation space\n",
    "- No verification that found solution is globally optimal\n",
    "\n",
    "**The gap:**\n",
    "- Some datasets have 2-6 valid permutations (discrete local minima)\n",
    "- Single initialization randomly selects one\n",
    "- 5-50% of real SEC-SAXS data may have this ambiguity\n",
    "- Users must manually validate physical plausibility\n",
    "\n",
    "### What Would Be Needed\n",
    "\n",
    "**Global optimization strategies:**\n",
    "1. Multi-start from different O(n) initializations\n",
    "2. Systematic enumeration of valid permutations  \n",
    "3. Basin-hopping or nested sampling\n",
    "4. Bayesian model comparison\n",
    "\n",
    "**For parametric models (lower dimensional):**\n",
    "- Global optimization becomes computationally tractable\n",
    "- Can explore all discrete possibilities systematically\n",
    "- Enables uncertainty quantification\n",
    "\n",
    "**This is the \"global optimization gap\" in model-free SEC-SAXS decomposition methods.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
