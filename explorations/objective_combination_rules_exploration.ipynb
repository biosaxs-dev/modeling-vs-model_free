{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0fe4cb",
   "metadata": {},
   "source": [
    "# Exploration: Alternative Objective Function Combination Rules\n",
    "\n",
    "**Goal**: Investigate how different mathematical operators for combining constraints encode different logical relationships and lead to fundamentally different solutions.\n",
    "\n",
    "## The Fundamental Question\n",
    "\n",
    "Standard regularized optimization uses **additive** combination:\n",
    "$$\\min_{P,C} \\|M - PC\\|^2 + \\lambda\\|D^2C\\|^2$$\n",
    "\n",
    "But why addition? What about:\n",
    "- **Multiplicative**: $\\|M - PC\\|^2 \\times \\lambda\\|D^2C\\|^2$\n",
    "- **Log-additive**: $\\log(\\|M - PC\\|^2) + \\lambda\\log(\\|D^2C\\|^2)$\n",
    "- **Weighted power**: $(\\|M - PC\\|^2)^p + \\lambda(\\|D^2C\\|^2)^q$\n",
    "- **Max operator**: $\\max(\\|M - PC\\|^2, \\lambda\\|D^2C\\|^2)$\n",
    "\n",
    "## Key Insight: Operators Encode Logic\n",
    "\n",
    "- **Addition (+)**: \"OR\"-like behavior - allows trade-offs between constraints\n",
    "- **Multiplication (×)**: \"AND\"-like enforcement - forces both terms to be balanced\n",
    "- **Log-additive**: \"AND\"-like (multiplicative in original space)\n",
    "- **Max**: \"Worst-case\" logic - minimize the larger violation\n",
    "\n",
    "**Each choice is a modeling decision about how constraints should interact!**\n",
    "\n",
    "---\n",
    "\n",
    "**Context**: Supporting analysis for underdeterminedness_exploration.ipynb Level 2 discussion.  \n",
    "**Date**: January 27, 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import qr\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9729d",
   "metadata": {},
   "source": [
    "## 1. Generate Test Data\n",
    "\n",
    "We'll use the same SEC-SAXS-like synthetic data as in underdeterminedness_exploration.ipynb for direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166486d2",
   "metadata": {},
   "source": [
    "## A Critical Note on \"AND\" vs \"OR\" Language\n",
    "\n",
    "**Important**: In this notebook, we use \"AND-like\" and \"OR-like\" to describe **optimization behavior**, not probabilistic logic:\n",
    "\n",
    "### Gradient Analysis Reveals True Behavior:\n",
    "\n",
    "**Additive: $f(A,B) = A + \\lambda B$**\n",
    "$$\\frac{\\partial f}{\\partial A} = 1, \\quad \\frac{\\partial f}{\\partial B} = \\lambda$$\n",
    "\n",
    "The gradients are **independent**. If we trade: $(A + \\epsilon) + (B - \\epsilon) = A + B$ (unchanged at first order)\n",
    "- **Allows marginal substitution** between constraints\n",
    "- → **\"OR\"-like**: Can trade one for the other\n",
    "\n",
    "**Multiplicative: $f(A,B) = A \\times \\lambda B$**\n",
    "$$\\frac{\\partial f}{\\partial A} = \\lambda B, \\quad \\frac{\\partial f}{\\partial B} = \\lambda A$$\n",
    "\n",
    "The gradients are **coupled**. If we trade: $(A + \\epsilon)(B - \\epsilon) = AB + (A-B)\\epsilon - \\epsilon^2$\n",
    "- If $A > B$: change is positive → push A down\n",
    "- If $A < B$: change is negative → but then A must catch up\n",
    "- **Forces balance** between A and B\n",
    "- → **\"AND\"-like**: Must satisfy both comparably\n",
    "\n",
    "**Note**: This is opposite to probability theory where $P(A \\cap B) = P(A) \\times P(B)$. Here we focus on **optimization dynamics**, not probabilistic semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2-component SEC-SAXS-like data\n",
    "n_q = 100  # Number of q-points (scattering angles)\n",
    "n_t = 50   # Number of time frames\n",
    "n_comp = 2  # Number of components\n",
    "\n",
    "# True components with intentional structure\n",
    "print(\"Generating ground truth components...\")\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create smooth, well-separated concentration profiles\n",
    "t = np.linspace(0, 1, n_t)\n",
    "C_true = np.zeros((n_comp, n_t))\n",
    "C_true[0, :] = np.exp(-50*(t - 0.3)**2)  # Peak at t=0.3\n",
    "C_true[1, :] = np.exp(-50*(t - 0.7)**2)  # Peak at t=0.7\n",
    "\n",
    "# Positive SAXS profiles\n",
    "P_true = np.random.rand(n_q, n_comp) + 1.0\n",
    "\n",
    "# Compute measured data\n",
    "M = P_true @ C_true\n",
    "\n",
    "print(f\"Data matrix M: {M.shape}\")\n",
    "print(f\"True P: {P_true.shape}, True C: {C_true.shape}\")\n",
    "print(f\"Reconstruction error (should be zero): {np.linalg.norm(M - P_true @ C_true):.2e}\")\n",
    "\n",
    "# Visualize true components\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(t, C_true[0, :], 'b-', linewidth=2, label='Component 1')\n",
    "axes[0].plot(t, C_true[1, :], 'r-', linewidth=2, label='Component 2')\n",
    "axes[0].set_title('True Concentration Profiles', fontweight='bold')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Concentration')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "im = axes[1].imshow(M, aspect='auto', cmap='viridis')\n",
    "axes[1].set_title('Measured Data Matrix M', fontweight='bold')\n",
    "axes[1].set_xlabel('Time Frame')\n",
    "axes[1].set_ylabel('q-point')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Ground truth data generated with smooth, well-separated profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff15ee5",
   "metadata": {},
   "source": [
    "## 2. Define Objective Functions with Different Combination Rules\n",
    "\n",
    "We'll implement multiple variants and compare their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothness_penalty(C):\n",
    "    \"\"\"\n",
    "    Compute smoothness penalty: ||D²C||²_F\n",
    "    where D² is the second derivative operator (discrete approximation)\n",
    "    \"\"\"\n",
    "    n_comp, n_t = C.shape\n",
    "    \n",
    "    # Second derivative via finite differences: d²f/dt² ≈ f(t-1) - 2f(t) + f(t+1)\n",
    "    D2C = np.zeros((n_comp, n_t - 2))\n",
    "    for i in range(n_comp):\n",
    "        for j in range(n_t - 2):\n",
    "            D2C[i, j] = C[i, j] - 2*C[i, j+1] + C[i, j+2]\n",
    "    \n",
    "    return np.linalg.norm(D2C, 'fro')**2\n",
    "\n",
    "def data_fit(P, C, M):\n",
    "    \"\"\"Data-fit term: ||M - PC||²\"\"\"\n",
    "    return np.linalg.norm(M - P @ C)**2\n",
    "\n",
    "# Define different combination rules\n",
    "def objective_additive(P, C, M, lambda_reg=1.0):\n",
    "    \"\"\"Standard additive: A + λB (allows trade-offs, OR-like)\"\"\"\n",
    "    A = data_fit(P, C, M)\n",
    "    B = smoothness_penalty(C)\n",
    "    return A + lambda_reg * B\n",
    "\n",
    "def objective_multiplicative(P, C, M, lambda_reg=1.0):\n",
    "    \"\"\"Multiplicative: A × λB (enforces balance, AND-like)\"\"\"\n",
    "    A = data_fit(P, C, M)\n",
    "    B = smoothness_penalty(C)\n",
    "    # Add small epsilon to avoid zeros\n",
    "    return (A + 1e-6) * (lambda_reg * B + 1e-6)\n",
    "\n",
    "def objective_log_additive(P, C, M, lambda_reg=1.0):\n",
    "    \"\"\"Log-additive: log(A) + λ log(B)\"\"\"\n",
    "    A = data_fit(P, C, M)\n",
    "    B = smoothness_penalty(C)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    return np.log(A + 1e-6) + lambda_reg * np.log(B + 1e-6)\n",
    "\n",
    "def objective_max(P, C, M, lambda_reg=1.0):\n",
    "    \"\"\"Max operator: max(A, λB) (worst-case, strongest AND enforcement)\"\"\"\n",
    "    A = data_fit(P, C, M)\n",
    "    B = smoothness_penalty(C)\n",
    "    return max(A, lambda_reg * B)\n",
    "\n",
    "def objective_weighted_power(P, C, M, lambda_reg=1.0, p=0.5, q=0.5):\n",
    "    \"\"\"Weighted power: A^p + λB^q\"\"\"\n",
    "    A = data_fit(P, C, M)\n",
    "    B = smoothness_penalty(C)\n",
    "    return A**p + lambda_reg * B**q\n",
    "\n",
    "print(\"Objective functions defined!\")\n",
    "print(\"\\nAvailable combination rules:\")\n",
    "print(\"  1. Additive:         A + λB\")\n",
    "print(\"  2. Multiplicative:   A × λB\")\n",
    "print(\"  3. Log-additive:     log(A) + λ log(B)\")\n",
    "print(\"  4. Max:              max(A, λB)\")\n",
    "print(\"  5. Weighted power:   A^p + λB^q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99479fb5",
   "metadata": {},
   "source": [
    "## 3. Test: How Do Different Rules Respond to the Same Data?\n",
    "\n",
    "Let's evaluate all objective functions on the same solution to see how they score it differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a850d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test solution (random orthogonal transformation of SVD)\n",
    "from scipy.linalg import svd\n",
    "\n",
    "U, S, Vt = svd(M, full_matrices=False)\n",
    "P_svd = U[:, :n_comp] @ np.diag(np.sqrt(S[:n_comp]))\n",
    "C_svd = np.diag(np.sqrt(S[:n_comp])) @ Vt[:n_comp, :]\n",
    "\n",
    "# Apply random orthogonal transformation\n",
    "R = qr(np.random.randn(n_comp, n_comp))[0]\n",
    "P_test = P_svd @ R\n",
    "C_test = np.linalg.inv(R) @ C_svd\n",
    "\n",
    "lambda_test = 1.0\n",
    "\n",
    "print(\"Testing different objective functions on the same solution:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSolution characteristics:\")\n",
    "print(f\"  Data-fit:  ||M - PC||² = {data_fit(P_test, C_test, M):.4e}\")\n",
    "print(f\"  Smoothness: ||D²C||²   = {smoothness_penalty(C_test):.4f}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "objectives = {\n",
    "    'Additive (A + λB)': objective_additive(P_test, C_test, M, lambda_test),\n",
    "    'Multiplicative (A × λB)': objective_multiplicative(P_test, C_test, M, lambda_test),\n",
    "    'Log-additive (log A + λ log B)': objective_log_additive(P_test, C_test, M, lambda_test),\n",
    "    'Max (max(A, λB))': objective_max(P_test, C_test, M, lambda_test),\n",
    "    'Weighted power (A^0.5 + λB^0.5)': objective_weighted_power(P_test, C_test, M, lambda_test, 0.5, 0.5),\n",
    "}\n",
    "\n",
    "print(\"Objective values:\")\n",
    "for name, value in objectives.items():\n",
    "    print(f\"  {name:<35} = {value:>12.6e}\")\n",
    "\n",
    "print(\"\\n✓ Same solution scores VERY differently under different combination rules!\")\n",
    "print(\"→ Each rule encodes different notion of 'optimal' solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4af77",
   "metadata": {},
   "source": [
    "## 4. Key Experiment: How Do Rules Respond to Trade-offs?\n",
    "\n",
    "Let's create solutions that:\n",
    "1. Fit data perfectly but are rough (high smoothness penalty)\n",
    "2. Are very smooth but don't fit data well\n",
    "\n",
    "This reveals the **logical structure** encoded by each operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extreme solutions to probe logical structure\n",
    "\n",
    "# Solution 1: Perfect data-fit, rough (high curvature)\n",
    "P_perfect = P_true.copy()\n",
    "C_rough = C_true.copy()\n",
    "# Add high-frequency noise to make it rough\n",
    "C_rough += 0.1 * np.random.randn(*C_true.shape)\n",
    "\n",
    "# Solution 2: Poor data-fit, very smooth\n",
    "C_smooth = np.zeros((n_comp, n_t))\n",
    "for i in range(n_comp):\n",
    "    # Ultra-smooth: just constant values\n",
    "    C_smooth[i, :] = np.mean(C_true[i, :])\n",
    "# Find P that minimizes data-fit for this smooth C\n",
    "P_smooth = M @ C_smooth.T @ np.linalg.inv(C_smooth @ C_smooth.T)\n",
    "\n",
    "# Solution 3: Balanced (moderate fit, moderate smoothness)\n",
    "P_balanced = P_svd.copy()\n",
    "C_balanced = C_svd.copy()\n",
    "\n",
    "print(\"Created three test solutions:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "solutions = {\n",
    "    'Perfect Fit, Rough': (P_perfect, C_rough),\n",
    "    'Poor Fit, Very Smooth': (P_smooth, C_smooth),\n",
    "    'Balanced': (P_balanced, C_balanced),\n",
    "}\n",
    "\n",
    "print(\"\\n{:<25} {:>12} {:>12}\".format(\"Solution\", \"Data-fit (A)\", \"Smoothness (B)\"))\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, (P, C) in solutions.items():\n",
    "    A = data_fit(P, C, M)\n",
    "    B = smoothness_penalty(C)\n",
    "    print(f\"{name:<25} {A:>12.4e} {B:>12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Now let's see how different combination rules score these solutions:\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d50258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all combinations\n",
    "lambda_test = 1.0\n",
    "\n",
    "objective_functions = {\n",
    "    'Additive (A + λB)': objective_additive,\n",
    "    'Multiplicative (A × λB)': objective_multiplicative,\n",
    "    'Log-additive (log A + λ log B)': objective_log_additive,\n",
    "    'Max (max(A, λB))': objective_max,\n",
    "    'Weighted power (A^0.5 + λB^0.5)': objective_weighted_power,\n",
    "}\n",
    "\n",
    "print(\"\\n{:<25} {:>15} {:>15} {:>15}\".format(\n",
    "    \"Combination Rule\", \"Perfect+Rough\", \"Poor+Smooth\", \"Balanced\"))\n",
    "print(\"-\"*70)\n",
    "\n",
    "for obj_name, obj_func in objective_functions.items():\n",
    "    scores = []\n",
    "    for sol_name, (P, C) in solutions.items():\n",
    "        if obj_name == 'Weighted power (A^0.5 + λB^0.5)':\n",
    "            score = obj_func(P, C, M, lambda_test, 0.5, 0.5)\n",
    "        else:\n",
    "            score = obj_func(P, C, M, lambda_test)\n",
    "        scores.append(score)\n",
    "    \n",
    "    print(f\"{obj_name:<25} {scores[0]:>15.4e} {scores[1]:>15.4e} {scores[2]:>15.4e}\")\n",
    "    \n",
    "    # Show which solution is preferred (lowest score)\n",
    "    best_idx = np.argmin(scores)\n",
    "    best_name = list(solutions.keys())[best_idx]\n",
    "    print(f\"{'→ Prefers:':<25} {best_name}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"  • Additive (OR-like): Allows trading one constraint for another\")\n",
    "print(\"  • Multiplicative (AND-like): Forces balance between constraints\")\n",
    "print(\"  • Log-additive: Scale-insensitive, also AND-like in original space\")\n",
    "print(\"  • Max: Only cares about the WORST violation (strongest AND)\")\n",
    "print(\"\\n→ Each rule encodes fundamentally different notion of optimality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27b4f6",
   "metadata": {},
   "source": [
    "## 5. Visualize Trade-off Surfaces\n",
    "\n",
    "Let's visualize how different combination rules create different \"valleys\" in the objective landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of (A, B) values\n",
    "A_values = np.logspace(-2, 2, 100)  # Data-fit values\n",
    "B_values = np.linspace(0.01, 10, 100)  # Smoothness values\n",
    "\n",
    "A_grid, B_grid = np.meshgrid(A_values, B_values)\n",
    "\n",
    "lambda_reg = 1.0\n",
    "\n",
    "# Compute objective surfaces\n",
    "Z_additive = A_grid + lambda_reg * B_grid\n",
    "Z_multiplicative = A_grid * (lambda_reg * B_grid)\n",
    "Z_log_additive = np.log(A_grid + 1e-10) + lambda_reg * np.log(B_grid + 1e-10)\n",
    "Z_max = np.maximum(A_grid, lambda_reg * B_grid)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "surfaces = [\n",
    "    (Z_additive, 'Additive: A + λB\\n(OR-like: allows trade-offs)', axes[0, 0]),\n",
    "    (Z_multiplicative, 'Multiplicative: A × λB\\n(AND-like: enforces balance)', axes[0, 1]),\n",
    "    (Z_log_additive, 'Log-additive: log(A) + λ log(B)\\n(AND-like: scale-invariant)', axes[1, 0]),\n",
    "    (Z_max, 'Max: max(A, λB)\\n(Strongest AND: worst-case)', axes[1, 1]),\n",
    "]\n",
    "\n",
    "for Z, title, ax in surfaces:\n",
    "    # Use log scale for A-axis\n",
    "    contour = ax.contourf(A_grid, B_grid, Z, levels=20, cmap='viridis')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Data-fit term (A)', fontsize=10)\n",
    "    ax.set_ylabel('Smoothness term (B)', fontsize=10)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Add contour lines\n",
    "    ax.contour(A_grid, B_grid, Z, levels=10, colors='white', alpha=0.3, linewidths=0.5)\n",
    "    \n",
    "    # Mark our three test solutions\n",
    "    for name, (P, C) in solutions.items():\n",
    "        A_val = data_fit(P, C, M)\n",
    "        B_val = smoothness_penalty(C)\n",
    "        marker = 'o' if name == 'Balanced' else ('s' if 'Rough' in name else '^')\n",
    "        ax.plot(A_val, B_val, marker, color='red', markersize=10, \n",
    "                markeredgecolor='white', markeredgewidth=2, label=name)\n",
    "    \n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(contour, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('objective_combination_rules_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: objective_combination_rules_comparison.png\")\n",
    "print(\"\\nKey insight from contour plots:\")\n",
    "print(\"  • Additive: Linear trade-offs (can substitute A for B)\")\n",
    "print(\"  • Multiplicative: Hyperbolic valleys (must balance both A and B)\")\n",
    "print(\"  • Log-additive: Different scale sensitivity, also enforces balance\")\n",
    "print(\"  • Max: L-shaped optimal region (only worst matters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd70c5a",
   "metadata": {},
   "source": [
    "## 6. Optimization Dynamics: Trade-offs vs Balance Enforcement\n",
    "\n",
    "Let's demonstrate explicitly how additive (+) allows trade-offs while multiplicative (×) enforces balance through coupled gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extreme test cases\n",
    "print(\"=\"*70)\n",
    "print(\"DEMONSTRATING LOGICAL STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Case 1: A ≈ 0, B large (fits data perfectly, very rough)\n",
    "A1, B1 = 1e-10, 100.0\n",
    "\n",
    "# Case 2: A large, B ≈ 0 (poor fit, perfectly smooth)\n",
    "A2, B2 = 100.0, 1e-10\n",
    "\n",
    "# Case 3: A and B moderate (balanced)\n",
    "A3, B3 = 1.0, 1.0\n",
    "\n",
    "lambda_reg = 1.0\n",
    "\n",
    "print(\"\\nThree extreme cases:\")\n",
    "print(f\"\\nCase 1: A ≈ 0,    B = {B1:.1f}   (perfect fit, very rough)\")\n",
    "print(f\"Case 2: A = {A2:.1f}, B ≈ 0      (poor fit, perfectly smooth)\")\n",
    "print(f\"Case 3: A = {A3:.1f}, B = {B3:.1f}    (balanced)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"How different combination rules score these:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "cases = [(A1, B1), (A2, B2), (A3, B3)]\n",
    "case_names = ['Case 1\\n(A≈0, B large)', 'Case 2\\n(A large, B≈0)', 'Case 3\\n(balanced)']\n",
    "\n",
    "# Additive\n",
    "add_scores = [A + lambda_reg * B for A, B in cases]\n",
    "print(f\"\\nAdditive (A + λB) - allows trade-offs (OR-like):\")\n",
    "for name, score in zip(case_names, add_scores):\n",
    "    print(f\"  {name.replace(chr(10), ' '):<25} = {score:.4e}\")\n",
    "print(f\"  → Preferred: {case_names[np.argmin(add_scores)].replace(chr(10), ' ')}\")\n",
    "print(f\"  → Interpretation: Can trade one constraint for another at the margin\")\n",
    "\n",
    "# Multiplicative\n",
    "mult_scores = [(A + 1e-10) * (lambda_reg * B + 1e-10) for A, B in cases]\n",
    "print(f\"\\nMultiplicative (A × λB) - enforces balance (AND-like):\")\n",
    "for name, score in zip(case_names, mult_scores):\n",
    "    print(f\"  {name.replace(chr(10), ' '):<25} = {score:.4e}\")\n",
    "print(f\"  → Preferred: {case_names[np.argmin(mult_scores)].replace(chr(10), ' ')}\")\n",
    "print(f\"  → Interpretation: Gradients ∂/∂A = B, ∂/∂B = A force both terms to be comparable\")\n",
    "\n",
    "# Log-additive\n",
    "log_scores = [np.log(A + 1e-10) + lambda_reg * np.log(B + 1e-10) for A, B in cases]\n",
    "print(f\"\\nLog-additive (log A + λ log B) - multiplicative in original space:\")\n",
    "for name, score in zip(case_names, log_scores):\n",
    "    print(f\"  {name.replace(chr(10), ' '):<25} = {score:.4e}\")\n",
    "print(f\"  → Preferred: {case_names[np.argmin(log_scores)].replace(chr(10), ' ')}\")\n",
    "print(f\"  → Interpretation: Equivalent to minimizing A^1 × B^λ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATION:\")\n",
    "print(\"  • Additive (+): Allows trading constraints (∂/∂A = ∂/∂B = 1)\")\n",
    "print(\"  • Multiplicative (×): Forces balance (∂/∂A = B, ∂/∂B = A)\")\n",
    "print(\"  • Log-additive: Also enforces balance (multiplicative in original space)\")\n",
    "print(\"\\n→ The choice of operator fundamentally changes solution preferences!\")\n",
    "print(\"→ Marginal optimization dynamics differ, not just global extrema!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6da61f",
   "metadata": {},
   "source": [
    "## 7. Implications for \"Model-Free\" Claims\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "1. **Mathematical operators encode constraint interaction**\n",
    "   - Addition (+): \"OR\"-like - allows trading one constraint for another (∂/∂A = ∂/∂B = 1)\n",
    "   - Multiplication (×): \"AND\"-like - forces balance between constraints (∂/∂A = B, ∂/∂B = A)\n",
    "   - Log-additive: Also \"AND\"-like (multiplicative in original space)\n",
    "   - Max: Strongest \"AND\" enforcement - only worst violation matters\n",
    "\n",
    "2. **Different rules prefer fundamentally different solutions**\n",
    "   - Same data, same regularizer, but different combinations → different optima\n",
    "   - The choice is NOT a technical detail!\n",
    "\n",
    "3. **The standard additive form has strong assumptions**\n",
    "   - Additive combination ↔ Independence assumption (Bayesian interpretation)\n",
    "   - Gaussian likelihood + Gaussian prior → additive in log-probability space\n",
    "   - Allows marginal trade-offs between data-fit and smoothness\n",
    "   - This is a **modeling choice** about constraint interaction\n",
    "\n",
    "### The Broader Argument\n",
    "\n",
    "**Every \"model-free\" method must make this choice:**\n",
    "- Should constraints allow trade-offs (additive)?\n",
    "- Or enforce balance (multiplicative)?\n",
    "- Or allow trade-offs (multiplicative)?\n",
    "- Or worst-case optimization (max)?\n",
    "\n",
    "- Allows trading data-fit for smoothness at the margin\n",
    "- Encodes \"AND\" logic: must fit data AND be smooth\n",
    "- Assumes independence of errors and prior\n",
    "- Gaussian noise + Gaussian smoothness prior\n",
    "- **This is an implicit modeling assumption!**\n",
    "\n",
    "- Multiplicative: Would enforce balance between fit and smoothness (∂/∂A = B)\n",
    "- Log-additive: Also enforces balance, scale-insensitive\n",
    "- Log-additive: Different scale sensitivity, different optima\n",
    "- Max: Would only care about worst violation\n",
    "\n",
    "### The Fundamental Point\n",
    "\n",
    "**Before even choosing WHAT to regularize, you must choose HOW to combine terms.**\n",
    "\n",
    "This is **implicit modeling** at its most fundamental level:\n",
    "- Not about functional form of components\n",
    "- Not about which constraints to add\n",
    "- But about the **logical structure** of optimization itself!\n",
    "\n",
    "1. Whether constraints should allow trade-offs (additive) or enforce balance (multiplicative)\n",
    "2. The optimization dynamics through gradient structure (∂/∂A, ∂/∂B)\n",
    "2. What trade-offs are acceptable\n",
    "3. What constitutes an \"optimal\" solution\n",
    "\n",
    "**There is no \"model-free\" way to make this choice!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad4d76",
   "metadata": {},
   "source": [
    "## 8. Summary and Takeaways\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Additive combination (A + λB)**:\n",
    "   - Allows trade-offs: can substitute one constraint for another\n",
    "   - Gradients independent: ∂/∂A = 1, ∂/∂B = 1\n",
    "   - Corresponds to Gaussian assumptions in Bayesian framework\n",
    "   - Standard choice in regularization literature\n",
    "\n",
    "2. **Multiplicative combination (A × λB)**:\n",
    "   - Enforces balance between terms\n",
    "   - Coupled gradients: ∂/∂A = B, ∂/∂B = A force A ≈ B\n",
    "   - Optimization dynamics prevent one term dominating\n",
    "   - Non-standard, no clear probabilistic interpretation\n",
    "\n",
    "3. **Log-additive combination (log A + λ log B)**:\n",
    "   - Scale-invariant behavior\n",
    "   - Equivalent to minimizing $A^1 \\times B^\\lambda$ in original space (multiplicative)\n",
    "   - Also enforces balance like multiplicative form\n",
    "   - Could correspond to log-normal distributions\n",
    "\n",
    "4. **Max combination (max(A, λB))**:\n",
    "   - \"Worst-case\" logic: only cares about largest violation\n",
    "   - Minimax optimization framework\n",
    "   - Very different optimal solutions\n",
    "\n",
    "### For the JOSS Paper Argument\n",
    "\n",
    "This exploration strengthens the critique of \"model-free\" claims by showing:\n",
    "\n",
    "**Level 1: The operator choice itself is a modeling decision**\n",
    "- Before choosing regularizer form\n",
    "- Before choosing constraint types\n",
    "- The basic mathematics of combination encodes assumptions\n",
    "\n",
    "**Level 2: Each choice has strong implications**\n",
    "- Different operators → fundamentally different solutions\n",
    "- No mathematical reason to prefer one over others a priori\n",
    "- Choice depends on domain beliefs about constraint interaction\n",
    "\n",
    "**Level 3: REGALS's additive choice is implicit modeling**\n",
    "- Additive = trade-off allowance + independence + Gaussian assumptions\n",
    "- Alternative choices (multiplicative) would enforce balance differently\n",
    "- This choice is hidden in the mathematical formulation\n",
    "\n",
    "**Level 4: Transparency matters**\n",
    "- Explicit models state these choices upfront\n",
    "- Implicit models hide them in optimization structure\n",
    "- Both are modeling - difference is transparency!\n",
    "\n",
    "### Future Research Directions\n",
    "\n",
    "1. **Theoretical**: Characterize solution sets for each combination rule\n",
    "2. **Empirical**: Test on real SEC-SAXS data - does choice matter in practice?\n",
    "3. **Bayesian**: Formal probabilistic interpretation of each rule\n",
    "4. **Hybrid**: Can we design adaptive rules that switch based on data?\n",
    "\n",
    "---\n",
    "\n",
    "**Created**: January 27, 2026  \n",
    "**Context**: Supporting analysis for [underdeterminedness_exploration.ipynb](underdeterminedness_exploration.ipynb) Level 2 discussion  \n",
    "**Status**: Complete - ready for paper integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
