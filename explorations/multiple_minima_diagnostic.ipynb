{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f295119c",
   "metadata": {},
   "source": [
    "# Multiple Minima Diagnostic: Objective vs Optimization Problem\n",
    "\n",
    "**Purpose**: Determine whether the Multiple Minima Conjecture (Stage 5) describes:\n",
    "- **Hypothesis 1**: Objective landscape problem (algorithm-independent)\n",
    "- **Hypothesis 2**: Optimization method problem (ALS-specific)\n",
    "\n",
    "**Context**: \n",
    "- Stage 5 found degeneracy with ALS + additive objective (0-35% success)\n",
    "- Stage 7 found log-additive escapes somewhat (25% success with ALS)\n",
    "- Molass uses log-additive + Basin-hopping (NOT ALS) + additional constraints (Rg-consistency)\n",
    "- **This experiment**: Not for precise Molass comparison, but to understand conjecture scope\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. **Does degenerate solution have LOWER objective value?** (Hypothesis 1 test)\n",
    "   - Compare obj(true) vs obj(degenerate) for additive objective\n",
    "   - If obj(degenerate) ≤ obj(true) → landscape problem (all optimizers affected)\n",
    "   - If obj(degenerate) > obj(true) → ALS gets trapped (global optimizers might escape)\n",
    "\n",
    "2. **Does Basin-hopping find true solution more reliably?**\n",
    "   - Test additive + Basin-hopping (vs additive + ALS baseline: 35%)\n",
    "   - Test log-additive + Basin-hopping (vs log-additive + ALS baseline: 25%)\n",
    "   - If success improves → partly optimization problem\n",
    "   - If success stays low → landscape problem\n",
    "\n",
    "3. **How does this inform conjecture relevance for Molass?**\n",
    "   - Molass uses log-additive + Basin-hopping + Rg-consistency\n",
    "   - If log-additive + Basin-hopping avoids multiple minima → conjecture doesn't constrain Molass\n",
    "   - If multiple minima persist → conjecture identifies fundamental challenge Molass must address\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "**Same problem as Stages 5-7**:\n",
    "- 2 components, correlated SAXS profiles (r ≈ 0.88)\n",
    "- SEC-SAXS realistic setup (Guinier-Porod profiles)\n",
    "- Generic smoothness Q = (D²)ᵀD²\n",
    "- λ = 0.1\n",
    "\n",
    "**Tests**:\n",
    "1. **Objective comparison**: Evaluate true vs degenerate solutions\n",
    "2. **Basin-hopping + additive**: Multi-start experiment (10 trials)\n",
    "3. **Basin-hopping + log-additive**: Multi-start experiment (10 trials)\n",
    "4. **Compare to ALS baselines**\n",
    "\n",
    "**Note**: This does NOT test Molass's full approach (missing Rg-consistency, parametric models, etc.). Purpose is understanding conjecture scope only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ccce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize, basinhopping\n",
    "from scipy.linalg import lstsq\n",
    "from molass.SAXS.Models.Simple import guinier_porod\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732ba91",
   "metadata": {},
   "source": [
    "## Section 1: Generate Realistic SEC-SAXS Data\n",
    "\n",
    "Same setup as Stage 8 (realistic Guinier-Porod profiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem dimensions\n",
    "n_components = 2\n",
    "n_q = 50  # q points\n",
    "K = 80     # frames\n",
    "\n",
    "# q-range for SAXS\n",
    "q = np.logspace(-2, 0, n_q)  # 0.01 to 1.0 Å⁻¹\n",
    "\n",
    "# Generate realistic SAXS profiles using Guinier-Porod model\n",
    "# Component 1: Larger particle (Rg = 40 Å), elutes EARLY (frame 35)\n",
    "Rg1 = 40.0  # Radius of gyration\n",
    "d1 = 4.0    # Porod exponent (sphere)\n",
    "G1 = 0.1    # Guinier prefactor\n",
    "p1_true = guinier_porod(q, G1, Rg1, d1)\n",
    "\n",
    "# Component 2: Smaller particle (Rg = 20 Å), elutes LATE (frame 55)\n",
    "Rg2 = 20.0\n",
    "d2 = 4.0\n",
    "G2 = 0.08\n",
    "p2_true = guinier_porod(q, G2, Rg2, d2)\n",
    "\n",
    "# Normalize to unit max\n",
    "p1_true = p1_true / np.max(p1_true)\n",
    "p2_true = p2_true / np.max(p2_true)\n",
    "P_true = np.column_stack([p1_true, p2_true])\n",
    "\n",
    "# Generate elution curves (Gaussians)\n",
    "frames = np.arange(K)\n",
    "c1_true = np.exp(-((frames - 35)**2) / (2 * 4**2))  # Early, narrower\n",
    "c2_true = np.exp(-((frames - 55)**2) / (2 * 6**2))  # Late, broader\n",
    "\n",
    "# Normalize\n",
    "c1_true = c1_true / np.max(c1_true)\n",
    "c2_true = c2_true / np.max(c2_true)\n",
    "C_true = np.vstack([c1_true, c2_true])\n",
    "\n",
    "# Generate data\n",
    "M_clean = P_true @ C_true\n",
    "noise_level = 0.01 * np.max(M_clean)\n",
    "noise = np.random.normal(0, noise_level, M_clean.shape)\n",
    "M = M_clean + noise\n",
    "\n",
    "# Calculate profile correlation\n",
    "correlation = np.corrcoef(p1_true, p2_true)[0, 1]\n",
    "\n",
    "print(f\"Data shape: {M.shape}\")\n",
    "print(f\"SNR: {np.max(M_clean) / noise_level:.1f}\")\n",
    "print(f\"Profile correlation: {correlation:.3f}\")\n",
    "print(f\"Component 1: Rg={Rg1:.0f}Å, peak at frame {np.argmax(c1_true)}\")\n",
    "print(f\"Component 2: Rg={Rg2:.0f}Å, peak at frame {np.argmax(c2_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d6383",
   "metadata": {},
   "source": [
    "## Section 2: Define Smoothness Regularization\n",
    "\n",
    "Generic Q = (D²)ᵀD² (same as Stages 5, 7, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_D2_operator(K):\n",
    "    \"\"\"Second derivative finite difference operator.\"\"\"\n",
    "    D2 = np.zeros((K-2, K))\n",
    "    for i in range(K-2):\n",
    "        D2[i, i] = 1\n",
    "        D2[i, i+1] = -2\n",
    "        D2[i, i+2] = 1\n",
    "    return D2\n",
    "\n",
    "D2 = build_D2_operator(K)\n",
    "Q_generic = D2.T @ D2\n",
    "\n",
    "print(f\"D² operator shape: {D2.shape}\")\n",
    "print(f\"Q matrix shape: {Q_generic.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31688621",
   "metadata": {},
   "source": [
    "## Section 3: Test Hypothesis 1 - Objective Value Comparison\n",
    "\n",
    "**Critical test**: Does degenerate solution have LOWER objective value than true solution?\n",
    "\n",
    "If yes → landscape problem (all optimizers prefer degenerate)\n",
    "If no → ALS gets trapped (global optimizers might escape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective_additive(P, C, M, Q, lambda_val):\n",
    "    \"\"\"Additive objective: ||M - PC||² + λ·tr(CQCᵀ)\"\"\"\n",
    "    reconstruction = np.linalg.norm(M - P @ C, 'fro')**2\n",
    "    smoothness = np.trace(C @ Q @ C.T)\n",
    "    return reconstruction + lambda_val * smoothness\n",
    "\n",
    "def compute_objective_log_additive(P, C, M, Q, lambda_val):\n",
    "    \"\"\"Log-additive: log(||M - PC||²) + λ·log(tr(CQCᵀ))\"\"\"\n",
    "    reconstruction = np.linalg.norm(M - P @ C, 'fro')**2\n",
    "    smoothness = np.trace(C @ Q @ C.T)\n",
    "    return np.log(reconstruction) + lambda_val * np.log(smoothness)\n",
    "\n",
    "lambda_val = 0.1\n",
    "\n",
    "# Evaluate TRUE solution\n",
    "obj_true_add = compute_objective_additive(P_true, C_true, M, Q_generic, lambda_val)\n",
    "obj_true_log = compute_objective_log_additive(P_true, C_true, M, Q_generic, lambda_val)\n",
    "\n",
    "print(\"TRUE SOLUTION OBJECTIVES:\")\n",
    "print(f\"  Additive:     {obj_true_add:.6f}\")\n",
    "print(f\"  Log-additive: {obj_true_log:.6f}\")\n",
    "print()\n",
    "\n",
    "# Create DEGENERATE solution (one bimodal, one flat)\n",
    "# Component 1: bimodal (covers both peaks)\n",
    "c1_deg = 0.5 * np.exp(-((frames - 35)**2) / (2 * 8**2)) + 0.5 * np.exp(-((frames - 55)**2) / (2 * 8**2))\n",
    "c1_deg = c1_deg / np.max(c1_deg)\n",
    "\n",
    "# Component 2: nearly flat (small constant)\n",
    "c2_deg = 0.05 * np.ones(K)\n",
    "\n",
    "C_degen = np.vstack([c1_deg, c2_deg])\n",
    "\n",
    "# Find P that best fits M given degenerate C (using least squares)\n",
    "P_degen = lstsq(C_degen.T, M.T)[0].T\n",
    "\n",
    "# Evaluate DEGENERATE solution\n",
    "obj_degen_add = compute_objective_additive(P_degen, C_degen, M, Q_generic, lambda_val)\n",
    "obj_degen_log = compute_objective_log_additive(P_degen, C_degen, M, Q_generic, lambda_val)\n",
    "\n",
    "print(\"DEGENERATE SOLUTION OBJECTIVES:\")\n",
    "print(f\"  Additive:     {obj_degen_add:.6f}\")\n",
    "print(f\"  Log-additive: {obj_degen_log:.6f}\")\n",
    "print()\n",
    "\n",
    "# CRITICAL COMPARISON\n",
    "print(\"=\"*70)\n",
    "print(\"HYPOTHESIS 1 TEST: Is degenerate solution better by objective?\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nAdditive objective:\")\n",
    "if obj_degen_add < obj_true_add:\n",
    "    print(f\"  ✓ DEGENERATE BETTER: {obj_degen_add:.6f} < {obj_true_add:.6f}\")\n",
    "    print(f\"    Difference: {obj_true_add - obj_degen_add:.6f}\")\n",
    "    print(f\"    → LANDSCAPE PROBLEM (all optimizers prefer degenerate)\")\n",
    "else:\n",
    "    print(f\"  ✗ TRUE BETTER: {obj_true_add:.6f} < {obj_degen_add:.6f}\")\n",
    "    print(f\"    Difference: {obj_degen_add - obj_true_add:.6f}\")\n",
    "    print(f\"    → ALS GETS TRAPPED (global optimizers might escape)\")\n",
    "\n",
    "print(f\"\\nLog-additive objective:\")\n",
    "if obj_degen_log < obj_true_log:\n",
    "    print(f\"  ✓ DEGENERATE BETTER: {obj_degen_log:.6f} < {obj_true_log:.6f}\")\n",
    "    print(f\"    Difference: {obj_true_log - obj_degen_log:.6f}\")\n",
    "    print(f\"    → LANDSCAPE PROBLEM (all optimizers prefer degenerate)\")\n",
    "else:\n",
    "    print(f\"  ✗ TRUE BETTER: {obj_true_log:.6f} < {obj_degen_log:.6f}\")\n",
    "    print(f\"    Difference: {obj_degen_log - obj_true_log:.6f}\")\n",
    "    print(f\"    → ALS GETS TRAPPED (global optimizers might escape)\")\n",
    "\n",
    "print()\n",
    "print(\"INTERPRETATION:\")\n",
    "    print(\"  If degenerate is better → Multiple Minima Conjecture applies to ALL optimizers\")\n",
    "    print(\"  If true is better → Conjecture may be ALS-specific, global optimizers might succeed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc57a31",
   "metadata": {},
   "source": [
    "## Section 4: Visualize True vs Degenerate Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# True solution\n",
    "axes[0].plot(frames, c1_true, 'b-', label='Component 1', linewidth=2)\n",
    "axes[0].plot(frames, c2_true, 'r-', label='Component 2', linewidth=2)\n",
    "axes[0].set_xlabel('Frame')\n",
    "axes[0].set_ylabel('Concentration')\n",
    "axes[0].set_title('Ground Truth', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Degenerate solution\n",
    "axes[1].plot(frames, c1_deg, 'b-', label='Component 1 (bimodal)', linewidth=2)\n",
    "axes[1].plot(frames, c2_deg, 'r-', label='Component 2 (flat)', linewidth=2)\n",
    "axes[1].set_xlabel('Frame')\n",
    "axes[1].set_ylabel('Concentration')\n",
    "axes[1].set_title('Degenerate Solution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Smoothness comparison\n",
    "smoothness_true = np.trace(C_true @ Q_generic @ C_true.T)\n",
    "smoothness_degen = np.trace(C_degen @ Q_generic @ C_degen.T)\n",
    "\n",
    "print(f\"\\nSmoothness penalty comparison:\")\n",
    "print(f\"  True:       tr(CQCᵀ) = {smoothness_true:.6f}\")\n",
    "print(f\"  Degenerate: tr(CQCᵀ) = {smoothness_degen:.6f}\")\n",
    "if smoothness_degen < smoothness_true:\n",
    "    print(f\"  → Degenerate is SMOOTHER (explains why it's preferred!)\")\n",
    "else:\n",
    "    print(f\"  → Degenerate is ROUGHER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130013b7",
   "metadata": {},
   "source": [
    "## Section 5: Basin-hopping Implementation\n",
    "\n",
    "Test whether global optimization escapes degeneracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5efe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_additive_basinhopping(x, M, Q, lambda_val, n_components, n_q, K):\n",
    "    \"\"\"Objective function for basin-hopping (additive).\"\"\"\n",
    "    # Unpack: first n_q*n_components elements are P (column-major), rest are C\n",
    "    P = x[:n_q * n_components].reshape((n_q, n_components), order='F')\n",
    "    C = x[n_q * n_components:].reshape((n_components, K), order='C')\n",
    "    \n",
    "    # Enforce non-negativity via absolute value\n",
    "    P = np.abs(P)\n",
    "    C = np.abs(C)\n",
    "    \n",
    "    reconstruction = np.linalg.norm(M - P @ C, 'fro')**2\n",
    "    smoothness = np.trace(C @ Q @ C.T)\n",
    "    \n",
    "    return reconstruction + lambda_val * smoothness\n",
    "\n",
    "def objective_log_additive_basinhopping(x, M, Q, lambda_val, n_components, n_q, K):\n",
    "    \"\"\"Objective function for basin-hopping (log-additive).\"\"\"\n",
    "    P = x[:n_q * n_components].reshape((n_q, n_components), order='F')\n",
    "    C = x[n_q * n_components:].reshape((n_components, K), order='C')\n",
    "    \n",
    "    # Enforce non-negativity\n",
    "    P = np.abs(P)\n",
    "    C = np.abs(C)\n",
    "    \n",
    "    reconstruction = np.linalg.norm(M - P @ C, 'fro')**2\n",
    "    smoothness = np.trace(C @ Q @ C.T)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    return np.log(reconstruction + eps) + lambda_val * np.log(smoothness + eps)\n",
    "\n",
    "def check_solution_correctness(P_opt, C_opt, P_true, C_true, threshold=0.3):\n",
    "    \"\"\"Check if solution matches ground truth (handling permutations).\"\"\"\n",
    "    n_components = C_true.shape[0]\n",
    "    \n",
    "    # Normalize for comparison\n",
    "    C_opt_norm = C_opt / (np.max(np.abs(C_opt), axis=1, keepdims=True) + 1e-10)\n",
    "    C_true_norm = C_true / (np.max(np.abs(C_true), axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    # Try both permutations\n",
    "    from itertools import permutations\n",
    "    \n",
    "    best_error = float('inf')\n",
    "    for perm in permutations(range(n_components)):\n",
    "        C_perm = C_opt_norm[list(perm), :]\n",
    "        error = np.mean([np.linalg.norm(C_perm[i] - C_true_norm[i]) \n",
    "                        for i in range(n_components)])\n",
    "        best_error = min(best_error, error)\n",
    "    \n",
    "    is_correct = best_error < threshold\n",
    "    \n",
    "    # Check for degeneracy (one component nearly flat)\n",
    "    C_std = np.std(C_opt, axis=1)\n",
    "    is_degenerate = np.any(C_std < 0.05 * np.max(C_std))\n",
    "    \n",
    "    return is_correct, is_degenerate, best_error\n",
    "\n",
    "print(\"Basin-hopping implementation ready.\")\n",
    "print(f\"Total variables: {n_q * n_components + n_components * K} (P: {n_q * n_components}, C: {n_components * K})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88620e",
   "metadata": {},
   "source": [
    "## Section 6: Test Basin-hopping with Additive Objective\n",
    "\n",
    "**Baseline**: ALS + additive = 35% (Stage 5)\n",
    "\n",
    "**Question**: Does global optimization improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Basin-hopping with ADDITIVE objective...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "n_trials = 10  # Reduced for computational cost\n",
    "results_basinhopping_add = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Random initialization\n",
    "    P_init = np.random.rand(n_q, n_components)\n",
    "    C_init = np.random.rand(n_components, K)\n",
    "    x0 = np.concatenate([P_init.flatten(order='F'), C_init.flatten(order='C')])\n",
    "    \n",
    "    # Basin-hopping optimization\n",
    "    result = basinhopping(\n",
    "        objective_additive_basinhopping,\n",
    "        x0,\n",
    "        minimizer_kwargs={\n",
    "            'args': (M, Q_generic, lambda_val, n_components, n_q, K),\n",
    "            'method': 'L-BFGS-B'\n",
    "        },\n",
    "        niter=50,  # Basin-hopping iterations\n",
    "        seed=trial\n",
    "    )\n",
    "    \n",
    "    # Extract solution\n",
    "    P_opt = result.x[:n_q * n_components].reshape((n_q, n_components), order='F')\n",
    "    C_opt = result.x[n_q * n_components:].reshape((n_components, K), order='C')\n",
    "    P_opt = np.abs(P_opt)\n",
    "    C_opt = np.abs(C_opt)\n",
    "    \n",
    "    # Check correctness\n",
    "    is_correct, is_degenerate, error = check_solution_correctness(P_opt, C_opt, P_true, C_true)\n",
    "    \n",
    "    results_basinhopping_add.append({\n",
    "        'trial': trial,\n",
    "        'correct': is_correct,\n",
    "        'degenerate': is_degenerate,\n",
    "        'error': error,\n",
    "        'objective': result.fun,\n",
    "        'P_opt': P_opt,\n",
    "        'C_opt': C_opt\n",
    "    })\n",
    "    \n",
    "    status = '✓ correct' if is_correct else ('⚠ degenerate' if is_degenerate else '✗ incorrect')\n",
    "    print(f\"Trial {trial:2d}: {status:15s} (obj={result.fun:8.2f}, error={error:.3f})\")\n",
    "\n",
    "# Summary\n",
    "n_correct = sum(r['correct'] for r in results_basinhopping_add)\n",
    "n_degenerate = sum(r['degenerate'] for r in results_basinhopping_add)\n",
    "success_rate = 100 * n_correct / n_trials\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASIN-HOPPING + ADDITIVE RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Success rate: {n_correct}/{n_trials} ({success_rate:.0f}%)\")\n",
    "print(f\"Degenerate: {n_degenerate}/{n_trials}\")\n",
    "print(f\"\\nComparison to ALS baseline: 35% (Stage 5)\")\n",
    "if success_rate > 35:\n",
    "    print(f\"→ IMPROVEMENT: Basin-hopping helps (+{success_rate-35:.0f}%)\")\n",
    "    print(f\"→ Suggests partly optimization problem (ALS gets trapped)\")\n",
    "else:\n",
    "    print(f\"→ NO IMPROVEMENT: Similar or worse than ALS\")\n",
    "    print(f\"→ Suggests landscape problem (degenerate has lower objective)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f4d56",
   "metadata": {},
   "source": [
    "## Section 7: Test Basin-hopping with Log-Additive Objective\n",
    "\n",
    "**Baseline**: ALS + log-additive = 25% (Stage 7)\n",
    "\n",
    "**Relevance to Molass**: Molass uses log-additive + Basin-hopping (plus additional constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Basin-hopping with LOG-ADDITIVE objective...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "results_basinhopping_log = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Random initialization\n",
    "    P_init = np.random.rand(n_q, n_components)\n",
    "    C_init = np.random.rand(n_components, K)\n",
    "    x0 = np.concatenate([P_init.flatten(order='F'), C_init.flatten(order='C')])\n",
    "    \n",
    "    # Basin-hopping optimization\n",
    "    result = basinhopping(\n",
    "        objective_log_additive_basinhopping,\n",
    "        x0,\n",
    "        minimizer_kwargs={\n",
    "            'args': (M, Q_generic, lambda_val, n_components, n_q, K),\n",
    "            'method': 'L-BFGS-B'\n",
    "        },\n",
    "        niter=50,\n",
    "        seed=trial\n",
    "    )\n",
    "    \n",
    "    # Extract solution\n",
    "    P_opt = result.x[:n_q * n_components].reshape((n_q, n_components), order='F')\n",
    "    C_opt = result.x[n_q * n_components:].reshape((n_components, K), order='C')\n",
    "    P_opt = np.abs(P_opt)\n",
    "    C_opt = np.abs(C_opt)\n",
    "    \n",
    "    # Check correctness\n",
    "    is_correct, is_degenerate, error = check_solution_correctness(P_opt, C_opt, P_true, C_true)\n",
    "    \n",
    "    results_basinhopping_log.append({\n",
    "        'trial': trial,\n",
    "        'correct': is_correct,\n",
    "        'degenerate': is_degenerate,\n",
    "        'error': error,\n",
    "        'objective': result.fun,\n",
    "        'P_opt': P_opt,\n",
    "        'C_opt': C_opt\n",
    "    })\n",
    "    \n",
    "    status = '✓ correct' if is_correct else ('⚠ degenerate' if is_degenerate else '✗ incorrect')\n",
    "    print(f\"Trial {trial:2d}: {status:15s} (obj={result.fun:8.2f}, error={error:.3f})\")\n",
    "\n",
    "# Summary\n",
    "n_correct = sum(r['correct'] for r in results_basinhopping_log)\n",
    "n_degenerate = sum(r['degenerate'] for r in results_basinhopping_log)\n",
    "success_rate = 100 * n_correct / n_trials\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASIN-HOPPING + LOG-ADDITIVE RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Success rate: {n_correct}/{n_trials} ({success_rate:.0f}%)\")\n",
    "print(f\"Degenerate: {n_degenerate}/{n_trials}\")\n",
    "print(f\"\\nComparison to ALS baseline: 25% (Stage 7)\")\n",
    "if success_rate > 25:\n",
    "    print(f\"→ IMPROVEMENT: Basin-hopping helps (+{success_rate-25:.0f}%)\")\n",
    "    print(f\"→ Suggests partly optimization problem\")\n",
    "else:\n",
    "    print(f\"→ NO IMPROVEMENT: Similar or worse than ALS\")\n",
    "    print(f\"→ Suggests landscape problem persists with log-additive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86d964",
   "metadata": {},
   "source": [
    "## Section 8: Summary and Interpretation\n",
    "\n",
    "**For JOSS validation context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MULTIPLE MINIMA CONJECTURE SCOPE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"RESEARCH QUESTIONS ANSWERED:\")\n",
    "print()\n",
    "print(\"1. Does degenerate solution have lower objective value?\")\n",
    "print(f\"   Additive:     {'YES - landscape problem' if obj_degen_add < obj_true_add else 'NO - ALS gets trapped'}\")\n",
    "print(f\"   Log-additive: {'YES - landscape problem' if obj_degen_log < obj_true_log else 'NO - ALS gets trapped'}\")\n",
    "print()\n",
    "print(\"2. Does Basin-hopping improve over ALS?\")\n",
    "success_add_bh = 100 * sum(r['correct'] for r in results_basinhopping_add) / len(results_basinhopping_add)\n",
    "success_log_bh = 100 * sum(r['correct'] for r in results_basinhopping_log) / len(results_basinhopping_log)\n",
    "print(f\"   Additive:     {success_add_bh:.0f}% (vs 35% ALS baseline)\")\n",
    "print(f\"   Log-additive: {success_log_bh:.0f}% (vs 25% ALS baseline)\")\n",
    "print()\n",
    "print(\"3. How does this inform theorem relevance for Molass?\")\n",
    "print()\n",
    "if obj_degen_add < obj_true_add and obj_degen_log < obj_true_log:\n",
    "    print(\"   FINDING: Degenerate solution has LOWER objective (both operators)\")\n",
    "    print(\"   → Multiple Minima Conjecture describes LANDSCAPE problem\")\n",
    "    print(\"   → Applies to ALL optimization methods (ALS, Basin-hopping, etc.)\")\n",
    "    print()\n",
    "    print(\"   IMPLICATION FOR MOLASS:\")\n",
    "    print(\"   - Molass uses log-additive + Basin-hopping (similar setup tested here)\")\n",
    "    if success_log_bh < 50:\n",
    "        print(\"   - Basin-hopping + log-additive still shows multiple minima issue\")\n",
    "        print(\"   - Molass's ADDITIONAL constraints (Rg-consistency, parametric models)\")\n",
    "        print(\"     likely address this fundamental limitation\")\n",
    "        print(\"   - Conjecture validates Molass's approach: explicit constraints needed\")\n",
    "    else:\n",
    "        print(\"   - Basin-hopping + log-additive shows improvement\")\n",
    "        print(\"   - Molass's additional constraints further strengthen this\")\n",
    "else:\n",
    "    print(\"   FINDING: True solution has LOWER objective\")\n",
    "    print(\"   → Complex landscape with multiple local minima (not just degeneracy)\")\n",
    "    print(\"   → Both ALS and Basin-hopping find incorrect solutions\")\n",
    "    print()\n",
    "    print(\"   IMPLICATION FOR MOLASS:\")\n",
    "    print(\"   - Conjecture applies broadly: multiple minima exist regardless of optimizer\")\n",
    "    print(\"   - Basin-hopping finds different minima (permuted/shifted) than ALS (degenerate)\")\n",
    "    print(\"   - Molass's additional constraints (Rg-consistency, parametric models) essential\")\n",
    "    print(\"   - Conjecture validates explicit physical constraints break ambiguity\")\n",
    "print()\n",
    "print(\"IMPORTANT CAVEAT:\")\n",
    "print(\"This experiment does NOT test Molass's full approach:\")\n",
    "print(\"  - Missing: Rg-consistency constraints\")\n",
    "print(\"  - Missing: Parametric profile models\")\n",
    "print(\"  - Missing: Other domain-specific constraints\")\n",
    "\n",
    "print()print(\"PURPOSE: Understand theorem scope, not precise Molass comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56755ff9",
   "metadata": {},
   "source": [
    "## Section 9: Visualize Best Solutions from Each Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].plot(frames, c1_true, 'b-', label='Component 1', linewidth=2)\n",
    "axes[0].plot(frames, c2_true, 'r-', label='Component 2', linewidth=2)\n",
    "axes[0].set_xlabel('Frame')\n",
    "axes[0].set_ylabel('Concentration')\n",
    "axes[0].set_title('Ground Truth', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Basin-hopping + additive (best solution)\n",
    "if results_basinhopping_add:\n",
    "    best_add = min(results_basinhopping_add, key=lambda x: x['error'])\n",
    "    C_best_add = best_add['C_opt']\n",
    "    C_norm = C_best_add / (np.max(np.abs(C_best_add), axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    status_add = 'Correct' if best_add['correct'] else ('Degenerate' if best_add['degenerate'] else 'Incorrect')\n",
    "    \n",
    "    axes[1].plot(frames, C_norm[0], 'b-', label='Comp 1', linewidth=2)\n",
    "    axes[1].plot(frames, C_norm[1], 'r-', label='Comp 2', linewidth=2)\n",
    "    axes[1].set_xlabel('Frame')\n",
    "    axes[1].set_ylabel('Concentration')\n",
    "    axes[1].set_title(f'Basin-hopping + Additive\\n({status_add}, error={best_add[\"error\"]:.3f})', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Basin-hopping + log-additive (best solution)\n",
    "if results_basinhopping_log:\n",
    "    best_log = min(results_basinhopping_log, key=lambda x: x['error'])\n",
    "    C_best_log = best_log['C_opt']\n",
    "    C_norm = C_best_log / (np.max(np.abs(C_best_log), axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    status_log = 'Correct' if best_log['correct'] else ('Degenerate' if best_log['degenerate'] else 'Incorrect')\n",
    "    \n",
    "    axes[2].plot(frames, C_norm[0], 'b-', label='Comp 1', linewidth=2)\n",
    "    axes[2].plot(frames, C_norm[1], 'r-', label='Comp 2', linewidth=2)\n",
    "    axes[2].set_xlabel('Frame')\n",
    "    axes[2].set_ylabel('Concentration')\n",
    "    axes[2].set_title(f'Basin-hopping + Log-additive\\n({status_log}, error={best_log[\"error\"]:.3f})', fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
